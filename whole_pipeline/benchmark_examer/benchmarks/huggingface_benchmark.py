# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# HuggingFace Benchmarkï¼šä»HuggingFace HubåŠ è½½benchmarkæ•°æ®
# """

# from typing import List, Dict, Any, Optional
# from .base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# class HuggingFaceBenchmark(BaseBenchmark):
#     """ä»HuggingFace HubåŠ è½½çš„Benchmark"""
    
#     def __init__(self, 
#                  name: str,
#                  hf_id: str,
#                  config: str = "default",
#                  split: str = None,
#                  **kwargs):
#         """
#         Args:
#             name: Benchmarkåç§°
#             hf_id: HuggingFaceæ•°æ®é›†IDï¼ˆå¦‚ "gqa", "clevr"ï¼‰
#             config: æ•°æ®é›†é…ç½®åï¼ˆå¦‚ "balanced", "default"ï¼‰
#             split: æ•°æ®é›†splitï¼ˆå¦‚ "validation", "test"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤split
#         """
#         self.hf_id = hf_id
#         self.config = config
#         self.split = split
#         self.benchmark_info = kwargs.get("benchmark_info", {})
        
#         # å¦‚æœæ²¡æœ‰æŒ‡å®šsplitï¼Œä½¿ç”¨é»˜è®¤split
#         if self.split is None:
#             self.split = self.benchmark_info.get("default_split", "validation")
        
#         # data_pathå‚æ•°åœ¨è¿™é‡Œä¸ä½¿ç”¨ï¼Œä½†éœ€è¦ä¼ é€’ä»¥æ»¡è¶³åŸºç±»è¦æ±‚
#         super().__init__(name, f"hf://{hf_id}/{config}/{self.split}", **kwargs)
#         self.description = f"HuggingFace Benchmark: {name} ({hf_id})"
    
#     def _load_data(self):
#         """ä»HuggingFace HubåŠ è½½æ•°æ®"""
#         try:
#             from datasets import load_dataset
#         except ImportError:
#             raise ImportError(
#                 "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
#             )
        
#         try:
#             # åŠ è½½æ•°æ®é›†
#             if self.config == "default" or self.config is None:
#                 dataset = load_dataset(self.hf_id, split=self.split)
#             else:
#                 dataset = load_dataset(self.hf_id, name=self.config, split=self.split)
            
#             self.tasks = []
            
#             # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
#             for idx, item in enumerate(dataset):
#                 task = self._convert_to_task(item, idx)
#                 if task:
#                     self.tasks.append(task)
        
#         except Exception as e:
#             raise RuntimeError(f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}")
    
#     def _convert_to_task(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
#         """
#         å°†æ•°æ®é›†itemè½¬æ¢ä¸ºBenchmarkTask
        
#         ä¸åŒbenchmarkçš„æ•°æ®æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
#         """
#         task_id = item.get("id", item.get("question_id", f"task_{idx}"))
        
#         # è·å–é—®é¢˜
#         question = item.get("question", item.get("text", ""))
#         if not question:
#             return None
        
#         # è·å–å›¾åƒ
#         images = []
#         if "image" in item:
#             # PIL Imageå¯¹è±¡
#             images.append(item["image"])
#         elif "image_path" in item:
#             images.append(item["image_path"])
#         elif "image_file" in item:
#             images.append(item["image_file"])
#         elif "img" in item:
#             images.append(item["img"])
        
#         # è·å–æ­£ç¡®ç­”æ¡ˆ
#         ground_truth = item.get("answer", item.get("answers", None))
#         if ground_truth is None:
#             ground_truth = item.get("label", item.get("target", ""))
        
#         # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
#         if isinstance(ground_truth, list) and len(ground_truth) > 0:
#             ground_truth = ground_truth[0]
        
#         # æå–metadataï¼ˆåŒ…æ‹¬taxonomyä¿¡æ¯ï¼‰
#         metadata = {}
#         taxonomy_fields = self.benchmark_info.get("native_taxonomy_fields", [])
#         for field in taxonomy_fields:
#             if field in item:
#                 metadata[field] = item[field]
        
#         # ä¿ç•™å…¶ä»–æœ‰ç”¨å­—æ®µ
#         for key in ["question_type", "semantic", "program", "question_family", 
#                    "chart_type", "category", "capability", "task_group"]:
#             if key in item and key not in metadata:
#                 metadata[key] = item[key]
        
#         return BenchmarkTask(
#             task_id=str(task_id),
#             question=str(question),
#             images=images if images else [],
#             ground_truth=ground_truth,
#             metadata=metadata
#         )
    
#     def evaluate_answer(self, 
#                        model_answer: str, 
#                        ground_truth: Any,
#                        task: BenchmarkTask) -> BenchmarkResult:
#         """
#         è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ
        
#         ç®€å•å®ç°ï¼šå­—ç¬¦ä¸²åŒ¹é…
#         å¯ä»¥æ ¹æ®ä¸åŒbenchmarkçš„ç‰¹ç‚¹å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘
#         """
#         model_answer_clean = str(model_answer).strip().lower()
#         ground_truth_clean = str(ground_truth).strip().lower()
        
#         # ç²¾ç¡®åŒ¹é…
#         is_correct = model_answer_clean == ground_truth_clean
        
#         # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœç­”æ¡ˆåŒ…å«å…³é”®è¯ï¼‰
#         if not is_correct:
#             if ground_truth_clean in model_answer_clean or model_answer_clean in ground_truth_clean:
#                 score = 0.5
#             else:
#                 score = 0.0
#         else:
#             score = 1.0
        
#         # å¯ä»¥åœ¨è¿™é‡Œå®ç°ç‰¹å®šbenchmarkçš„è¯„ä¼°é€»è¾‘
#         # ä¾‹å¦‚GQAå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ŒCLEVRéœ€è¦æ‰§è¡Œç¨‹åºç­‰
        
#         return BenchmarkResult(
#             task_id=task.task_id,
#             question=task.question,
#             ground_truth=ground_truth,
#             model_answer=model_answer,
#             is_correct=is_correct,
#             score=score,
#             metadata={"evaluation_method": "exact_match", "benchmark": self.name}
#         )
    
#     def get_info(self) -> Dict[str, Any]:
#         """è·å–benchmarkä¿¡æ¯"""
#         info = super().get_info()
#         info.update({
#             "hf_id": self.hf_id,
#             "config": self.config,
#             "split": self.split,
#             "source": "huggingface",
#             "native_taxonomy_fields": self.benchmark_info.get("native_taxonomy_fields", [])
#         })
#         return info





#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace Benchmarkï¼šä»HuggingFace HubåŠ è½½benchmarkæ•°æ®
"""

from typing import List, Dict, Any, Optional
from .base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


class HuggingFaceBenchmark(BaseBenchmark):
    """ä»HuggingFace HubåŠ è½½çš„Benchmark"""
    
    def __init__(self, 
                 name: str,
                 hf_id: str,
                 config: str = "default",
                 split: str = None,
                 **kwargs):
        """
        Args:
            name: Benchmarkåç§°
            hf_id: HuggingFaceæ•°æ®é›†IDï¼ˆå®Œæ•´IDï¼Œå¦‚ "lmms-lab/GQA"ï¼‰
            config: æ•°æ®é›†é…ç½®åï¼ˆå¦‚ "balanced", "default"ï¼‰
            split: æ•°æ®é›†splitï¼ˆå¦‚ "validation", "test"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤split
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬ benchmark_info
        """
        self.hf_id = hf_id
        self.config = config if config and config != "default" else None
        self.benchmark_info = kwargs.get("benchmark_info", {})
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šsplitï¼Œä½¿ç”¨benchmark_infoä¸­çš„é»˜è®¤split
        if split is None:
            split = self.benchmark_info.get("default_split", "validation")
        self.split = split
        
        # è·å–å¯ç”¨çš„splitsï¼ˆç”¨äºéªŒè¯ï¼‰
        self.available_splits = self.benchmark_info.get("available_splits", [])
        
        # data_pathå‚æ•°åœ¨è¿™é‡Œä¸ä½¿ç”¨ï¼Œä½†éœ€è¦ä¼ é€’ä»¥æ»¡è¶³åŸºç±»è¦æ±‚
        data_path = f"hf://{self.hf_id}"
        if self.config:
            data_path += f"/{self.config}"
        data_path += f"/{self.split}"
        
        super().__init__(name, data_path, **kwargs)
        self.description = f"HuggingFace Benchmark: {name} ({self.hf_id})"
    
    # def _load_data(self):
    #     """ä»HuggingFace HubåŠ è½½æ•°æ®"""
    #     try:
    #         from datasets import load_dataset
    #     except ImportError:
    #         raise ImportError(
    #             "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
    #         )
        
    #     try:
    #         print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
    #         print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            
    #         # éªŒè¯splitæ˜¯å¦å¯ç”¨
    #         if self.available_splits and self.split not in self.available_splits:
    #             print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
    #         # åŠ è½½æ•°æ®é›†
    #         load_kwargs = {
    #             "split": self.split,
    #             "trust_remote_code": True
    #         }
            
    #         if self.config:
    #             load_kwargs["name"] = self.config
            
    #         dataset = load_dataset(self.hf_id, **load_kwargs)
            
    #         print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®")
            
    #         self.tasks = []
            
    #         # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
    #         for idx, item in enumerate(dataset):
    #             task = self._convert_to_task(item, idx)
    #             if task:
    #                 self.tasks.append(task)
            
    #         print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        
    #     except Exception as e:
    #         # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    #         error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
    #         print(f"âœ— {error_msg}")
            
    #         # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
    #         suggestions = []
    #         if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
    #             suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
    #             suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
    #             if self.config:
    #                 suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
    #             suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
    #             if self.available_splits:
    #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
    #         elif "split" in str(e).lower():
    #             suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
    #             if self.available_splits:
    #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
    #             suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
    #         if suggestions:
    #             error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
    #         raise RuntimeError(error_msg)

    def _load_data(self):
        """ä»HuggingFace HubåŠ è½½æ•°æ®"""
        try:
            from datasets import load_dataset
        except ImportError:
            raise ImportError(
                "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
            )
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
            print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            
            # éªŒè¯splitæ˜¯å¦å¯ç”¨
            if self.available_splits and self.split not in self.available_splits:
                print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
            # åŠ è½½æ•°æ®é›†
            load_kwargs = {
                "split": self.split
            }
            
            if self.config:
                load_kwargs["name"] = self.config
            
            dataset = load_dataset(self.hf_id, **load_kwargs)
            
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®")
            
            self.tasks = []
            
            # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
            for idx, item in enumerate(dataset):
                task = self._convert_to_task(item, idx)
                if task:
                    self.tasks.append(task)
            
            print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
            print(f"âœ— {error_msg}")
            
            # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
            suggestions = []
            if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
                suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
                suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
                if self.config:
                    suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
                suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
                if self.available_splits:
                    suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
            elif "Config name is missing" in str(e):
                # ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å¯ç”¨çš„configs
                suggestions.append(f"â€¢ æ•°æ®é›†éœ€è¦æŒ‡å®šconfigåç§°")
                suggestions.append(f"â€¢ å½“å‰configè®¾ç½®: {self.config or 'None'}")
                suggestions.append(f"â€¢ è¯·åœ¨configæ–‡ä»¶ä¸­ä¸ºæ­¤benchmarkæŒ‡å®šæ­£ç¡®çš„configåç§°")
                # å°è¯•æå–å¯ç”¨configsåˆ—è¡¨
                import re
                configs_match = re.search(r"available configs: (\[.*?\])", str(e))
                if configs_match:
                    suggestions.append(f"â€¢ å¯ç”¨çš„configs: {configs_match.group(1)}")
            elif "split" in str(e).lower():
                suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
                if self.available_splits:
                    suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
                suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
            if suggestions:
                error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
            raise RuntimeError(error_msg)
    
    def _convert_to_task(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
        """
        å°†æ•°æ®é›†itemè½¬æ¢ä¸ºBenchmarkTask
        
        ä¸åŒbenchmarkçš„æ•°æ®æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
        """
        task_id = item.get("id", item.get("question_id", item.get("questionId", f"task_{idx}")))
        
        # è·å–é—®é¢˜ - æ”¯æŒå¤šç§å­—æ®µå
        question = item.get("question", item.get("text", item.get("query", "")))
        if not question:
            return None
        
        # è·å–å›¾åƒ - æ”¯æŒå¤šç§å­—æ®µå
        images = []
        image_fields = ["image", "img", "image_path", "image_file", "imageId"]
        for field in image_fields:
            if field in item and item[field] is not None:
                images.append(item[field])
                break  # åªå–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾åƒå­—æ®µ
        
        # è·å–æ­£ç¡®ç­”æ¡ˆ - æ”¯æŒå¤šç§å­—æ®µå
        ground_truth = None
        answer_fields = ["answer", "answers", "label", "target", "gt_answer"]
        for field in answer_fields:
            if field in item and item[field] is not None:
                ground_truth = item[field]
                break
        
        # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
        if isinstance(ground_truth, list) and len(ground_truth) > 0:
            ground_truth = ground_truth[0]
        
        # æå–metadataï¼ˆåŒ…æ‹¬taxonomyä¿¡æ¯ï¼‰
        metadata = {}
        
        # 1. é¦–å…ˆæå–configä¸­å®šä¹‰çš„native_taxonomy_fields
        taxonomy_fields = self.benchmark_info.get("native_taxonomy_fields", [])
        for field in taxonomy_fields:
            if field in item:
                metadata[field] = item[field]
        
        # 2. ç„¶åæå–å…¶ä»–å¸¸è§çš„æœ‰ç”¨å­—æ®µï¼ˆå¦‚æœä¸åœ¨taxonomyä¸­ï¼‰
        additional_fields = [
            "question_type", "semantic", "program", "question_family", 
            "chart_type", "category", "capability", "task_group",
            "difficulty", "skill", "domain", "task", "subcategory"
        ]
        for key in additional_fields:
            if key in item and key not in metadata:
                metadata[key] = item[key]
        
        # 3. æ·»åŠ benchmarké…ç½®ä¸­çš„noteï¼ˆå¦‚æœæœ‰ï¼‰
        if "note" in self.benchmark_info:
            metadata["benchmark_note"] = self.benchmark_info["note"]
        
        return BenchmarkTask(
            task_id=str(task_id),
            question=str(question),
            images=images if images else [],
            ground_truth=ground_truth,
            metadata=metadata
        )
    
    def evaluate_answer(self, 
                       model_answer: str, 
                       ground_truth: Any,
                       task: BenchmarkTask) -> BenchmarkResult:
        """
        è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ
        
        ç®€å•å®ç°ï¼šå­—ç¬¦ä¸²åŒ¹é…
        å¯ä»¥æ ¹æ®ä¸åŒbenchmarkçš„ç‰¹ç‚¹å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘
        """
        model_answer_clean = str(model_answer).strip().lower()
        ground_truth_clean = str(ground_truth).strip().lower()
        
        # ç²¾ç¡®åŒ¹é…
        is_correct = model_answer_clean == ground_truth_clean
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœç­”æ¡ˆåŒ…å«å…³é”®è¯ï¼‰
        if not is_correct:
            if ground_truth_clean in model_answer_clean or model_answer_clean in ground_truth_clean:
                score = 0.5
            else:
                score = 0.0
        else:
            score = 1.0
        
        # å¯ä»¥åœ¨è¿™é‡Œå®ç°ç‰¹å®šbenchmarkçš„è¯„ä¼°é€»è¾‘
        # ä¾‹å¦‚GQAå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ŒCLEVRéœ€è¦æ‰§è¡Œç¨‹åºç­‰
        
        return BenchmarkResult(
            task_id=task.task_id,
            question=task.question,
            ground_truth=ground_truth,
            model_answer=model_answer,
            is_correct=is_correct,
            score=score,
            metadata={"evaluation_method": "exact_match", "benchmark": self.name}
        )
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–benchmarkä¿¡æ¯"""
        info = super().get_info()
        info.update({
            "hf_id": self.hf_id,
            "config": self.config,
            "split": self.split,
            "available_splits": self.available_splits,
            "source": "huggingface",
            "native_taxonomy_fields": self.benchmark_info.get("native_taxonomy_fields", []),
            "note": self.benchmark_info.get("note", "")
        })
        return info