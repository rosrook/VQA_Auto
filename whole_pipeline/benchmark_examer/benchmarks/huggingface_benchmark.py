# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# HuggingFace Benchmarkï¼šä»HuggingFace HubåŠ è½½benchmarkæ•°æ®
# """

# from typing import List, Dict, Any, Optional
# from .base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# class HuggingFaceBenchmark(BaseBenchmark):
#     """ä»HuggingFace HubåŠ è½½çš„Benchmark"""
    
#     def __init__(self, 
#                  name: str,
#                  hf_id: str,
#                  config: str = "default",
#                  split: str = None,
#                  **kwargs):
#         """
#         Args:
#             name: Benchmarkåç§°
#             hf_id: HuggingFaceæ•°æ®é›†IDï¼ˆå®Œæ•´IDï¼Œå¦‚ "lmms-lab/GQA"ï¼‰
#             config: æ•°æ®é›†é…ç½®åï¼ˆå¦‚ "balanced", "default"ï¼‰
#             split: æ•°æ®é›†splitï¼ˆå¦‚ "validation", "test"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤split
#             **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬ benchmark_info
#         """
#         self.hf_id = hf_id
#         self.config = config if config and config != "default" else None
#         self.benchmark_info = kwargs.get("benchmark_info", {})
        
#         # å¦‚æœæ²¡æœ‰æŒ‡å®šsplitï¼Œä½¿ç”¨benchmark_infoä¸­çš„é»˜è®¤split
#         if split is None:
#             split = self.benchmark_info.get("default_split", "validation")
#         self.split = split
        
#         # è·å–å¯ç”¨çš„splitsï¼ˆç”¨äºéªŒè¯ï¼‰
#         self.available_splits = self.benchmark_info.get("available_splits", [])
        
#         # data_pathå‚æ•°åœ¨è¿™é‡Œä¸ä½¿ç”¨ï¼Œä½†éœ€è¦ä¼ é€’ä»¥æ»¡è¶³åŸºç±»è¦æ±‚
#         data_path = f"hf://{self.hf_id}"
#         if self.config:
#             data_path += f"/{self.config}"
#         data_path += f"/{self.split}"
        
#         super().__init__(name, data_path, **kwargs)
#         self.description = f"HuggingFace Benchmark: {name} ({self.hf_id})"
    
#     # def _load_data(self):
#     #     """ä»HuggingFace HubåŠ è½½æ•°æ®"""
#     #     try:
#     #         from datasets import load_dataset
#     #     except ImportError:
#     #         raise ImportError(
#     #             "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
#     #         )
        
#     #     try:
#     #         print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
#     #         print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            
#     #         # éªŒè¯splitæ˜¯å¦å¯ç”¨
#     #         if self.available_splits and self.split not in self.available_splits:
#     #             print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
#     #         # åŠ è½½æ•°æ®é›†
#     #         load_kwargs = {
#     #             "split": self.split,
#     #             "trust_remote_code": True
#     #         }
            
#     #         if self.config:
#     #             load_kwargs["name"] = self.config
            
#     #         dataset = load_dataset(self.hf_id, **load_kwargs)
            
#     #         print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®")
            
#     #         self.tasks = []
            
#     #         # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
#     #         for idx, item in enumerate(dataset):
#     #             task = self._convert_to_task(item, idx)
#     #             if task:
#     #                 self.tasks.append(task)
            
#     #         print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        
#     #     except Exception as e:
#     #         # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
#     #         error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
#     #         print(f"âœ— {error_msg}")
            
#     #         # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
#     #         suggestions = []
#     #         if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
#     #             suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
#     #             suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
#     #             if self.config:
#     #                 suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
#     #             suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
#     #             if self.available_splits:
#     #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
#     #         elif "split" in str(e).lower():
#     #             suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
#     #             if self.available_splits:
#     #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
#     #             suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
#     #         if suggestions:
#     #             error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
#     #         raise RuntimeError(error_msg)

#     def _load_data(self):
#         """ä»HuggingFace HubåŠ è½½æ•°æ®"""
#         try:
#             from datasets import load_dataset
#         except ImportError:
#             raise ImportError(
#                 "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
#             )
        
#         try:
#             print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
#             print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            
#             # éªŒè¯splitæ˜¯å¦å¯ç”¨
#             if self.available_splits and self.split not in self.available_splits:
#                 print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
#             # åŠ è½½æ•°æ®é›†
#             load_kwargs = {
#                 "split": self.split
#             }
            
#             if self.config:
#                 load_kwargs["name"] = self.config
            
#             dataset = load_dataset(self.hf_id, **load_kwargs)
            
#             print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®")
            
#             self.tasks = []
            
#             # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
#             for idx, item in enumerate(dataset):
#                 task = self._convert_to_task(item, idx)
#                 if task:
#                     self.tasks.append(task)
            
#             print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        
#         except Exception as e:
#             # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
#             error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
#             print(f"âœ— {error_msg}")
            
#             # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
#             suggestions = []
#             if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
#                 suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
#                 suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
#                 if self.config:
#                     suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
#                 suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
#                 if self.available_splits:
#                     suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
#             elif "Config name is missing" in str(e):
#                 # ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å¯ç”¨çš„configs
#                 suggestions.append(f"â€¢ æ•°æ®é›†éœ€è¦æŒ‡å®šconfigåç§°")
#                 suggestions.append(f"â€¢ å½“å‰configè®¾ç½®: {self.config or 'None'}")
#                 suggestions.append(f"â€¢ è¯·åœ¨configæ–‡ä»¶ä¸­ä¸ºæ­¤benchmarkæŒ‡å®šæ­£ç¡®çš„configåç§°")
#                 # å°è¯•æå–å¯ç”¨configsåˆ—è¡¨
#                 import re
#                 configs_match = re.search(r"available configs: (\[.*?\])", str(e))
#                 if configs_match:
#                     suggestions.append(f"â€¢ å¯ç”¨çš„configs: {configs_match.group(1)}")
#             elif "split" in str(e).lower():
#                 suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
#                 if self.available_splits:
#                     suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
#                 suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
#             if suggestions:
#                 error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
#             raise RuntimeError(error_msg)
    
#     def _convert_to_task(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
#         """
#         å°†æ•°æ®é›†itemè½¬æ¢ä¸ºBenchmarkTask
        
#         ä¸åŒbenchmarkçš„æ•°æ®æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
#         """
#         task_id = item.get("id", item.get("question_id", item.get("questionId", f"task_{idx}")))
        
#         # è·å–é—®é¢˜ - æ”¯æŒå¤šç§å­—æ®µå
#         question = item.get("question", item.get("text", item.get("query", "")))
#         if not question:
#             return None
        
#         # è·å–å›¾åƒ - æ”¯æŒå¤šç§å­—æ®µå
#         images = []
#         image_fields = ["image", "img", "image_path", "image_file", "imageId"]
#         for field in image_fields:
#             if field in item and item[field] is not None:
#                 images.append(item[field])
#                 break  # åªå–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾åƒå­—æ®µ
        
#         # è·å–æ­£ç¡®ç­”æ¡ˆ - æ”¯æŒå¤šç§å­—æ®µå
#         ground_truth = None
#         answer_fields = ["answer", "answers", "label", "target", "gt_answer"]
#         for field in answer_fields:
#             if field in item and item[field] is not None:
#                 ground_truth = item[field]
#                 break
        
#         # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
#         if isinstance(ground_truth, list) and len(ground_truth) > 0:
#             ground_truth = ground_truth[0]
        
#         # æå–metadataï¼ˆåŒ…æ‹¬taxonomyä¿¡æ¯ï¼‰
#         metadata = {}
        
#         # 1. é¦–å…ˆæå–configä¸­å®šä¹‰çš„native_taxonomy_fields
#         taxonomy_fields = self.benchmark_info.get("native_taxonomy_fields", [])
#         for field in taxonomy_fields:
#             if field in item:
#                 metadata[field] = item[field]
        
#         # 2. ç„¶åæå–å…¶ä»–å¸¸è§çš„æœ‰ç”¨å­—æ®µï¼ˆå¦‚æœä¸åœ¨taxonomyä¸­ï¼‰
#         additional_fields = [
#             "question_type", "semantic", "program", "question_family", 
#             "chart_type", "category", "capability", "task_group",
#             "difficulty", "skill", "domain", "task", "subcategory"
#         ]
#         for key in additional_fields:
#             if key in item and key not in metadata:
#                 metadata[key] = item[key]
        
#         # 3. æ·»åŠ benchmarké…ç½®ä¸­çš„noteï¼ˆå¦‚æœæœ‰ï¼‰
#         if "note" in self.benchmark_info:
#             metadata["benchmark_note"] = self.benchmark_info["note"]
        
#         return BenchmarkTask(
#             task_id=str(task_id),
#             question=str(question),
#             images=images if images else [],
#             ground_truth=ground_truth,
#             metadata=metadata
#         )
    
#     def evaluate_answer(self, 
#                        model_answer: str, 
#                        ground_truth: Any,
#                        task: BenchmarkTask) -> BenchmarkResult:
#         """
#         è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ
        
#         ç®€å•å®ç°ï¼šå­—ç¬¦ä¸²åŒ¹é…
#         å¯ä»¥æ ¹æ®ä¸åŒbenchmarkçš„ç‰¹ç‚¹å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘
#         """
#         model_answer_clean = str(model_answer).strip().lower()
#         ground_truth_clean = str(ground_truth).strip().lower()
        
#         # ç²¾ç¡®åŒ¹é…
#         is_correct = model_answer_clean == ground_truth_clean
        
#         # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœç­”æ¡ˆåŒ…å«å…³é”®è¯ï¼‰
#         if not is_correct:
#             if ground_truth_clean in model_answer_clean or model_answer_clean in ground_truth_clean:
#                 score = 0.5
#             else:
#                 score = 0.0
#         else:
#             score = 1.0
        
#         # å¯ä»¥åœ¨è¿™é‡Œå®ç°ç‰¹å®šbenchmarkçš„è¯„ä¼°é€»è¾‘
#         # ä¾‹å¦‚GQAå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ŒCLEVRéœ€è¦æ‰§è¡Œç¨‹åºç­‰
        
#         return BenchmarkResult(
#             task_id=task.task_id,
#             question=task.question,
#             ground_truth=ground_truth,
#             model_answer=model_answer,
#             is_correct=is_correct,
#             score=score,
#             metadata={"evaluation_method": "exact_match", "benchmark": self.name}
#         )
    
#     def get_info(self) -> Dict[str, Any]:
#         """è·å–benchmarkä¿¡æ¯"""
#         info = super().get_info()
#         info.update({
#             "hf_id": self.hf_id,
#             "config": self.config,
#             "split": self.split,
#             "available_splits": self.available_splits,
#             "source": "huggingface",
#             "native_taxonomy_fields": self.benchmark_info.get("native_taxonomy_fields", []),
#             "note": self.benchmark_info.get("note", "")
#         })
#         return info






# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# HuggingFace Benchmarkï¼šä»HuggingFace HubåŠ è½½benchmarkæ•°æ®
# """

# from typing import List, Dict, Any, Optional
# from .base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# class HuggingFaceBenchmark(BaseBenchmark):
#     """ä»HuggingFace HubåŠ è½½çš„Benchmark"""
    
#     def __init__(self, 
#                  name: str,
#                  hf_id: str,
#                  config: str = "default",
#                  split: str = None,
#                  **kwargs):
#         """
#         Args:
#             name: Benchmarkåç§°
#             hf_id: HuggingFaceæ•°æ®é›†IDï¼ˆå¦‚ "gqa", "clevr"ï¼‰
#             config: æ•°æ®é›†é…ç½®åï¼ˆå¦‚ "balanced", "default"ï¼‰
#             split: æ•°æ®é›†splitï¼ˆå¦‚ "validation", "test"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤split
#         """
#         self.hf_id = hf_id
#         self.config = config
#         self.split = split
#         self.benchmark_info = kwargs.get("benchmark_info", {})
        
#         # å¦‚æœæ²¡æœ‰æŒ‡å®šsplitï¼Œä½¿ç”¨é»˜è®¤split
#         if self.split is None:
#             self.split = self.benchmark_info.get("default_split", "validation")
        
#         # data_pathå‚æ•°åœ¨è¿™é‡Œä¸ä½¿ç”¨ï¼Œä½†éœ€è¦ä¼ é€’ä»¥æ»¡è¶³åŸºç±»è¦æ±‚
#         super().__init__(name, f"hf://{hf_id}/{config}/{self.split}", **kwargs)
#         self.description = f"HuggingFace Benchmark: {name} ({hf_id})"
    
#     def _load_data(self):
#         """ä»HuggingFace HubåŠ è½½æ•°æ®"""
#         try:
#             from datasets import load_dataset
#         except ImportError:
#             raise ImportError(
#                 "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
#             )
        
#         try:
#             # åŠ è½½æ•°æ®é›†
#             if self.config == "default" or self.config is None:
#                 dataset = load_dataset(self.hf_id, split=self.split)
#             else:
#                 dataset = load_dataset(self.hf_id, name=self.config, split=self.split)
            
#             self.tasks = []
            
#             # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
#             for idx, item in enumerate(dataset):
#                 task = self._convert_to_task(item, idx)
#                 if task:
#                     self.tasks.append(task)
        
#         except Exception as e:
#             raise RuntimeError(f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}")
    
#     def _convert_to_task(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
#         """
#         å°†æ•°æ®é›†itemè½¬æ¢ä¸ºBenchmarkTask
        
#         ä¸åŒbenchmarkçš„æ•°æ®æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
#         """
#         task_id = item.get("id", item.get("question_id", f"task_{idx}"))
        
#         # è·å–é—®é¢˜
#         question = item.get("question", item.get("text", ""))
#         if not question:
#             return None
        
#         # è·å–å›¾åƒ
#         images = []
#         if "image" in item:
#             # PIL Imageå¯¹è±¡
#             images.append(item["image"])
#         elif "image_path" in item:
#             images.append(item["image_path"])
#         elif "image_file" in item:
#             images.append(item["image_file"])
#         elif "img" in item:
#             images.append(item["img"])
        
#         # è·å–æ­£ç¡®ç­”æ¡ˆ
#         ground_truth = item.get("answer", item.get("answers", None))
#         if ground_truth is None:
#             ground_truth = item.get("label", item.get("target", ""))
        
#         # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
#         if isinstance(ground_truth, list) and len(ground_truth) > 0:
#             ground_truth = ground_truth[0]
        
#         # æå–metadataï¼ˆåŒ…æ‹¬taxonomyä¿¡æ¯ï¼‰
#         metadata = {}
#         taxonomy_fields = self.benchmark_info.get("native_taxonomy_fields", [])
#         for field in taxonomy_fields:
#             if field in item:
#                 metadata[field] = item[field]
        
#         # ä¿ç•™å…¶ä»–æœ‰ç”¨å­—æ®µ
#         for key in ["question_type", "semantic", "program", "question_family", 
#                    "chart_type", "category", "capability", "task_group"]:
#             if key in item and key not in metadata:
#                 metadata[key] = item[key]
        
#         return BenchmarkTask(
#             task_id=str(task_id),
#             question=str(question),
#             images=images if images else [],
#             ground_truth=ground_truth,
#             metadata=metadata
#         )
    
#     def evaluate_answer(self, 
#                        model_answer: str, 
#                        ground_truth: Any,
#                        task: BenchmarkTask) -> BenchmarkResult:
#         """
#         è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ
        
#         ç®€å•å®ç°ï¼šå­—ç¬¦ä¸²åŒ¹é…
#         å¯ä»¥æ ¹æ®ä¸åŒbenchmarkçš„ç‰¹ç‚¹å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘
#         """
#         model_answer_clean = str(model_answer).strip().lower()
#         ground_truth_clean = str(ground_truth).strip().lower()
        
#         # ç²¾ç¡®åŒ¹é…
#         is_correct = model_answer_clean == ground_truth_clean
        
#         # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœç­”æ¡ˆåŒ…å«å…³é”®è¯ï¼‰
#         if not is_correct:
#             if ground_truth_clean in model_answer_clean or model_answer_clean in ground_truth_clean:
#                 score = 0.5
#             else:
#                 score = 0.0
#         else:
#             score = 1.0
        
#         # å¯ä»¥åœ¨è¿™é‡Œå®ç°ç‰¹å®šbenchmarkçš„è¯„ä¼°é€»è¾‘
#         # ä¾‹å¦‚GQAå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ŒCLEVRéœ€è¦æ‰§è¡Œç¨‹åºç­‰
        
#         return BenchmarkResult(
#             task_id=task.task_id,
#             question=task.question,
#             ground_truth=ground_truth,
#             model_answer=model_answer,
#             is_correct=is_correct,
#             score=score,
#             metadata={"evaluation_method": "exact_match", "benchmark": self.name}
#         )
    
#     def get_info(self) -> Dict[str, Any]:
#         """è·å–benchmarkä¿¡æ¯"""
#         info = super().get_info()
#         info.update({
#             "hf_id": self.hf_id,
#             "config": self.config,
#             "split": self.split,
#             "source": "huggingface",
#             "native_taxonomy_fields": self.benchmark_info.get("native_taxonomy_fields", [])
#         })
#         return info





#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace Benchmarkï¼šä»HuggingFace HubåŠ è½½benchmarkæ•°æ®
"""

from typing import List, Dict, Any, Optional
from .base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


class HuggingFaceBenchmark(BaseBenchmark):
    """ä»HuggingFace HubåŠ è½½çš„Benchmark"""
    
    def __init__(self, 
                 name: str,
                 hf_id: str,
                 config: str = "default",
                 split: str = None,
                 load_data_on_init: bool = False,
                 **kwargs):
        """
        Args:
            name: Benchmarkåç§°
            hf_id: HuggingFaceæ•°æ®é›†IDï¼ˆå®Œæ•´IDï¼Œå¦‚ "lmms-lab/GQA"ï¼‰
            config: æ•°æ®é›†é…ç½®åï¼ˆå¦‚ "balanced", "default"ï¼‰
            split: æ•°æ®é›†splitï¼ˆå¦‚ "validation", "test"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤split
            load_data_on_init: æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶åŠ è½½æ•°æ®ï¼ˆFalseè¡¨ç¤ºå»¶è¿ŸåŠ è½½ï¼Œä½¿ç”¨æµå¼ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬ benchmark_info
        """
        self.hf_id = hf_id
        self.config = config if config and config != "default" else None
        self.benchmark_info = kwargs.get("benchmark_info", {})
        self._dataset = None  # å»¶è¿ŸåŠ è½½æ•°æ®é›†
        self._use_streaming = not load_data_on_init  # ä½¿ç”¨æµå¼åŠ è½½
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šsplitï¼Œä½¿ç”¨benchmark_infoä¸­çš„é»˜è®¤split
        if split is None:
            split = self.benchmark_info.get("default_split", "validation")
        self.split = split
        
        # è·å–å¯ç”¨çš„splitsï¼ˆç”¨äºéªŒè¯ï¼‰
        self.available_splits = self.benchmark_info.get("available_splits", [])
        
        # data_pathå‚æ•°åœ¨è¿™é‡Œä¸ä½¿ç”¨ï¼Œä½†éœ€è¦ä¼ é€’ä»¥æ»¡è¶³åŸºç±»è¦æ±‚
        data_path = f"hf://{self.hf_id}"
        if self.config:
            data_path += f"/{self.config}"
        data_path += f"/{self.split}"
        
        super().__init__(name, data_path, load_data_on_init=load_data_on_init, **kwargs)
        self.description = f"HuggingFace Benchmark: {name} ({self.hf_id})"
    
    # def _load_data(self):
    #     """ä»HuggingFace HubåŠ è½½æ•°æ®"""
    #     try:
    #         from datasets import load_dataset
    #     except ImportError:
    #         raise ImportError(
    #             "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
    #         )
        
    #     try:
    #         print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
    #         print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            
    #         # éªŒè¯splitæ˜¯å¦å¯ç”¨
    #         if self.available_splits and self.split not in self.available_splits:
    #             print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
    #         # åŠ è½½æ•°æ®é›†
    #         load_kwargs = {
    #             "split": self.split,
    #             "trust_remote_code": True
    #         }
            
    #         if self.config:
    #             load_kwargs["name"] = self.config
            
    #         dataset = load_dataset(self.hf_id, **load_kwargs)
            
    #         print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®")
            
    #         self.tasks = []
            
    #         # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
    #         for idx, item in enumerate(dataset):
    #             task = self._convert_to_task(item, idx)
    #             if task:
    #                 self.tasks.append(task)
            
    #         print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
        
    #     except Exception as e:
    #         # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    #         error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
    #         print(f"âœ— {error_msg}")
            
    #         # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
    #         suggestions = []
    #         if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
    #             suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
    #             suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
    #             if self.config:
    #                 suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
    #             suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
    #             if self.available_splits:
    #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
    #         elif "split" in str(e).lower():
    #             suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
    #             if self.available_splits:
    #                 suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
    #             suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
    #         if suggestions:
    #             error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
    #         raise RuntimeError(error_msg)

    def _get_dataset(self):
        """è·å–æ•°æ®é›†ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œæ”¯æŒæµå¼ï¼‰"""
        if self._dataset is not None:
            return self._dataset
        
        try:
            from datasets import load_dataset
        except ImportError:
            raise ImportError(
                "éœ€è¦å®‰è£…datasetsåº“: pip install datasets"
            )
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.hf_id}")
            print(f"   é…ç½®: {self.config or 'default'}, Split: {self.split}")
            print(f"   æ¨¡å¼: {'æµå¼åŠ è½½' if self._use_streaming else 'æ‰¹é‡åŠ è½½'}")
            
            # éªŒè¯splitæ˜¯å¦å¯ç”¨
            if self.available_splits and self.split not in self.available_splits:
                print(f"âš ï¸  è­¦å‘Š: split '{self.split}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ {self.available_splits}")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äº GQA çš„ testdev_balanced_images é…ç½®ï¼Œéœ€è¦åŒæ—¶åŠ è½½é—®é¢˜å’Œç­”æ¡ˆ
            # testdev_balanced_images åªåŒ…å«å›¾åƒï¼Œéœ€è¦ä»å…¶ä»–é…ç½®è·å–é—®é¢˜å’Œç­”æ¡ˆ
            if self.hf_id == "lmms-lab/GQA" and self.config == "testdev_balanced_images":
                print(f"  â„¹ï¸  æ£€æµ‹åˆ° GQA testdev_balanced_images é…ç½®ï¼Œå°†åŒæ—¶åŠ è½½é—®é¢˜å’Œç­”æ¡ˆæ•°æ®")
                # åŠ è½½å›¾åƒæ•°æ®
                image_kwargs = {
                    "split": self.split,
                    "streaming": self._use_streaming,
                    "name": self.config
                }
                print(f"   åŠ è½½å›¾åƒæ•°æ®ï¼Œå‚æ•°: {image_kwargs}")
                image_dataset = load_dataset(self.hf_id, **image_kwargs)
                
                # å°è¯•åŠ è½½åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„æ•°æ®
                # æ ¹æ®é…ç½®åç§°æ¨æ–­å¯¹åº”çš„ instructions é…ç½®
                # testdev_balanced_images -> testdev_balanced_instructions
                base_config = self.config.replace("_images", "")
                print(f"  ğŸ” åŸºç¡€é…ç½®: {base_config} (ä» {self.config} æå–)")
                
                # æ„å»ºå¯èƒ½çš„é…ç½®åˆ—è¡¨
                question_configs = [
                    f"{base_config}_instructions",  # testdev_balanced_instructions
                ]
                # å¦‚æœbase_configåŒ…å«_balancedï¼Œä¹Ÿå°è¯•_all_instructions
                if "_balanced" in base_config:
                    split_name = base_config.split("_")[0]  # testdev
                    question_configs.append(f"{split_name}_all_instructions")  # testdev_all_instructions
                
                print(f"  ğŸ” å°†å°è¯•ä»¥ä¸‹é…ç½®: {question_configs}")
                question_dataset = None
                
                for q_config in question_configs:
                    try:
                        # å¯¹äº instructions é…ç½®ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„ split å¤„ç†
                        # å…ˆå°è¯•ä½¿ç”¨ç›¸åŒçš„ splitï¼Œå¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ splitï¼ˆè®©æ•°æ®é›†è‡ªå·±å†³å®šï¼‰
                        question_kwargs = {
                            "split": self.split,
                            "streaming": self._use_streaming,
                            "name": q_config
                        }
                        
                        print(f"   å°è¯•åŠ è½½é—®é¢˜å’Œç­”æ¡ˆæ•°æ®ï¼Œé…ç½®: {q_config}, å‚æ•°: {question_kwargs}")
                        try:
                            temp_dataset = load_dataset(self.hf_id, **question_kwargs)
                        except Exception as split_error:
                            # å¦‚æœä½¿ç”¨ split å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ split
                            print(f"   ä½¿ç”¨ split={self.split} å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ split: {split_error}")
                            question_kwargs_no_split = {
                                "streaming": self._use_streaming,
                                "name": q_config
                            }
                            temp_dataset = load_dataset(self.hf_id, **question_kwargs_no_split)
                            # å¦‚æœæˆåŠŸï¼Œæ›´æ–° question_kwargs ä»¥ä¾¿åç»­ä½¿ç”¨
                            question_kwargs = question_kwargs_no_split
                        
                        # æ£€æŸ¥ç¬¬ä¸€ä¸ªitemæ˜¯å¦åŒ…å«questionå­—æ®µ
                        if self._use_streaming:
                            # æµå¼æ¨¡å¼ï¼šåˆ›å»ºè¿­ä»£å™¨å¹¶æ£€æŸ¥ç¬¬ä¸€ä¸ªitem
                            temp_iter = iter(temp_dataset)
                            test_item = next(temp_iter)
                            # é‡æ–°åˆ›å»ºæ•°æ®é›†ï¼ˆå› ä¸ºè¿­ä»£å™¨å·²è¢«æ¶ˆè€—ï¼‰
                            question_dataset = load_dataset(self.hf_id, **question_kwargs)
                        else:
                            # éæµå¼æ¨¡å¼ï¼šç›´æ¥æ£€æŸ¥
                            test_item = temp_dataset[0] if len(temp_dataset) > 0 else {}
                            question_dataset = temp_dataset
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ç›¸å…³çš„å­—æ®µ
                        has_question_field = any(field in test_item for field in [
                            "question", "text", "sent", "sentence", "instruction", 
                            "prompt", "input", "query", "question_text"
                        ])
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç­”æ¡ˆå­—æ®µ
                        has_answer_field = any(field in test_item for field in [
                            "answer", "answers", "label", "target", "gt_answer", "ground_truth"
                        ])
                        
                        if has_question_field or has_answer_field:
                            print(f"  âœ“ æ‰¾åˆ°åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„é…ç½®: {q_config}")
                            print(f"     åŒ…å«çš„å­—æ®µ: {list(test_item.keys())}")
                            break
                        else:
                            print(f"  âš ï¸  é…ç½® {q_config} ä¸åŒ…å«é—®é¢˜æˆ–ç­”æ¡ˆå­—æ®µï¼Œå­—æ®µ: {list(test_item.keys())}")
                            question_dataset = None
                    except Exception as e:
                        print(f"  âš ï¸  é…ç½® {q_config or 'default'} åŠ è½½å¤±è´¥: {e}")
                        question_dataset = None
                        continue
                
                if question_dataset:
                    # åˆå¹¶æ•°æ®é›†ï¼šå°†é—®é¢˜å’Œç­”æ¡ˆåˆå¹¶åˆ°å›¾åƒæ•°æ®ä¸­
                    self._dataset = self._merge_gqa_datasets(image_dataset, question_dataset)
                    print(f"âœ“ GQA æ•°æ®é›†åˆå¹¶å®Œæˆ")
                else:
                    print(f"  âš ï¸  æ— æ³•æ‰¾åˆ°åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„é…ç½®")
                    print(f"  â„¹ï¸  å°†ä»…ä½¿ç”¨å›¾åƒæ•°æ®ï¼ˆå¯èƒ½æ— æ³•è·å–é—®é¢˜å’Œç­”æ¡ˆï¼‰")
                    self._dataset = image_dataset
            else:
                # æ™®é€šåŠ è½½æ–¹å¼
                load_kwargs = {
                    "split": self.split,
                    "streaming": self._use_streaming
                }
                
                if self.config:
                    load_kwargs["name"] = self.config
                
                print(f"   åŠ è½½å‚æ•°: {load_kwargs}")
                self._dataset = load_dataset(self.hf_id, **load_kwargs)
            
            if not self._use_streaming:
                print(f"âœ“ æˆåŠŸåŠ è½½ {len(self._dataset)} æ¡æ•°æ®")
            else:
                print(f"âœ“ æµå¼æ•°æ®é›†å·²å°±ç»ª")
            
            return self._dataset
        
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = f"ä»HuggingFaceåŠ è½½æ•°æ®å¤±è´¥ ({self.hf_id}): {e}"
            print(f"âœ— {error_msg}")
            
            # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
            suggestions = []
            if "doesn't exist" in str(e) or "cannot be accessed" in str(e):
                suggestions.append(f"â€¢ æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®: {self.hf_id}")
                suggestions.append(f"â€¢ è®¿é—® https://huggingface.co/datasets/{self.hf_id} ç¡®è®¤æ•°æ®é›†å­˜åœ¨")
                if self.config:
                    suggestions.append(f"â€¢ æ£€æŸ¥é…ç½®åç§°æ˜¯å¦æ­£ç¡®: {self.config}")
                suggestions.append(f"â€¢ æ£€æŸ¥splitæ˜¯å¦æ­£ç¡®: {self.split}")
                if self.available_splits:
                    suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
            elif "Config name is missing" in str(e):
                suggestions.append(f"â€¢ æ•°æ®é›†éœ€è¦æŒ‡å®šconfigåç§°")
                suggestions.append(f"â€¢ å½“å‰configè®¾ç½®: {self.config or 'None'}")
                suggestions.append(f"â€¢ è¯·åœ¨configæ–‡ä»¶ä¸­ä¸ºæ­¤benchmarkæŒ‡å®šæ­£ç¡®çš„configåç§°")
                import re
                configs_match = re.search(r"available configs: (\[.*?\])", str(e))
                if configs_match:
                    suggestions.append(f"â€¢ å¯ç”¨çš„configs: {configs_match.group(1)}")
            elif "split" in str(e).lower():
                suggestions.append(f"â€¢ å½“å‰ä½¿ç”¨çš„split: {self.split}")
                if self.available_splits:
                    suggestions.append(f"â€¢ å¯ç”¨çš„splits: {self.available_splits}")
                suggestions.append(f"â€¢ å°è¯•ä½¿ç”¨å…¶ä»–splitï¼Œæˆ–åœ¨configä¸­æ›´æ–°available_splits")
            
            if suggestions:
                error_msg += "\nå»ºè®®:\n" + "\n".join(suggestions)
            
            raise RuntimeError(error_msg)
    
    def _merge_gqa_datasets(self, image_dataset, question_dataset):
        """
        åˆå¹¶ GQA çš„å›¾åƒæ•°æ®é›†å’Œé—®é¢˜æ•°æ®é›†
        
        Args:
            image_dataset: åŒ…å«å›¾åƒçš„æ•°æ®é›†ï¼ˆåªæœ‰ id å’Œ imageï¼‰
            question_dataset: åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„æ•°æ®é›†ï¼ˆæœ‰ id, question, answer ç­‰ï¼‰
        
        Returns:
            åˆå¹¶åçš„æ•°æ®é›†è¿­ä»£å™¨
        """
        if self._use_streaming:
            # æµå¼æ¨¡å¼ï¼šåˆ›å»ºä¸€ä¸ªåˆå¹¶è¿­ä»£å™¨
            # å¯¹äºæµå¼æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ„å»ºä¸€ä¸ª ID åˆ°é—®é¢˜çš„æ˜ å°„
            # ä½†ç”±äºæ˜¯æµå¼çš„ï¼Œæˆ‘ä»¬éœ€è¦è¾¹è¿­ä»£è¾¹åŒ¹é…
            
            class MergedDatasetIterator:
                def __init__(self, image_iter, question_iter):
                    self.image_iter = iter(image_iter)
                    self.question_iter = iter(question_iter)
                    # é¢„åŠ è½½ä¸€äº›é—®é¢˜æ•°æ®åˆ°å†…å­˜ï¼ˆç”¨äºåŒ¹é…ï¼‰
                    self.question_cache = {}
                    self._preload_questions()
                
                def _preload_questions(self):
                    """é¢„åŠ è½½ä¸€äº›é—®é¢˜æ•°æ®åˆ°ç¼“å­˜"""
                    try:
                        count = 0
                        for item in self.question_iter:
                            # ä½¿ç”¨ imageId ä½œä¸ºé”®ï¼Œå› ä¸ºå›¾åƒæ•°æ®é›†çš„ id å¯¹åº”é—®é¢˜æ•°æ®é›†çš„ imageId
                            image_id = item.get("imageId", item.get("image_id", ""))
                            if image_id:
                                # å¦‚æœåŒä¸€ä¸ªå›¾åƒæœ‰å¤šä¸ªé—®é¢˜ï¼Œå­˜å‚¨ä¸ºåˆ—è¡¨
                                if image_id not in self.question_cache:
                                    self.question_cache[image_id] = []
                                self.question_cache[image_id].append(item)
                                count += 1
                            # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
                            if count >= 50000:  # å¢åŠ ç¼“å­˜å¤§å°ä»¥è¦†ç›–æ›´å¤šæ•°æ®
                                break
                        print(f"  âœ“ é¢„åŠ è½½äº† {count} ä¸ªé—®é¢˜åˆ°ç¼“å­˜ï¼Œè¦†ç›– {len(self.question_cache)} ä¸ªå›¾åƒ")
                    except StopIteration:
                        print(f"  âœ“ é¢„åŠ è½½å®Œæˆï¼Œå…± {count} ä¸ªé—®é¢˜ï¼Œè¦†ç›– {len(self.question_cache)} ä¸ªå›¾åƒ")
                    except Exception as e:
                        print(f"  âš ï¸  é¢„åŠ è½½é—®é¢˜æ—¶å‡ºé”™: {e}")
                
                def __iter__(self):
                    return self
                
                def __next__(self):
                    # è·å–ä¸‹ä¸€ä¸ªå›¾åƒé¡¹
                    image_item = next(self.image_iter)
                    image_id = image_item.get("id", "")
                    
                    # å°è¯•ä»ç¼“å­˜ä¸­æ‰¾åˆ°åŒ¹é…çš„é—®é¢˜ï¼ˆä½¿ç”¨ imageId åŒ¹é…ï¼‰
                    if image_id in self.question_cache:
                        question_items = self.question_cache[image_id]
                        # å¦‚æœæœ‰å¤šä¸ªé—®é¢˜ï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆæˆ–è€…å¯ä»¥éšæœºé€‰æ‹©ï¼‰
                        question_item = question_items[0] if isinstance(question_items, list) else question_items
                        # åˆå¹¶æ•°æ®
                        merged_item = {**image_item}
                        merged_item.update(question_item)
                        return merged_item
                    else:
                        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰åŒ¹é…çš„é—®é¢˜ï¼Œè¿”å›å›¾åƒé¡¹ï¼ˆä½†ä¼šå› ä¸ºæ²¡æœ‰questionè€Œè¢«è·³è¿‡ï¼‰
                        return image_item
            
            return MergedDatasetIterator(image_dataset, question_dataset)
        else:
            # éæµå¼æ¨¡å¼ï¼šæ„å»º ID æ˜ å°„å¹¶åˆå¹¶
            # ä½¿ç”¨ imageId ä½œä¸ºé”®ï¼Œå› ä¸ºå›¾åƒæ•°æ®é›†çš„ id å¯¹åº”é—®é¢˜æ•°æ®é›†çš„ imageId
            question_dict = {}
            for item in question_dataset:
                image_id = item.get("imageId", item.get("image_id", ""))
                if image_id:
                    # å¦‚æœåŒä¸€ä¸ªå›¾åƒæœ‰å¤šä¸ªé—®é¢˜ï¼Œå­˜å‚¨ä¸ºåˆ—è¡¨
                    if image_id not in question_dict:
                        question_dict[image_id] = []
                    question_dict[image_id].append(item)
            
            # åˆå¹¶æ•°æ®
            merged_data = []
            for image_item in image_dataset:
                image_id = image_item.get("id", "")
                merged_item = {**image_item}
                if image_id in question_dict:
                    # å¦‚æœæœ‰å¤šä¸ªé—®é¢˜ï¼Œå–ç¬¬ä¸€ä¸ª
                    question_items = question_dict[image_id]
                    question_item = question_items[0] if isinstance(question_items, list) else question_items
                    merged_item.update(question_item)
                merged_data.append(merged_item)
            
            return merged_data
    
    def _load_data(self):
        """ä»HuggingFace HubåŠ è½½æ•°æ®ï¼ˆä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰"""
        dataset = self._get_dataset()
        
        if self._use_streaming:
            # æµå¼åŠ è½½æ¨¡å¼ï¼Œä¸é¢„åŠ è½½æ‰€æœ‰æ•°æ®
            return
        
        self.tasks = []
        
        # å°†æ•°æ®é›†è½¬æ¢ä¸ºBenchmarkTaskåˆ—è¡¨
        for idx, item in enumerate(dataset):
            task = self._convert_to_task(item, idx)
            if task:
                self.tasks.append(task)
        
        print(f"âœ“ è½¬æ¢å®Œæˆï¼Œå…± {len(self.tasks)} ä¸ªä»»åŠ¡")
    
    def get_dataset_iterator(self):
        """
        è·å–æ•°æ®é›†è¿­ä»£å™¨ï¼ˆç”¨äºæµå¼å¤„ç†ï¼‰
        
        Returns:
            æ•°æ®é›†è¿­ä»£å™¨
        """
        dataset = self._get_dataset()
        
        if self._use_streaming:
            # æµå¼æ•°æ®é›†ï¼Œç›´æ¥è¿”å›è¿­ä»£å™¨
            if hasattr(dataset, '__iter__'):
                return iter(dataset)
            else:
                # å¦‚æœä¸æ˜¯è¿­ä»£å™¨ï¼Œå°è¯•è½¬æ¢ä¸ºè¿­ä»£å™¨
                return iter(dataset)
        else:
            # æ™®é€šæ•°æ®é›†ï¼Œè¿”å›åˆ—è¡¨è¿­ä»£å™¨
            if self.tasks:
                return iter(self.tasks)
            else:
                # å¦‚æœæ²¡æœ‰é¢„åŠ è½½çš„ä»»åŠ¡ï¼Œä»æ•°æ®é›†åˆ›å»º
                return iter(dataset)
    
    def get_task_from_item(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
        """
        ä»æ•°æ®é›†itemåˆ›å»ºBenchmarkTaskï¼ˆç”¨äºæµå¼å¤„ç†ï¼‰
        
        Args:
            item: æ•°æ®é›†ä¸­çš„ä¸€ä¸ªitem
            idx: ç´¢å¼•
        
        Returns:
            BenchmarkTaskæˆ–None
        """
        return self._convert_to_task(item, idx)
    
    def _convert_to_task(self, item: Dict[str, Any], idx: int) -> Optional[BenchmarkTask]:
        """
        å°†æ•°æ®é›†itemè½¬æ¢ä¸ºBenchmarkTask
        
        ä¸åŒbenchmarkçš„æ•°æ®æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
        """
        task_id = item.get("id", item.get("question_id", item.get("questionId", f"task_{idx}")))
        
        # è·å–é—®é¢˜ - æ”¯æŒå¤šç§å­—æ®µåï¼ˆåŒ…æ‹¬GQAå¯èƒ½ä½¿ç”¨çš„å­—æ®µï¼‰
        question = None
        question_fields = ["question", "text", "query", "sent", "sentence", "prompt", "input"]
        for field in question_fields:
            if field in item and item[field] is not None:
                question_value = item[field]
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ä¸”éç©ºï¼Œä½¿ç”¨å®ƒ
                if isinstance(question_value, str) and question_value.strip():
                    question = question_value
                    break
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªéç©ºå­—ç¬¦ä¸²
                elif isinstance(question_value, list) and len(question_value) > 0:
                    first_item = question_value[0]
                    if isinstance(first_item, str) and first_item.strip():
                        question = first_item
                        break
        
        if not question:
            # è°ƒè¯•ï¼šæ‰“å°itemçš„é”®ä»¥å¸®åŠ©è¯Šæ–­é—®é¢˜
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                print(f"  ğŸ” è°ƒè¯•: item (idx={idx}) çš„å­—æ®µ: {list(item.keys())}")
                # æ‰“å°æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡æœ¬çš„å­—æ®µçš„å€¼
                for key in item.keys():
                    value = item[key]
                    if isinstance(value, str) and len(value) > 0:
                        print(f"    - {key}: {value[:100]}...")
                    elif isinstance(value, list) and len(value) > 0:
                        print(f"    - {key}: {type(value[0])} list with {len(value)} items")
            return None
        
        # è·å–å›¾åƒ - æ”¯æŒå¤šç§å­—æ®µå
        images = []
        image_fields = ["image", "img", "image_path", "image_file", "imageId"]
        for field in image_fields:
            if field in item and item[field] is not None:
                images.append(item[field])
                break  # åªå–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾åƒå­—æ®µ
        
        # è·å–æ­£ç¡®ç­”æ¡ˆ - æ”¯æŒå¤šç§å­—æ®µå
        ground_truth = None
        answer_fields = ["answer", "answers", "label", "target", "gt_answer"]
        for field in answer_fields:
            if field in item and item[field] is not None:
                ground_truth = item[field]
                break
        
        # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
        if isinstance(ground_truth, list) and len(ground_truth) > 0:
            ground_truth = ground_truth[0]
        
        # æå–metadataï¼ˆåŒ…æ‹¬taxonomyä¿¡æ¯ï¼‰
        metadata = {}
        
        # 1. é¦–å…ˆæå–configä¸­å®šä¹‰çš„native_taxonomy_fields
        taxonomy_fields = self.benchmark_info.get("native_taxonomy_fields", [])
        for field in taxonomy_fields:
            if field in item:
                metadata[field] = item[field]
        
        # 2. ç„¶åæå–å…¶ä»–å¸¸è§çš„æœ‰ç”¨å­—æ®µï¼ˆå¦‚æœä¸åœ¨taxonomyä¸­ï¼‰
        additional_fields = [
            "question_type", "semantic", "program", "question_family", 
            "chart_type", "category", "capability", "task_group",
            "difficulty", "skill", "domain", "task", "subcategory"
        ]
        for key in additional_fields:
            if key in item and key not in metadata:
                metadata[key] = item[key]
        
        # 3. æ·»åŠ benchmarké…ç½®ä¸­çš„noteï¼ˆå¦‚æœæœ‰ï¼‰
        if "note" in self.benchmark_info:
            metadata["benchmark_note"] = self.benchmark_info["note"]
        
        return BenchmarkTask(
            task_id=str(task_id),
            question=str(question),
            images=images if images else [],
            ground_truth=ground_truth,
            metadata=metadata
        )
    
    def evaluate_answer(self, 
                       model_answer: str, 
                       ground_truth: Any,
                       task: BenchmarkTask) -> BenchmarkResult:
        """
        è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ
        
        ç®€å•å®ç°ï¼šå­—ç¬¦ä¸²åŒ¹é…
        å¯ä»¥æ ¹æ®ä¸åŒbenchmarkçš„ç‰¹ç‚¹å®ç°æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘
        """
        model_answer_clean = str(model_answer).strip().lower()
        ground_truth_clean = str(ground_truth).strip().lower()
        
        # ç²¾ç¡®åŒ¹é…
        is_correct = model_answer_clean == ground_truth_clean
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœç­”æ¡ˆåŒ…å«å…³é”®è¯ï¼‰
        if not is_correct:
            if ground_truth_clean in model_answer_clean or model_answer_clean in ground_truth_clean:
                score = 0.5
            else:
                score = 0.0
        else:
            score = 1.0
        
        # å¯ä»¥åœ¨è¿™é‡Œå®ç°ç‰¹å®šbenchmarkçš„è¯„ä¼°é€»è¾‘
        # ä¾‹å¦‚GQAå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ŒCLEVRéœ€è¦æ‰§è¡Œç¨‹åºç­‰
        
        return BenchmarkResult(
            task_id=task.task_id,
            question=task.question,
            ground_truth=ground_truth,
            model_answer=model_answer,
            is_correct=is_correct,
            score=score,
            metadata={"evaluation_method": "exact_match", "benchmark": self.name}
        )
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–benchmarkä¿¡æ¯"""
        info = super().get_info()
        info.update({
            "hf_id": self.hf_id,
            "config": self.config,
            "split": self.split,
            "available_splits": self.available_splits,
            "source": "huggingface",
            "native_taxonomy_fields": self.benchmark_info.get("native_taxonomy_fields", []),
            "note": self.benchmark_info.get("note", ""),
            "use_streaming": self._use_streaming
        })
        
        # å¦‚æœæ˜¯æµå¼åŠ è½½ï¼Œnum_taskså¯èƒ½æœªçŸ¥
        if self._use_streaming and self._dataset is None:
            info["num_tasks"] = "unknown (streaming)"
        
        return info