# # #!/usr/bin/env python3
# # # -*- coding: utf-8 -*-
# # """
# # æµ‹è¯•æ‰§è¡Œå™¨ï¼šæ‰§è¡Œbenchmarkæµ‹è¯•
# # """

# # import time
# # from typing import List, Dict, Any, Optional

# # try:
# #     from tqdm import tqdm
# #     HAS_TQDM = True
# # except ImportError:
# #     HAS_TQDM = False
# #     # ç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
# #     def tqdm(iterable, desc=""):
# #         print(f"{desc}...")
# #         return iterable

# # import sys
# # from pathlib import Path

# # # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
# # current_dir = Path(__file__).parent.parent
# # sys.path.insert(0, str(current_dir))

# # from .model_adapter import BaseModelAdapter
# # from ..benchmarks.base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# # class TestExecutor:
# #     """æµ‹è¯•æ‰§è¡Œå™¨"""
    
# #     def __init__(self, model_adapter: BaseModelAdapter):
# #         self.model_adapter = model_adapter
# #         self.model_info = model_adapter.get_model_info()
    
# #     def run_benchmark(self, 
# #                      benchmark: BaseBenchmark,
# #                      max_samples: Optional[int] = None,
# #                      verbose: bool = True) -> List[BenchmarkResult]:
# #         """
# #         è¿è¡Œå•ä¸ªbenchmarkæµ‹è¯•
        
# #         Args:
# #             benchmark: Benchmarkå®ä¾‹
# #             max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
# #             verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
# #         Returns:
# #             æµ‹è¯•ç»“æœåˆ—è¡¨
# #         """
# #         tasks = benchmark.get_tasks()
# #         if max_samples:
# #             tasks = tasks[:max_samples]
        
# #         results = []
        
# #         if verbose:
# #             tasks_iter = tqdm(tasks, desc=f"Testing {benchmark.name}")
# #         else:
# #             tasks_iter = tasks
        
# #         for task in tasks_iter:
# #             try:
# #                 # æ„å»ºæç¤º
# #                 prompt = self._build_prompt(task)
                
# #                 # è°ƒç”¨æ¨¡å‹
# #                 model_response = self.model_adapter.generate(
# #                     prompt=prompt,
# #                     images=task.images
# #                 )
                
# #                 # è¯„ä¼°ç­”æ¡ˆ
# #                 result = benchmark.evaluate_answer(
# #                     model_answer=model_response.get("text", ""),
# #                     ground_truth=task.ground_truth,
# #                     task=task
# #                 )
                
# #                 results.append(result)
                
# #             except Exception as e:
# #                 # è®°å½•é”™è¯¯
# #                 result = BenchmarkResult(
# #                     task_id=task.task_id,
# #                     question=task.question,
# #                     ground_truth=task.ground_truth,
# #                     model_answer="",
# #                     is_correct=False,
# #                     score=0.0,
# #                     metadata={"error": str(e)}
# #                 )
# #                 results.append(result)
        
# #         return results
    
# #     def _build_prompt(self, task: BenchmarkTask) -> str:
# #         """æ„å»ºæç¤º"""
# #         # åŸºæœ¬æç¤ºæ ¼å¼
# #         prompt = task.question
# #         return prompt







# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# æµ‹è¯•æ‰§è¡Œå™¨ï¼šæ‰§è¡Œbenchmarkæµ‹è¯•
# """

# import time
# from typing import List, Dict, Any, Optional

# try:
#     from tqdm import tqdm
#     HAS_TQDM = True
# except ImportError:
#     HAS_TQDM = False
#     # ç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
#     def tqdm(iterable, desc=""):
#         print(f"{desc}...")
#         return iterable

# import sys
# from pathlib import Path

# # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
# current_dir = Path(__file__).parent.parent
# sys.path.insert(0, str(current_dir))

# from .model_adapter import BaseModelAdapter
# from ..benchmarks.base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# class TestExecutor:
#     """æµ‹è¯•æ‰§è¡Œå™¨"""
    
#     def __init__(self, model_adapter: BaseModelAdapter, verbose: bool = False):
#         self.model_adapter = model_adapter
#         self.model_info = model_adapter.get_model_info()
#         self._verbose = verbose  # è¯¦ç»†è¾“å‡ºæ ‡å¿—
    
#     def run_benchmark(self, 
#                      benchmark: BaseBenchmark,
#                      max_samples: Optional[int] = None,
#                      batch_size: int = 1,
#                      verbose: bool = True) -> List[BenchmarkResult]:
#         """
#         è¿è¡Œå•ä¸ªbenchmarkæµ‹è¯•ï¼ˆæ”¯æŒæµå¼å’Œæ‰¹é‡å¤„ç†ï¼‰
        
#         Args:
#             benchmark: Benchmarkå®ä¾‹
#             max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
#             batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆ1è¡¨ç¤ºé€ä¸ªå¤„ç†ï¼Œ>1è¡¨ç¤ºæ‰¹é‡å¤„ç†ï¼‰
#             verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
#         Returns:
#             æµ‹è¯•ç»“æœåˆ—è¡¨
#         """
#         # æ›´æ–°è¯¦ç»†è¾“å‡ºæ ‡å¿—
#         self._verbose = verbose
        
#         if verbose:
#             print(f"\n  ğŸš€ å¼€å§‹æµ‹è¯• Benchmark: {benchmark.name}")
#             print(f"     æ¨¡å¼: {'æµå¼' if hasattr(benchmark, '_use_streaming') and benchmark._use_streaming else 'æ‰¹é‡'}")
#             print(f"     æ‰¹å¤§å°: {batch_size}, æœ€å¤§æ ·æœ¬: {max_samples or 'æ— é™åˆ¶'}")
        
#         # æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼åŠ è½½
#         use_streaming = hasattr(benchmark, 'get_dataset_iterator') and hasattr(benchmark, 'get_task_from_item')
        
#         if use_streaming and hasattr(benchmark, '_use_streaming') and benchmark._use_streaming:
#             # æµå¼å¤„ç†æ¨¡å¼
#             results = self._run_benchmark_streaming(
#                 benchmark, max_samples, batch_size, verbose
#             )
#         else:
#             # æ™®é€šæ¨¡å¼ï¼ˆä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ä»»åŠ¡ï¼‰
#             if verbose:
#                 print(f"  ğŸ“¦ åŠ è½½æ‰€æœ‰ä»»åŠ¡...")
            
#             tasks = benchmark.get_tasks()
#             if max_samples:
#                 tasks = tasks[:max_samples]
            
#             if verbose:
#                 print(f"  âœ“ åŠ è½½äº† {len(tasks)} ä¸ªä»»åŠ¡")
            
#             if verbose:
#                 tasks_iter = tqdm(tasks, desc=f"Testing {benchmark.name}", unit="task")
#             else:
#                 tasks_iter = tasks
            
#             results = self._process_tasks(benchmark, tasks_iter, batch_size)
        
#         if verbose:
#             correct_count = sum(1 for r in results if r.is_correct)
#             error_count = sum(1 for r in results if r.metadata and 'error' in r.metadata)
#             print(f"  âœ… æµ‹è¯•å®Œæˆ: {correct_count}/{len(results)} æ­£ç¡®, {error_count} é”™è¯¯")
        
#         return results
    
#     def _run_benchmark_streaming(self,
#                                 benchmark: BaseBenchmark,
#                                 max_samples: Optional[int],
#                                 batch_size: int,
#                                 verbose: bool) -> List[BenchmarkResult]:
#         """æµå¼å¤„ç†benchmark"""
#         results = []
        
#         if verbose:
#             print(f"  ğŸ“Š å¼€å§‹æµå¼å¤„ç†ï¼Œbatch_size={batch_size}, max_samples={max_samples or 'all'}")
        
#         try:
#             dataset_iter = benchmark.get_dataset_iterator()
#         except Exception as e:
#             print(f"  âœ— è·å–æ•°æ®é›†è¿­ä»£å™¨å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
#             return results
        
#         processed_count = 0
#         skipped_count = 0
#         error_count = 0
        
#         # åˆ›å»ºè¿›åº¦æ¡
#         if verbose:
#             if max_samples:
#                 pbar = tqdm(total=max_samples, desc=f"Testing {benchmark.name} (streaming)", unit="task")
#             else:
#                 pbar = tqdm(desc=f"Testing {benchmark.name} (streaming)", unit="task")
        
#         try:
#             batch_tasks = []
#             batch_indices = []
            
#             idx = 0
#             while True:
#                 try:
#                     # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°
#                     if max_samples and processed_count >= max_samples:
#                         if verbose:
#                             print(f"\n  âœ“ å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ ({max_samples})")
#                         break
                    
#                     # å°è¯•è·å–ä¸‹ä¸€ä¸ªitem
#                     try:
#                         item = next(dataset_iter)
#                     except StopIteration:
#                         if verbose:
#                             print(f"\n  âœ“ æ•°æ®é›†è¿­ä»£å®Œæˆ (å…±å¤„ç† {idx} ä¸ªitems)")
#                         break
#                     except Exception as e:
#                         print(f"  âœ— è·å–æ•°æ®é¡¹å¤±è´¥ (idx={idx}): {e}")
#                         error_count += 1
#                         if verbose and error_count <= 10:
#                             import traceback
#                             traceback.print_exc()
#                         idx += 1
#                         continue
                    
#                     try:
#                         # ä»itemåˆ›å»ºtask
#                         task = benchmark.get_task_from_item(item, idx)
#                         if task is None:
#                             skipped_count += 1
#                             if verbose and skipped_count <= 5:
#                                 print(f"  âš ï¸  è·³è¿‡æ— æ•ˆitem (idx={idx})")
#                             idx += 1
#                             continue
                        
#                         batch_tasks.append(task)
#                         batch_indices.append(idx)
                        
#                         # è¾¾åˆ°batch_sizeæ—¶å¤„ç†ä¸€æ‰¹
#                         if len(batch_tasks) >= batch_size:
#                             if verbose:
#                                 print(f"  ğŸ”„ å¤„ç†æ‰¹æ¬¡ (size={len(batch_tasks)}, items {batch_indices[0]}-{batch_indices[-1]})")
                            
#                             try:
#                                 batch_results = self._process_task_batch(benchmark, batch_tasks)
#                                 results.extend(batch_results)
#                                 processed_count += len(batch_results)
                                
#                                 # ç»Ÿè®¡æ‰¹æ¬¡ä¸­çš„æ­£ç¡®/é”™è¯¯æ•°é‡
#                                 batch_correct = sum(1 for r in batch_results if r.is_correct)
#                                 batch_errors = sum(1 for r in batch_results if r.metadata and 'error' in r.metadata)
#                                 error_count += batch_errors
                                
#                                 if verbose:
#                                     pbar.update(len(batch_results))
#                                     print(f"    æ‰¹æ¬¡ç»“æœ: {batch_correct}/{len(batch_results)} æ­£ç¡®, {batch_errors} é”™è¯¯")
                            
#                             except Exception as e:
#                                 error_count += len(batch_tasks)
#                                 print(f"  âœ— æ‰¹æ¬¡å¤„ç†å¤±è´¥ (items {batch_indices[0]}-{batch_indices[-1]}): {e}")
#                                 if verbose:
#                                     import traceback
#                                     traceback.print_exc()
#                                 # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡åˆ›å»ºé”™è¯¯ç»“æœ
#                                 for task in batch_tasks:
#                                     error_result = BenchmarkResult(
#                                         task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
#                                         question=task.question if hasattr(task, 'question') else "",
#                                         ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
#                                         model_answer="",
#                                         is_correct=False,
#                                         score=0.0,
#                                         metadata={"error": str(e)}
#                                     )
#                                     results.append(error_result)
                            
#                             batch_tasks = []
#                             batch_indices = []
                    
#                     except Exception as e:
#                         skipped_count += 1
#                         error_count += 1
#                         print(f"  âœ— å¤„ç†itemå¤±è´¥ (idx={idx}): {e}")
#                         if verbose and error_count <= 10:
#                             import traceback
#                             traceback.print_exc()
                    
#                     idx += 1
                
#                 except KeyboardInterrupt:
#                     raise
                
#                 except StopIteration:
#                     break
                
#                 except Exception as e:
#                     print(f"  âœ— è¿­ä»£è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ (idx={idx}): {e}")
#                     if verbose:
#                         import traceback
#                         traceback.print_exc()
#                     idx += 1
#                     continue
            
#             # å¤„ç†å‰©ä½™çš„task
#             if batch_tasks:
#                 if verbose:
#                     print(f"  ğŸ”„ å¤„ç†æœ€åä¸€æ‰¹ (size={len(batch_tasks)}, items {batch_indices[0]}-{batch_indices[-1]})")
                
#                 try:
#                     batch_results = self._process_task_batch(benchmark, batch_tasks)
#                     results.extend(batch_results)
#                     processed_count += len(batch_results)
                    
#                     batch_correct = sum(1 for r in batch_results if r.is_correct)
#                     batch_errors = sum(1 for r in batch_results if r.metadata and 'error' in r.metadata)
#                     error_count += batch_errors
                    
#                     if verbose:
#                         pbar.update(len(batch_results))
#                         print(f"    æ‰¹æ¬¡ç»“æœ: {batch_correct}/{len(batch_results)} æ­£ç¡®, {batch_errors} é”™è¯¯")
                
#                 except Exception as e:
#                     error_count += len(batch_tasks)
#                     print(f"  âœ— æœ€åä¸€æ‰¹å¤„ç†å¤±è´¥: {e}")
#                     import traceback
#                     traceback.print_exc()
#                     # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡åˆ›å»ºé”™è¯¯ç»“æœ
#                     for task in batch_tasks:
#                         error_result = BenchmarkResult(
#                             task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
#                             question=task.question if hasattr(task, 'question') else "",
#                             ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
#                             model_answer="",
#                             is_correct=False,
#                             score=0.0,
#                             metadata={"error": str(e)}
#                         )
#                         results.append(error_result)
        
#         except KeyboardInterrupt:
#             if verbose:
#                 print(f"\n  âš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
#             raise
        
#         except Exception as e:
#             print(f"  âœ— æµå¼å¤„ç†å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
#             import traceback
#             traceback.print_exc()
        
#         finally:
#             if verbose:
#                 pbar.close()
#                 print(f"  ğŸ“Š æµå¼å¤„ç†å®Œæˆ:")
#                 print(f"    æ€»å¤„ç†æ•°: {processed_count}")
#                 print(f"    è·³è¿‡æ•°: {skipped_count}")
#                 print(f"    é”™è¯¯æ•°: {error_count}")
#                 print(f"    æˆåŠŸç»“æœ: {len(results)}")
        
#         return results
    
#     def _process_tasks(self, benchmark: BaseBenchmark, tasks_iter, batch_size: int) -> List[BenchmarkResult]:
#         """å¤„ç†ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰"""
#         if batch_size > 1:
#             # æ‰¹é‡å¤„ç†
#             tasks_list = list(tasks_iter)
#             results = []
#             for i in range(0, len(tasks_list), batch_size):
#                 batch = tasks_list[i:i+batch_size]
#                 batch_results = self._process_task_batch(benchmark, batch)
#                 results.extend(batch_results)
#             return results
#         else:
#             # é€ä¸ªå¤„ç†
#             return self._process_task_batch(benchmark, tasks_iter)
    
#     def _process_task_batch(self, benchmark: BaseBenchmark, tasks) -> List[BenchmarkResult]:
#         """å¤„ç†ä¸€æ‰¹ä»»åŠ¡"""
#         results = []
        
#         for i, task in enumerate(tasks):
#             try:
#                 # æ„å»ºæç¤º
#                 prompt = self._build_prompt(task)
                
#                 # è°ƒç”¨æ¨¡å‹
#                 if hasattr(self, '_verbose') and self._verbose:
#                     print(f"      [{i+1}/{len(tasks)}] å¤„ç†ä»»åŠ¡: {task.task_id[:50]}...")
                
#                 model_response = self.model_adapter.generate(
#                     prompt=prompt,
#                     images=task.images
#                 )
                
#                 model_answer = model_response.get("text", "")
                
#                 # è¯„ä¼°ç­”æ¡ˆ
#                 result = benchmark.evaluate_answer(
#                     model_answer=model_answer,
#                     ground_truth=task.ground_truth,
#                     task=task
#                 )
                
#                 results.append(result)
                
#                 if hasattr(self, '_verbose') and self._verbose:
#                     status = "âœ“" if result.is_correct else "âœ—"
#                     print(f"        {status} ç­”æ¡ˆ: {model_answer[:50]}... | GT: {task.ground_truth}")
                
#             except KeyboardInterrupt:
#                 raise
            
#             except Exception as e:
#                 # è®°å½•é”™è¯¯ï¼Œä½†ç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡
#                 error_msg = str(e)
#                 if hasattr(self, '_verbose') and self._verbose:
#                     print(f"        âœ— ä»»åŠ¡å¤„ç†é”™è¯¯: {error_msg[:100]}")
                
#                 result = BenchmarkResult(
#                     task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
#                     question=task.question if hasattr(task, 'question') else "",
#                     ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
#                     model_answer="",
#                     is_correct=False,
#                     score=0.0,
#                     metadata={"error": error_msg}
#                 )
#                 results.append(result)
        
#         return results
    
#     def _build_prompt(self, task: BenchmarkTask) -> str:
#         """æ„å»ºæç¤º"""
#         # åŸºæœ¬æç¤ºæ ¼å¼
#         prompt = task.question
#         return prompt




# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# æµ‹è¯•æ‰§è¡Œå™¨ï¼šæ‰§è¡Œbenchmarkæµ‹è¯•
# """

# import time
# from typing import List, Dict, Any, Optional

# try:
#     from tqdm import tqdm
#     HAS_TQDM = True
# except ImportError:
#     HAS_TQDM = False
#     # ç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
#     def tqdm(iterable, desc=""):
#         print(f"{desc}...")
#         return iterable

# import sys
# from pathlib import Path

# # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
# current_dir = Path(__file__).parent.parent
# sys.path.insert(0, str(current_dir))

# from .model_adapter import BaseModelAdapter
# from ..benchmarks.base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


# class TestExecutor:
#     """æµ‹è¯•æ‰§è¡Œå™¨"""
    
#     def __init__(self, model_adapter: BaseModelAdapter):
#         self.model_adapter = model_adapter
#         self.model_info = model_adapter.get_model_info()
    
#     def run_benchmark(self, 
#                      benchmark: BaseBenchmark,
#                      max_samples: Optional[int] = None,
#                      verbose: bool = True) -> List[BenchmarkResult]:
#         """
#         è¿è¡Œå•ä¸ªbenchmarkæµ‹è¯•
        
#         Args:
#             benchmark: Benchmarkå®ä¾‹
#             max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
#             verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
#         Returns:
#             æµ‹è¯•ç»“æœåˆ—è¡¨
#         """
#         tasks = benchmark.get_tasks()
#         if max_samples:
#             tasks = tasks[:max_samples]
        
#         results = []
        
#         if verbose:
#             tasks_iter = tqdm(tasks, desc=f"Testing {benchmark.name}")
#         else:
#             tasks_iter = tasks
        
#         for task in tasks_iter:
#             try:
#                 # æ„å»ºæç¤º
#                 prompt = self._build_prompt(task)
                
#                 # è°ƒç”¨æ¨¡å‹
#                 model_response = self.model_adapter.generate(
#                     prompt=prompt,
#                     images=task.images
#                 )
                
#                 # è¯„ä¼°ç­”æ¡ˆ
#                 result = benchmark.evaluate_answer(
#                     model_answer=model_response.get("text", ""),
#                     ground_truth=task.ground_truth,
#                     task=task
#                 )
                
#                 results.append(result)
                
#             except Exception as e:
#                 # è®°å½•é”™è¯¯
#                 result = BenchmarkResult(
#                     task_id=task.task_id,
#                     question=task.question,
#                     ground_truth=task.ground_truth,
#                     model_answer="",
#                     is_correct=False,
#                     score=0.0,
#                     metadata={"error": str(e)}
#                 )
#                 results.append(result)
        
#         return results
    
#     def _build_prompt(self, task: BenchmarkTask) -> str:
#         """æ„å»ºæç¤º"""
#         # åŸºæœ¬æç¤ºæ ¼å¼
#         prompt = task.question
#         return prompt







#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ‰§è¡Œå™¨ï¼šæ‰§è¡Œbenchmarkæµ‹è¯•
"""

import time
from typing import List, Dict, Any, Optional

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # ç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
    def tqdm(iterable, desc=""):
        print(f"{desc}...")
        return iterable

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from .model_adapter import BaseModelAdapter
from ..benchmarks.base_benchmark import BaseBenchmark, BenchmarkTask, BenchmarkResult


class TestExecutor:
    """æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, model_adapter: BaseModelAdapter, verbose: bool = False):
        self.model_adapter = model_adapter
        self.model_info = model_adapter.get_model_info()
        self._verbose = verbose  # è¯¦ç»†è¾“å‡ºæ ‡å¿—
    
    def run_benchmark(self, 
                     benchmark: BaseBenchmark,
                     max_samples: Optional[int] = None,
                     batch_size: int = 1,
                     verbose: bool = True) -> List[BenchmarkResult]:
        """
        è¿è¡Œå•ä¸ªbenchmarkæµ‹è¯•ï¼ˆæ”¯æŒæµå¼å’Œæ‰¹é‡å¤„ç†ï¼‰
        
        Args:
            benchmark: Benchmarkå®ä¾‹
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆ1è¡¨ç¤ºé€ä¸ªå¤„ç†ï¼Œ>1è¡¨ç¤ºæ‰¹é‡å¤„ç†ï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        # æ›´æ–°è¯¦ç»†è¾“å‡ºæ ‡å¿—
        self._verbose = verbose
        
        if verbose:
            print(f"\n  ğŸš€ å¼€å§‹æµ‹è¯• Benchmark: {benchmark.name}")
            print(f"     æ¨¡å¼: {'æµå¼' if hasattr(benchmark, '_use_streaming') and benchmark._use_streaming else 'æ‰¹é‡'}")
            print(f"     æ‰¹å¤§å°: {batch_size}, æœ€å¤§æ ·æœ¬: {max_samples or 'æ— é™åˆ¶'}")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼åŠ è½½
        use_streaming = hasattr(benchmark, 'get_dataset_iterator') and hasattr(benchmark, 'get_task_from_item')
        
        if use_streaming and hasattr(benchmark, '_use_streaming') and benchmark._use_streaming:
            # æµå¼å¤„ç†æ¨¡å¼
            results = self._run_benchmark_streaming(
                benchmark, max_samples, batch_size, verbose
            )
        else:
            # æ™®é€šæ¨¡å¼ï¼ˆä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ä»»åŠ¡ï¼‰
            if verbose:
                print(f"  ğŸ“¦ åŠ è½½æ‰€æœ‰ä»»åŠ¡...")
            
            tasks = benchmark.get_tasks()
            if max_samples:
                tasks = tasks[:max_samples]
            
            if verbose:
                print(f"  âœ“ åŠ è½½äº† {len(tasks)} ä¸ªä»»åŠ¡")
            
            if verbose:
                tasks_iter = tqdm(tasks, desc=f"Testing {benchmark.name}", unit="task")
            else:
                tasks_iter = tasks
            
            results = self._process_tasks(benchmark, tasks_iter, batch_size)
        
        if verbose:
            correct_count = sum(1 for r in results if r.is_correct)
            error_count = sum(1 for r in results if r.metadata and 'error' in r.metadata)
            print(f"  âœ… æµ‹è¯•å®Œæˆ: {correct_count}/{len(results)} æ­£ç¡®, {error_count} é”™è¯¯")
        
        return results
    
    def _run_benchmark_streaming(self,
                                benchmark: BaseBenchmark,
                                max_samples: Optional[int],
                                batch_size: int,
                                verbose: bool) -> List[BenchmarkResult]:
        """æµå¼å¤„ç†benchmark"""
        results = []
        
        if verbose:
            print(f"  ğŸ“Š å¼€å§‹æµå¼å¤„ç†ï¼Œbatch_size={batch_size}, max_samples={max_samples or 'all'}")
        
        try:
            dataset_iter = benchmark.get_dataset_iterator()
        except Exception as e:
            print(f"  âœ— è·å–æ•°æ®é›†è¿­ä»£å™¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return results
        
        processed_count = 0
        skipped_count = 0
        error_count = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        if verbose:
            if max_samples:
                pbar = tqdm(total=max_samples, desc=f"Testing {benchmark.name} (streaming)", unit="task")
            else:
                pbar = tqdm(desc=f"Testing {benchmark.name} (streaming)", unit="task")
        
        try:
            batch_tasks = []
            batch_indices = []
            
            idx = 0
            while True:
                try:
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°
                    if max_samples and processed_count >= max_samples:
                        if verbose:
                            print(f"\n  âœ“ å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ ({max_samples})")
                        break
                    
                    # å°è¯•è·å–ä¸‹ä¸€ä¸ªitem
                    try:
                        item = next(dataset_iter)
                    except StopIteration:
                        if verbose:
                            print(f"\n  âœ“ æ•°æ®é›†è¿­ä»£å®Œæˆ (å…±å¤„ç† {idx} ä¸ªitems)")
                        break
                    except Exception as e:
                        print(f"  âœ— è·å–æ•°æ®é¡¹å¤±è´¥ (idx={idx}): {e}")
                        error_count += 1
                        if verbose and error_count <= 10:
                            import traceback
                            traceback.print_exc()
                        idx += 1
                        continue
                    
                    try:
                        # ä»itemåˆ›å»ºtask
                        task = benchmark.get_task_from_item(item, idx)
                        if task is None:
                            skipped_count += 1
                            if verbose and skipped_count <= 5:
                                print(f"  âš ï¸  è·³è¿‡æ— æ•ˆitem (idx={idx})")
                            idx += 1
                            continue
                        
                        batch_tasks.append(task)
                        batch_indices.append(idx)
                        
                        # è¾¾åˆ°batch_sizeæ—¶å¤„ç†ä¸€æ‰¹
                        if len(batch_tasks) >= batch_size:
                            if verbose:
                                print(f"  ğŸ”„ å¤„ç†æ‰¹æ¬¡ (size={len(batch_tasks)}, items {batch_indices[0]}-{batch_indices[-1]})")
                            
                            try:
                                batch_results = self._process_task_batch(benchmark, batch_tasks)
                                results.extend(batch_results)
                                processed_count += len(batch_results)
                                
                                # ç»Ÿè®¡æ‰¹æ¬¡ä¸­çš„æ­£ç¡®/é”™è¯¯æ•°é‡
                                batch_correct = sum(1 for r in batch_results if r.is_correct)
                                batch_errors = sum(1 for r in batch_results if r.metadata and 'error' in r.metadata)
                                error_count += batch_errors
                                
                                if verbose:
                                    pbar.update(len(batch_results))
                                    print(f"    æ‰¹æ¬¡ç»“æœ: {batch_correct}/{len(batch_results)} æ­£ç¡®, {batch_errors} é”™è¯¯")
                            
                            except Exception as e:
                                error_count += len(batch_tasks)
                                print(f"  âœ— æ‰¹æ¬¡å¤„ç†å¤±è´¥ (items {batch_indices[0]}-{batch_indices[-1]}): {e}")
                                if verbose:
                                    import traceback
                                    traceback.print_exc()
                                # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡åˆ›å»ºé”™è¯¯ç»“æœ
                                for task in batch_tasks:
                                    error_result = BenchmarkResult(
                                        task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
                                        question=task.question if hasattr(task, 'question') else "",
                                        ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
                                        model_answer="",
                                        is_correct=False,
                                        score=0.0,
                                        metadata={"error": str(e)}
                                    )
                                    results.append(error_result)
                            
                            batch_tasks = []
                            batch_indices = []
                    
                    except Exception as e:
                        skipped_count += 1
                        error_count += 1
                        print(f"  âœ— å¤„ç†itemå¤±è´¥ (idx={idx}): {e}")
                        if verbose and error_count <= 10:
                            import traceback
                            traceback.print_exc()
                    
                    idx += 1
                
                except KeyboardInterrupt:
                    raise
                
                except StopIteration:
                    break
                
                except Exception as e:
                    print(f"  âœ— è¿­ä»£è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ (idx={idx}): {e}")
                    if verbose:
                        import traceback
                        traceback.print_exc()
                    idx += 1
                    continue
            
            # å¤„ç†å‰©ä½™çš„task
            if batch_tasks:
                if verbose:
                    print(f"  ğŸ”„ å¤„ç†æœ€åä¸€æ‰¹ (size={len(batch_tasks)}, items {batch_indices[0]}-{batch_indices[-1]})")
                
                try:
                    batch_results = self._process_task_batch(benchmark, batch_tasks)
                    results.extend(batch_results)
                    processed_count += len(batch_results)
                    
                    batch_correct = sum(1 for r in batch_results if r.is_correct)
                    batch_errors = sum(1 for r in batch_results if r.metadata and 'error' in r.metadata)
                    error_count += batch_errors
                    
                    if verbose:
                        pbar.update(len(batch_results))
                        print(f"    æ‰¹æ¬¡ç»“æœ: {batch_correct}/{len(batch_results)} æ­£ç¡®, {batch_errors} é”™è¯¯")
                
                except Exception as e:
                    error_count += len(batch_tasks)
                    print(f"  âœ— æœ€åä¸€æ‰¹å¤„ç†å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡åˆ›å»ºé”™è¯¯ç»“æœ
                    for task in batch_tasks:
                        error_result = BenchmarkResult(
                            task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
                            question=task.question if hasattr(task, 'question') else "",
                            ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
                            model_answer="",
                            is_correct=False,
                            score=0.0,
                            metadata={"error": str(e)}
                        )
                        results.append(error_result)
        
        except KeyboardInterrupt:
            if verbose:
                print(f"\n  âš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
            raise
        
        except Exception as e:
            print(f"  âœ— æµå¼å¤„ç†å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            if verbose:
                pbar.close()
                print(f"  ğŸ“Š æµå¼å¤„ç†å®Œæˆ:")
                print(f"    æ€»å¤„ç†æ•°: {processed_count}")
                print(f"    è·³è¿‡æ•°: {skipped_count}")
                print(f"    é”™è¯¯æ•°: {error_count}")
                print(f"    æˆåŠŸç»“æœ: {len(results)}")
        
        return results
    
    def _process_tasks(self, benchmark: BaseBenchmark, tasks_iter, batch_size: int) -> List[BenchmarkResult]:
        """å¤„ç†ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰"""
        if batch_size > 1:
            # æ‰¹é‡å¤„ç†
            tasks_list = list(tasks_iter)
            results = []
            for i in range(0, len(tasks_list), batch_size):
                batch = tasks_list[i:i+batch_size]
                batch_results = self._process_task_batch(benchmark, batch)
                results.extend(batch_results)
            return results
        else:
            # é€ä¸ªå¤„ç†
            return self._process_task_batch(benchmark, tasks_iter)
    
    def _process_task_batch(self, benchmark: BaseBenchmark, tasks) -> List[BenchmarkResult]:
        """å¤„ç†ä¸€æ‰¹ä»»åŠ¡"""
        results = []
        
        for i, task in enumerate(tasks):
            try:
                # æ„å»ºæç¤º
                prompt = self._build_prompt(task)
                
                # è°ƒç”¨æ¨¡å‹
                if hasattr(self, '_verbose') and self._verbose:
                    print(f"      [{i+1}/{len(tasks)}] å¤„ç†ä»»åŠ¡: {task.task_id[:50]}...")
                    print(f"        ğŸ“ Prompt: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
                    print(f"        ğŸ–¼ï¸  å›¾åƒæ•°é‡: {len(task.images) if task.images else 0}")
                
                model_response = self.model_adapter.generate(
                    prompt=prompt,
                    images=task.images
                )

                # è¾“å‡ºæ¨¡å‹è¿”å›å€¼çš„è°ƒè¯•ä¿¡æ¯
                if hasattr(self, '_verbose') and self._verbose:
                    print(f"        ğŸ” æ¨¡å‹è¿”å›å€¼ç±»å‹: {type(model_response)}")
                    print(f"        ğŸ” æ¨¡å‹è¿”å›å€¼çš„é”®: {list(model_response.keys()) if isinstance(model_response, dict) else 'N/A'}")
                    # è¾“å‡ºå®Œæ•´çš„è¿”å›å€¼ï¼ˆå¦‚æœæ˜¯å­—å…¸ï¼Œæ ¼å¼åŒ–è¾“å‡ºï¼‰
                    if isinstance(model_response, dict):
                        print(f"        ğŸ” æ¨¡å‹è¿”å›å€¼å†…å®¹:")
                        for key, value in model_response.items():
                            if key == "raw" and isinstance(value, dict):
                                # rawå­—æ®µå¯èƒ½å¾ˆå¤§ï¼Œåªæ˜¾ç¤ºç±»å‹å’Œéƒ¨åˆ†ä¿¡æ¯
                                print(f"          - {key}: {type(value).__name__} (éšè—è¯¦ç»†å†…å®¹)")
                            else:
                                # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
                                value_str = str(value)
                                if len(value_str) > 200:
                                    value_str = value_str[:200] + "... (å·²æˆªæ–­)"
                                print(f"          - {key}: {value_str}")
                    else:
                        print(f"        ğŸ” æ¨¡å‹è¿”å›å€¼: {model_response}")
                
                model_answer = model_response.get("text", "")
                
                # è¯„ä¼°ç­”æ¡ˆ
                result = benchmark.evaluate_answer(
                    model_answer=model_answer,
                    ground_truth=task.ground_truth,
                    task=task
                )
                
                results.append(result)
                
                if hasattr(self, '_verbose') and self._verbose:
                    status = "âœ“" if result.is_correct else "âœ—"
                    print(f"        {status} ç­”æ¡ˆ: {model_answer[:50]}... | GT: {task.ground_truth}")
                
            except KeyboardInterrupt:
                raise
            
            except Exception as e:
                # è®°å½•é”™è¯¯ï¼Œä½†ç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡
                error_msg = str(e)
                if hasattr(self, '_verbose') and self._verbose:
                    print(f"        âœ— ä»»åŠ¡å¤„ç†é”™è¯¯: {error_msg[:100]}")
                
                result = BenchmarkResult(
                    task_id=task.task_id if hasattr(task, 'task_id') else f"error_{len(results)}",
                    question=task.question if hasattr(task, 'question') else "",
                    ground_truth=task.ground_truth if hasattr(task, 'ground_truth') else None,
                    model_answer="",
                    is_correct=False,
                    score=0.0,
                    metadata={"error": error_msg}
                )
                results.append(result)
        
        return results
    
    def _build_prompt(self, task: BenchmarkTask) -> str:
        """
        æ„å»ºæç¤º
        
        å¯¹äºä¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„promptæ ¼å¼ï¼š
        - BLIP VQAæ¨¡å‹ï¼šé€šå¸¸ç›´æ¥ä½¿ç”¨é—®é¢˜æ–‡æœ¬å³å¯
        - å…¶ä»–VQAæ¨¡å‹ï¼šå¯èƒ½éœ€è¦ "Question: ... Answer:" æ ¼å¼
        """
        # åŸºæœ¬æç¤ºæ ¼å¼ï¼šç›´æ¥ä½¿ç”¨é—®é¢˜æ–‡æœ¬
        prompt = task.question
        
        # å¯ä»¥æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´promptæ ¼å¼
        # ä¾‹å¦‚å¯¹äºæŸäº›æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ï¼š
        # prompt = f"Question: {task.question} Answer:"
        # æˆ–è€…
        # prompt = f"Q: {task.question}\nA:"
        
        return prompt
