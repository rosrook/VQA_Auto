# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# æ¨¡å‹é€‚é…å™¨ï¼šç»Ÿä¸€æ¥å£ï¼Œå…è®¸ç”¨æˆ·æ¥å…¥è‡ªå·±çš„æ¨¡å‹
# """

# import abc
# from typing import Any, Dict, List, Optional, Union


# class BaseModelAdapter(abc.ABC):
#     """æ¨¡å‹é€‚é…å™¨åŸºç±»"""
    
#     @abc.abstractmethod
#     def generate(self, 
#                 prompt: str,
#                 images: Optional[List[str]] = None,
#                 **kwargs) -> Dict[str, Any]:
#         """
#         ç”Ÿæˆæ¨¡å‹å“åº”
        
#         Args:
#             prompt: æ–‡æœ¬æç¤º
#             images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–base64ç¼–ç åˆ—è¡¨
#             **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokensç­‰ï¼‰
        
#         Returns:
#             {
#                 "text": "æ¨¡å‹è¾“å‡ºæ–‡æœ¬",
#                 "usage": {...},  # å¯é€‰çš„tokenä½¿ç”¨ä¿¡æ¯
#                 "raw": {...}     # å¯é€‰çš„åŸå§‹å“åº”
#             }
#         """
#         raise NotImplementedError
    
#     @abc.abstractmethod
#     def get_model_info(self) -> Dict[str, Any]:
#         """
#         è·å–æ¨¡å‹ä¿¡æ¯
        
#         Returns:
#             {
#                 "name": "æ¨¡å‹åç§°",
#                 "type": "æ¨¡å‹ç±»å‹",
#                 "version": "ç‰ˆæœ¬å·"ï¼ˆå¯é€‰ï¼‰
#             }
#         """
#         raise NotImplementedError


# class ModelAdapterFactory:
#     """æ¨¡å‹é€‚é…å™¨å·¥å‚"""
    
#     _adapters = {}
    
#     @classmethod
#     def register(cls, name: str, adapter_class):
#         """æ³¨å†Œé€‚é…å™¨ç±»"""
#         cls._adapters[name] = adapter_class
    
#     @classmethod
#     def create(cls, name: str, **kwargs) -> BaseModelAdapter:
#         """åˆ›å»ºé€‚é…å™¨å®ä¾‹"""
#         if name not in cls._adapters:
#             raise ValueError(f"æœªçŸ¥çš„é€‚é…å™¨ç±»å‹: {name}")
#         return cls._adapters[name](**kwargs)
    
#     @classmethod
#     def list_adapters(cls) -> List[str]:
#         """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„é€‚é…å™¨"""
#         return list(cls._adapters.keys())


# # ç¤ºä¾‹ï¼šOpenAIå…¼å®¹çš„é€‚é…å™¨
# class OpenAIAdapter(BaseModelAdapter):
#     """OpenAIå…¼å®¹çš„APIé€‚é…å™¨"""
    
#     def __init__(self, api_key: str, base_url: str, model: str, **kwargs):
#         self.api_key = api_key
#         self.base_url = base_url.rstrip("/")
#         self.model = model
#         self.kwargs = kwargs
    
#     def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
#         import requests
        
#         messages = [{"role": "user", "content": []}]
        
#         # æ·»åŠ æ–‡æœ¬
#         messages[0]["content"].append({"type": "text", "text": prompt})
        
#         # æ·»åŠ å›¾åƒï¼ˆå¦‚æœæä¾›ï¼‰
#         if images:
#             for img in images:
#                 if img.startswith("http"):
#                     messages[0]["content"].append({"type": "image_url", "image_url": {"url": img}})
#                 else:
#                     # å‡è®¾æ˜¯base64
#                     messages[0]["content"].append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img}"}})
        
#         payload = {
#             "model": self.model,
#             "messages": messages,
#             **kwargs
#         }
        
#         headers = {
#             "Authorization": f"Bearer {self.api_key}",
#             "Content-Type": "application/json"
#         }
        
#         response = requests.post(
#             f"{self.base_url}/chat/completions",
#             json=payload,
#             headers=headers,
#             timeout=kwargs.get("timeout", 30)
#         )
#         response.raise_for_status()
#         result = response.json()
        
#         return {
#             "text": result["choices"][0]["message"]["content"],
#             "usage": result.get("usage", {}),
#             "raw": result
#         }
    
#     def get_model_info(self) -> Dict[str, Any]:
#         return {
#             "name": self.model,
#             "type": "openai_compatible",
#             "base_url": self.base_url
#         }


# # HuggingFaceæœ¬åœ°æ¨¡å‹é€‚é…å™¨
# # class HuggingFaceAdapter(BaseModelAdapter):
# #     """HuggingFaceæœ¬åœ°æ¨¡å‹é€‚é…å™¨ï¼ˆæ”¯æŒæ–‡æœ¬å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰"""
    
# #     def __init__(self, 
# #                  model_id: str,
# #                  device: str = "cuda",
# #                  dtype: str = "auto",
# #                  trust_remote_code: bool = False,
# #                  load_in_8bit: bool = False,
# #                  load_in_4bit: bool = False,
# #                  max_new_tokens: int = 1024,
# #                  temperature: float = 0.7,
# #                  top_p: float = 0.9,
# #                  **kwargs):
# #         """
# #         Args:
# #             model_id: HuggingFaceæ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ "Qwen/Qwen-VL-Chat"ï¼‰
# #             device: è®¾å¤‡ ("cuda", "cpu", "mps"ç­‰)
# #             dtype: æ•°æ®ç±»å‹ ("auto", "float16", "bfloat16"ç­‰)
# #             trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
# #             load_in_8bit: æ˜¯å¦ä½¿ç”¨8bité‡åŒ–
# #             load_in_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
# #             max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
# #             temperature: é‡‡æ ·æ¸©åº¦
# #             top_p: nucleusé‡‡æ ·å‚æ•°
# #         """
# #         self.model_id = model_id
# #         self.device = device
# #         self.max_new_tokens = max_new_tokens
# #         self.temperature = temperature
# #         self.top_p = top_p
        
# #         try:
# #             from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
# #             from PIL import Image
# #             import torch
# #         except ImportError:
# #             raise ImportError(
# #                 "éœ€è¦å®‰è£…transformerså’Œtorch: pip install transformers torch pillow"
# #             )
        
# #         self.torch = torch
# #         self.Image = Image
        
# #         # åŠ è½½tokenizer/processor
# #         try:
# #             self.processor = AutoProcessor.from_pretrained(
# #                 model_id, 
# #                 trust_remote_code=trust_remote_code
# #             )
# #             self.has_processor = True
# #         except:
# #             self.tokenizer = AutoTokenizer.from_pretrained(
# #                 model_id,
# #                 trust_remote_code=trust_remote_code
# #             )
# #             self.has_processor = False
        
# #         # ç¡®å®šæ•°æ®ç±»å‹
# #         if dtype == "auto":
# #             if self.torch.cuda.is_available():
# #                 dtype = self.torch.float16
# #             else:
# #                 dtype = self.torch.float32
# #         elif dtype == "float16":
# #             dtype = self.torch.float16
# #         elif dtype == "bfloat16":
# #             dtype = self.torch.bfloat16
# #         else:
# #             dtype = self.torch.float32
        
# #         # åŠ è½½æ¨¡å‹
# #         load_kwargs = {
# #             "trust_remote_code": trust_remote_code,
# #             "device_map": device if device != "cpu" else None,
# #         }
        
# #         if device == "cpu":
# #             load_kwargs["torch_dtype"] = dtype
# #         else:
# #             load_kwargs["torch_dtype"] = dtype
        
# #         if load_in_8bit:
# #             load_kwargs["load_in_8bit"] = True
# #         elif load_in_4bit:
# #             load_kwargs["load_in_4bit"] = True
        
# #         self.model = AutoModelForCausalLM.from_pretrained(
# #             model_id,
# #             **load_kwargs
# #         )
# #         self.model.eval()
    
# #     def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
# #         """ç”Ÿæˆæ¨¡å‹å“åº”"""
# #         max_new_tokens = kwargs.get("max_tokens", kwargs.get("max_new_tokens", self.max_new_tokens))
# #         temperature = kwargs.get("temperature", self.temperature)
# #         top_p = kwargs.get("top_p", self.top_p)
        
# #         # å¤„ç†å›¾åƒ
# #         pil_images = None
# #         if images:
# #             pil_images = []
# #             for img_path in images:
# #                 if isinstance(img_path, str):
# #                     if img_path.startswith("http"):
# #                         # ä»URLåŠ è½½
# #                         import requests
# #                         from io import BytesIO
# #                         response = requests.get(img_path)
# #                         pil_images.append(self.Image.open(BytesIO(response.content)))
# #                     elif img_path.startswith("data:image"):
# #                         # Base64ç¼–ç çš„å›¾åƒ
# #                         import base64
# #                         from io import BytesIO
# #                         header, encoded = img_path.split(",", 1)
# #                         img_data = base64.b64decode(encoded)
# #                         pil_images.append(self.Image.open(BytesIO(img_data)))
# #                     else:
# #                         # æœ¬åœ°æ–‡ä»¶è·¯å¾„
# #                         pil_images.append(self.Image.open(img_path))
# #                 else:
# #                     # å‡è®¾å·²ç»æ˜¯PIL Image
# #                     pil_images.append(img_path)
        
# #         try:
# #             # æ–¹æ³•1: ä½¿ç”¨processorï¼ˆé€‚ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰
# #             if self.has_processor and pil_images:
# #                 inputs = self.processor(
# #                     text=prompt,
# #                     images=pil_images,
# #                     return_tensors="pt"
# #                 )
# #                 inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
# #                          for k, v in inputs.items()}
                
# #                 with self.torch.no_grad():
# #                     outputs = self.model.generate(
# #                         **inputs,
# #                         max_new_tokens=max_new_tokens,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         do_sample=temperature > 0,
# #                         **kwargs
# #                     )
                
# #                 # è§£ç è¾“å‡º
# #                 generated_text = self.processor.decode(
# #                     outputs[0],
# #                     skip_special_tokens=True
# #                 )
                
# #                 # ç§»é™¤è¾“å…¥éƒ¨åˆ†
# #                 if prompt in generated_text:
# #                     generated_text = generated_text.replace(prompt, "").strip()
            
# #             # æ–¹æ³•2: ä½¿ç”¨chatæ¥å£ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
# #             elif hasattr(self.model, 'chat') and pil_images:
# #                 if self.has_processor:
# #                     response, _ = self.model.chat(
# #                         self.processor,
# #                         query=prompt,
# #                         history=None,
# #                         images=pil_images,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         max_new_tokens=max_new_tokens,
# #                     )
# #                 else:
# #                     response, _ = self.model.chat(
# #                         self.tokenizer,
# #                         query=prompt,
# #                         history=None,
# #                         images=pil_images,
# #                     )
# #                 generated_text = response
            
# #             # æ–¹æ³•3: çº¯æ–‡æœ¬ç”Ÿæˆ
# #             else:
# #                 if self.has_processor:
# #                     inputs = self.processor(text=prompt, return_tensors="pt")
# #                 else:
# #                     inputs = self.tokenizer(prompt, return_tensors="pt")
                
# #                 inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
# #                          for k, v in inputs.items()}
                
# #                 with self.torch.no_grad():
# #                     outputs = self.model.generate(
# #                         **inputs,
# #                         max_new_tokens=max_new_tokens,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         do_sample=temperature > 0,
# #                         **kwargs
# #                     )
                
# #                 if self.has_processor:
# #                     generated_text = self.processor.decode(
# #                         outputs[0],
# #                         skip_special_tokens=True
# #                     )
# #                 else:
# #                     generated_text = self.tokenizer.decode(
# #                         outputs[0],
# #                         skip_special_tokens=True
# #                     )
                
# #                 # ç§»é™¤è¾“å…¥éƒ¨åˆ†
# #                 if prompt in generated_text:
# #                     generated_text = generated_text.replace(prompt, "").strip()
            
# #             return {
# #                 "text": generated_text,
# #                 "usage": {"prompt_tokens": 0, "completion_tokens": 0},  # å¯ä»¥æ·»åŠ å®é™…tokenç»Ÿè®¡
# #                 "raw": {"generated_text": generated_text}
# #             }
        
# #         except Exception as e:
# #             raise RuntimeError(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
# #     def get_model_info(self) -> Dict[str, Any]:
# #         return {
# #             "name": self.model_id,
# #             "type": "huggingface_local",
# #             "device": self.device
# #         }


# # class HuggingFaceAdapter(BaseModelAdapter):
# #     """HuggingFaceæœ¬åœ°æ¨¡å‹é€‚é…å™¨ï¼ˆæ”¯æŒæ–‡æœ¬å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰"""
    
# #     def __init__(self, 
# #                  model_id: str,
# #                  device: str = "cuda",
# #                  dtype: str = "auto",
# #                  trust_remote_code: bool = False,
# #                  load_in_8bit: bool = False,
# #                  load_in_4bit: bool = False,
# #                  max_new_tokens: int = 1024,
# #                  temperature: float = 0.7,
# #                  top_p: float = 0.9,
# #                  **kwargs):
# #         """
# #         Args:
# #             model_id: HuggingFaceæ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ "Qwen/Qwen-VL-Chat"ï¼‰
# #             device: è®¾å¤‡ ("cuda", "cpu", "mps"ç­‰)
# #             dtype: æ•°æ®ç±»å‹ ("auto", "float16", "bfloat16"ç­‰)
# #             trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
# #             load_in_8bit: æ˜¯å¦ä½¿ç”¨8bité‡åŒ–
# #             load_in_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
# #             max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
# #             temperature: é‡‡æ ·æ¸©åº¦
# #             top_p: nucleusé‡‡æ ·å‚æ•°
# #         """
# #         self.model_id = model_id
# #         self.device = device
# #         self.max_new_tokens = max_new_tokens
# #         self.temperature = temperature
# #         self.top_p = top_p
        
# #         try:
# #             from transformers import (
# #                 AutoConfig,
# #                 AutoModelForCausalLM,
# #                 AutoModelForVision2Seq,
# #                 BlipForConditionalGeneration,
# #                 AutoTokenizer,
# #                 AutoProcessor
# #             )
# #             from PIL import Image
# #             import torch
# #         except ImportError:
# #             raise ImportError(
# #                 "éœ€è¦å®‰è£…transformerså’Œtorch: pip install transformers torch pillow"
# #             )
        
# #         self.torch = torch
# #         self.Image = Image
        
# #         # åŠ è½½é…ç½®ä»¥ç¡®å®šæ¨¡å‹ç±»å‹
# #         config = AutoConfig.from_pretrained(
# #             model_id,
# #             trust_remote_code=trust_remote_code
# #         )
        
# #         # æ ¹æ®æ¨¡å‹æ¶æ„ç¡®å®šæ¨¡å‹ç±»å‹
# #         model_type = config.model_type.lower()
# #         architectures = getattr(config, 'architectures', [])
        
# #         # åˆ¤æ–­æ˜¯å¦ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹
# #         is_vision_model = any([
# #             'vision' in model_type,
# #             'vl' in model_type,
# #             'blip' in model_type,
# #             'clip' in model_type,
# #             'llava' in model_type,
# #             'qwen-vl' in model_type,
# #             any('Vision' in arch or 'VL' in arch or 'Blip' in arch or 'LLaVA' in arch 
# #                 for arch in architectures)
# #         ])
        
# #         # åŠ è½½tokenizer/processor
# #         try:
# #             self.processor = AutoProcessor.from_pretrained(
# #                 model_id, 
# #                 trust_remote_code=trust_remote_code
# #             )
# #             self.has_processor = True
# #         except:
# #             self.tokenizer = AutoTokenizer.from_pretrained(
# #                 model_id,
# #                 trust_remote_code=trust_remote_code
# #             )
# #             self.has_processor = False
        
# #         # ç¡®å®šæ•°æ®ç±»å‹
# #         if dtype == "auto":
# #             if self.torch.cuda.is_available():
# #                 dtype = self.torch.float16
# #             else:
# #                 dtype = self.torch.float32
# #         elif dtype == "float16":
# #             dtype = self.torch.float16
# #         elif dtype == "bfloat16":
# #             dtype = self.torch.bfloat16
# #         else:
# #             dtype = self.torch.float32
        
# #         # å‡†å¤‡åŠ è½½å‚æ•°
# #         load_kwargs = {
# #             "trust_remote_code": trust_remote_code,
# #             "device_map": device if device != "cpu" else None,
# #             "torch_dtype": dtype,
# #         }
        
# #         if load_in_8bit:
# #             load_kwargs["load_in_8bit"] = True
# #         elif load_in_4bit:
# #             load_kwargs["load_in_4bit"] = True
        
# #         # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹å¼
# #         try:
# #             if 'blip' in model_type:
# #                 # BLIPç³»åˆ—æ¨¡å‹
# #                 self.model = BlipForConditionalGeneration.from_pretrained(
# #                     model_id,
# #                     **load_kwargs
# #                 )
# #                 self.model_class = 'blip'
# #             elif is_vision_model:
# #                 # å°è¯•ä½¿ç”¨AutoModelForVision2Seq
# #                 try:
# #                     self.model = AutoModelForVision2Seq.from_pretrained(
# #                         model_id,
# #                         **load_kwargs
# #                     )
# #                     self.model_class = 'vision2seq'
# #                 except:
# #                     # å¦‚æœå¤±è´¥ï¼Œå°è¯•AutoModelForCausalLMï¼ˆæŸäº›VLæ¨¡å‹ä½¿ç”¨CausalLMæ¶æ„ï¼‰
# #                     self.model = AutoModelForCausalLM.from_pretrained(
# #                         model_id,
# #                         **load_kwargs
# #                     )
# #                     self.model_class = 'causal_lm'
# #             else:
# #                 # çº¯æ–‡æœ¬æ¨¡å‹
# #                 self.model = AutoModelForCausalLM.from_pretrained(
# #                     model_id,
# #                     **load_kwargs
# #                 )
# #                 self.model_class = 'causal_lm'
                
# #         except Exception as e:
# #             # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæœ€åå°è¯•AutoModelForCausalLM
# #             print(f"è­¦å‘Š: ä½¿ç”¨æ ‡å‡†æ–¹æ³•åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {e}")
# #             self.model = AutoModelForCausalLM.from_pretrained(
# #                 model_id,
# #                 **load_kwargs
# #             )
# #             self.model_class = 'causal_lm'
        
# #         self.model.eval()
# #         self.is_vision_model = is_vision_model
    
# #     def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
# #         """ç”Ÿæˆæ¨¡å‹å“åº”"""
# #         max_new_tokens = kwargs.get("max_tokens", kwargs.get("max_new_tokens", self.max_new_tokens))
# #         temperature = kwargs.get("temperature", self.temperature)
# #         top_p = kwargs.get("top_p", self.top_p)
        
# #         # å¤„ç†å›¾åƒ
# #         pil_images = None
# #         if images:
# #             pil_images = []
# #             for img_path in images:
# #                 if isinstance(img_path, str):
# #                     if img_path.startswith("http"):
# #                         # ä»URLåŠ è½½
# #                         import requests
# #                         from io import BytesIO
# #                         response = requests.get(img_path)
# #                         pil_images.append(self.Image.open(BytesIO(response.content)))
# #                     elif img_path.startswith("data:image"):
# #                         # Base64ç¼–ç çš„å›¾åƒ
# #                         import base64
# #                         from io import BytesIO
# #                         header, encoded = img_path.split(",", 1)
# #                         img_data = base64.b64decode(encoded)
# #                         pil_images.append(self.Image.open(BytesIO(img_data)))
# #                     else:
# #                         # æœ¬åœ°æ–‡ä»¶è·¯å¾„
# #                         pil_images.append(self.Image.open(img_path))
# #                 else:
# #                     # å‡è®¾å·²ç»æ˜¯PIL Image
# #                     pil_images.append(img_path)
        
# #         try:
# #             # æ–¹æ³•1: ä½¿ç”¨processorï¼ˆé€‚ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰
# #             if self.has_processor and pil_images:
# #                 inputs = self.processor(
# #                     text=prompt,
# #                     images=pil_images,
# #                     return_tensors="pt"
# #                 )
# #                 inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
# #                          for k, v in inputs.items()}
                
# #                 with self.torch.no_grad():
# #                     outputs = self.model.generate(
# #                         **inputs,
# #                         max_new_tokens=max_new_tokens,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         do_sample=temperature > 0,
# #                         **kwargs
# #                     )
                
# #                 # è§£ç è¾“å‡º
# #                 generated_text = self.processor.decode(
# #                     outputs[0],
# #                     skip_special_tokens=True
# #                 )
                
# #                 # ç§»é™¤è¾“å…¥éƒ¨åˆ†
# #                 if prompt in generated_text:
# #                     generated_text = generated_text.replace(prompt, "").strip()
            
# #             # æ–¹æ³•2: ä½¿ç”¨chatæ¥å£ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
# #             elif hasattr(self.model, 'chat') and pil_images:
# #                 if self.has_processor:
# #                     response, _ = self.model.chat(
# #                         self.processor,
# #                         query=prompt,
# #                         history=None,
# #                         images=pil_images,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         max_new_tokens=max_new_tokens,
# #                     )
# #                 else:
# #                     response, _ = self.model.chat(
# #                         self.tokenizer,
# #                         query=prompt,
# #                         history=None,
# #                         images=pil_images,
# #                     )
# #                 generated_text = response
            
# #             # æ–¹æ³•3: çº¯æ–‡æœ¬ç”Ÿæˆ
# #             else:
# #                 if self.has_processor:
# #                     inputs = self.processor(text=prompt, return_tensors="pt")
# #                 else:
# #                     inputs = self.tokenizer(prompt, return_tensors="pt")
                
# #                 inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
# #                          for k, v in inputs.items()}
                
# #                 with self.torch.no_grad():
# #                     outputs = self.model.generate(
# #                         **inputs,
# #                         max_new_tokens=max_new_tokens,
# #                         temperature=temperature,
# #                         top_p=top_p,
# #                         do_sample=temperature > 0,
# #                         **kwargs
# #                     )
                
# #                 if self.has_processor:
# #                     generated_text = self.processor.decode(
# #                         outputs[0],
# #                         skip_special_tokens=True
# #                     )
# #                 else:
# #                     generated_text = self.tokenizer.decode(
# #                         outputs[0],
# #                         skip_special_tokens=True
# #                     )
                
# #                 # ç§»é™¤è¾“å…¥éƒ¨åˆ†
# #                 if prompt in generated_text:
# #                     generated_text = generated_text.replace(prompt, "").strip()
            
# #             return {
# #                 "text": generated_text,
# #                 "usage": {"prompt_tokens": 0, "completion_tokens": 0},
# #                 "raw": {"generated_text": generated_text}
# #             }
        
# #         except Exception as e:
# #             raise RuntimeError(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
# #     def get_model_info(self) -> Dict[str, Any]:
# #         return {
# #             "name": self.model_id,
# #             "type": "huggingface_local",
# #             "device": self.device,
# #             "model_class": self.model_class,
# #             "is_vision_model": self.is_vision_model
# #         }


# class HuggingFaceAdapter(BaseModelAdapter):
#     """HuggingFaceæœ¬åœ°æ¨¡å‹é€‚é…å™¨ï¼ˆæ”¯æŒæ–‡æœ¬å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰"""
    
#     def __init__(self, 
#                  model_id: str,
#                  device: str = "cuda",
#                  dtype: str = "auto",
#                  trust_remote_code: bool = False,
#                  load_in_8bit: bool = False,
#                  load_in_4bit: bool = False,
#                  max_new_tokens: int = 1024,
#                  temperature: float = 0.7,
#                  top_p: float = 0.9,
#                  **kwargs):
#         """
#         Args:
#             model_id: HuggingFaceæ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ "Qwen/Qwen-VL-Chat"ï¼‰
#             device: è®¾å¤‡ ("cuda", "cpu", "mps"ç­‰)
#             dtype: æ•°æ®ç±»å‹ ("auto", "float16", "bfloat16"ç­‰)
#             trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
#             load_in_8bit: æ˜¯å¦ä½¿ç”¨8bité‡åŒ–
#             load_in_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
#             max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
#             temperature: é‡‡æ ·æ¸©åº¦
#             top_p: nucleusé‡‡æ ·å‚æ•°
#         """
#         self.model_id = model_id
#         self.device = device
#         self.max_new_tokens = max_new_tokens
#         self.temperature = temperature
#         self.top_p = top_p
        
#         try:
#             from transformers import (
#                 AutoConfig,
#                 AutoModelForCausalLM,
#                 AutoModelForVision2Seq,
#                 BlipForConditionalGeneration,
#                 AutoTokenizer,
#                 AutoProcessor
#             )
#             from PIL import Image
#             import torch
#         except ImportError:
#             raise ImportError(
#                 "éœ€è¦å®‰è£…transformerså’Œtorch: pip install transformers torch pillow"
#             )
        
#         self.torch = torch
#         self.Image = Image
        
#         # åŠ è½½é…ç½®ä»¥ç¡®å®šæ¨¡å‹ç±»å‹
#         config = AutoConfig.from_pretrained(
#             model_id,
#             trust_remote_code=trust_remote_code
#         )
        
#         # æ ¹æ®æ¨¡å‹æ¶æ„ç¡®å®šæ¨¡å‹ç±»å‹
#         model_type = config.model_type.lower()
#         architectures = getattr(config, 'architectures', [])
        
#         # åˆ¤æ–­æ˜¯å¦ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹
#         is_vision_model = any([
#             'vision' in model_type,
#             'vl' in model_type,
#             'blip' in model_type,
#             'clip' in model_type,
#             'llava' in model_type,
#             'qwen-vl' in model_type,
#             any('Vision' in arch or 'VL' in arch or 'Blip' in arch or 'LLaVA' in arch 
#                 for arch in architectures)
#         ])
        
#         # åŠ è½½tokenizer/processor
#         try:
#             self.processor = AutoProcessor.from_pretrained(
#                 model_id, 
#                 trust_remote_code=trust_remote_code
#             )
#             self.has_processor = True
#         except:
#             self.tokenizer = AutoTokenizer.from_pretrained(
#                 model_id,
#                 trust_remote_code=trust_remote_code
#             )
#             self.has_processor = False
        
#         # ç¡®å®šæ•°æ®ç±»å‹
#         if dtype == "auto":
#             if self.torch.cuda.is_available():
#                 dtype_obj = self.torch.float16
#             else:
#                 dtype_obj = self.torch.float32
#         elif dtype == "float16":
#             dtype_obj = self.torch.float16
#         elif dtype == "bfloat16":
#             dtype_obj = self.torch.bfloat16
#         else:
#             dtype_obj = self.torch.float32
        
#         # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†accelerateï¼ˆç”¨äºdevice_mapï¼‰
#         try:
#             import accelerate
#             has_accelerate = True
#         except ImportError:
#             has_accelerate = False
        
#         # å‡†å¤‡åŠ è½½å‚æ•°
#         load_kwargs = {
#             "trust_remote_code": trust_remote_code,
#             "torch_dtype": dtype_obj,
#         }
        
#         # åªåœ¨æœ‰accelerateæˆ–deviceä¸ºcpuæ—¶è®¾ç½®device_map
#         if device == "cpu":
#             # CPUæ¨¡å¼ä¸éœ€è¦device_map
#             pass
#         elif has_accelerate:
#             load_kwargs["device_map"] = device
#         else:
#             # æ²¡æœ‰accelerateï¼Œæ‰‹åŠ¨æŒ‡å®šè®¾å¤‡
#             print(f"è­¦å‘Š: æœªå®‰è£…accelerateï¼Œå°†åœ¨åŠ è½½åæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ° {device}")
        
#         if load_in_8bit:
#             load_kwargs["load_in_8bit"] = True
#         elif load_in_4bit:
#             load_kwargs["load_in_4bit"] = True
        
#         # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹å¼
#         self.model = None
#         self.model_class = None
        
#         try:
#             if 'blip' in model_type:
#                 # BLIPç³»åˆ—æ¨¡å‹
#                 self.model = BlipForConditionalGeneration.from_pretrained(
#                     model_id,
#                     **load_kwargs
#                 )
#                 self.model_class = 'blip'
#             elif is_vision_model:
#                 # å°è¯•ä½¿ç”¨AutoModelForVision2Seq
#                 try:
#                     self.model = AutoModelForVision2Seq.from_pretrained(
#                         model_id,
#                         **load_kwargs
#                     )
#                     self.model_class = 'vision2seq'
#                 except Exception as e:
#                     # æŸäº›VLæ¨¡å‹ä½¿ç”¨CausalLMæ¶æ„ï¼Œä½†éœ€è¦ç¡®ä¿ä¸æ˜¯BLIP
#                     if 'blip' not in model_type:
#                         self.model = AutoModelForCausalLM.from_pretrained(
#                             model_id,
#                             **load_kwargs
#                         )
#                         self.model_class = 'causal_lm'
#                     else:
#                         raise e
#             else:
#                 # çº¯æ–‡æœ¬æ¨¡å‹
#                 self.model = AutoModelForCausalLM.from_pretrained(
#                     model_id,
#                     **load_kwargs
#                 )
#                 self.model_class = 'causal_lm'
                
#         except Exception as e:
#             raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}\næç¤º: å¦‚æœæ˜¯device_mapç›¸å…³é”™è¯¯ï¼Œè¯·å®‰è£…accelerate: pip install accelerate")
        
#         # å¦‚æœæ²¡æœ‰ä½¿ç”¨device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹
#         if device != "cpu" and not has_accelerate and "device_map" not in load_kwargs:
#             self.model = self.model.to(device)
        
#         self.model.eval()
#         self.is_vision_model = is_vision_model
        
#         # ç”¨äºè·Ÿè¸ªæ˜¯å¦æ˜¯é¦–æ¬¡è°ƒç”¨
#         self._first_call = True
#         self._first_output_printed = False
    
#     def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
#         """ç”Ÿæˆæ¨¡å‹å“åº”"""
#         max_new_tokens = kwargs.get("max_tokens", kwargs.get("max_new_tokens", self.max_new_tokens))
#         temperature = kwargs.get("temperature", self.temperature)
#         top_p = kwargs.get("top_p", self.top_p)
        
#         # å¤„ç†å›¾åƒ
#         pil_images = None
#         if images:
#             pil_images = []
#             for img_path in images:
#                 if isinstance(img_path, str):
#                     if img_path.startswith("http"):
#                         # ä»URLåŠ è½½
#                         import requests
#                         from io import BytesIO
#                         response = requests.get(img_path)
#                         pil_images.append(self.Image.open(BytesIO(response.content)))
#                     elif img_path.startswith("data:image"):
#                         # Base64ç¼–ç çš„å›¾åƒ
#                         import base64
#                         from io import BytesIO
#                         header, encoded = img_path.split(",", 1)
#                         img_data = base64.b64decode(encoded)
#                         pil_images.append(self.Image.open(BytesIO(img_data)))
#                     else:
#                         # æœ¬åœ°æ–‡ä»¶è·¯å¾„
#                         pil_images.append(self.Image.open(img_path))
#                 else:
#                     # å‡è®¾å·²ç»æ˜¯PIL Image
#                     pil_images.append(img_path)
        
#         # ç”¨äºä¿å­˜è¾“å‡ºä¿¡æ¯ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶ä½¿ç”¨ï¼‰
#         outputs_info = None
#         inputs_info = None
#         generation_method = None
        
#         try:
#             # æ–¹æ³•1: ä½¿ç”¨processorï¼ˆé€‚ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰
#             if self.has_processor and pil_images:
#                 generation_method = "processor_with_images"
                
#                 # å¯¹äºBLIP VQAæ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
#                 # BLIP VQAåº”è¯¥ä»å›¾åƒç‰¹å¾ç”Ÿæˆï¼Œè€Œä¸æ˜¯ä»ç¼–ç çš„é—®é¢˜
#                 # æˆ‘ä»¬éœ€è¦åªä¼ å…¥å›¾åƒå’Œé—®é¢˜æ–‡æœ¬ï¼Œè®©æ¨¡å‹è‡ªå·±å¤„ç†
#                 if self.model_class == 'blip':
#                     # BLIP VQAçš„ç‰¹æ®Šå¤„ç†ï¼šåªä¼ å…¥å›¾åƒå’Œé—®é¢˜æ–‡æœ¬ï¼Œä¸ä¼ å…¥é¢„ç¼–ç çš„input_ids
#                     inputs = self.processor(
#                         images=pil_images,
#                         text=prompt,
#                         return_tensors="pt",
#                         padding=True
#                     )
#                     inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
#                              for k, v in inputs.items()}
                    
#                     # å¯¹äºBLIP VQAï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨é¢„ç¼–ç çš„input_idsä½œä¸ºç”Ÿæˆèµ·ç‚¹
#                     # è€Œæ˜¯è®©æ¨¡å‹ä»å›¾åƒç‰¹å¾å¼€å§‹ç”Ÿæˆ
#                     # æ‰€ä»¥æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªdecoder_input_idsï¼Œé€šå¸¸ä»¥[BOS] tokenå¼€å§‹
#                     if 'input_ids' in inputs:
#                         # è·å–decoderçš„èµ·å§‹tokenï¼ˆé€šå¸¸æ˜¯[BOS] tokenï¼‰
#                         decoder_start_token_id = self.model.config.decoder_start_token_id if hasattr(self.model.config, 'decoder_start_token_id') else None
#                         if decoder_start_token_id is None:
#                             # å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œå°è¯•ä½¿ç”¨processorçš„tokenizerçš„bos_token_id
#                             if hasattr(self.processor, 'tokenizer'):
#                                 decoder_start_token_id = self.processor.tokenizer.bos_token_id
#                             elif hasattr(self.processor, 'decoder_tokenizer'):
#                                 decoder_start_token_id = self.processor.decoder_tokenizer.bos_token_id
                        
#                         # åˆ›å»ºdecoder_input_idsï¼Œä»BOS tokenå¼€å§‹
#                         batch_size = inputs['pixel_values'].shape[0] if hasattr(inputs['pixel_values'], 'shape') else 1
                        
#                         # è·å–decoderçš„èµ·å§‹token ID
#                         if decoder_start_token_id is None:
#                             # å°è¯•å…¶ä»–æ–¹å¼è·å–BOS token
#                             if hasattr(self.processor, 'tokenizer') and hasattr(self.processor.tokenizer, 'bos_token_id'):
#                                 decoder_start_token_id = self.processor.tokenizer.bos_token_id
#                             elif hasattr(self.processor, 'decoder_tokenizer') and hasattr(self.processor.decoder_tokenizer, 'bos_token_id'):
#                                 decoder_start_token_id = self.processor.decoder_tokenizer.bos_token_id
#                             elif hasattr(self.model.config, 'bos_token_id'):
#                                 decoder_start_token_id = self.model.config.bos_token_id
                        
#                         # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•pad_token_id
#                         if decoder_start_token_id is None:
#                             pad_token_id = getattr(self.model.config, 'pad_token_id', None)
#                             if pad_token_id is None:
#                                 # æœ€åä½¿ç”¨0ä½œä¸ºé»˜è®¤å€¼
#                                 decoder_start_token_id = 0
#                             else:
#                                 decoder_start_token_id = pad_token_id
                        
#                         # ç¡®ä¿decoder_start_token_idæ˜¯æ•´æ•°
#                         decoder_start_token_id = int(decoder_start_token_id)
                        
#                         # åˆ›å»ºdecoder_input_ids
#                         decoder_input_ids = self.torch.full(
#                             (batch_size, 1), 
#                             decoder_start_token_id, 
#                             dtype=self.torch.long, 
#                             device=self.device
#                         )
                        
#                         # ç§»é™¤input_idsï¼Œåªä¿ç•™pixel_valueså’Œå…¶ä»–å¿…è¦çš„è¾“å…¥
#                         # ä½¿ç”¨decoder_input_idsä»£æ›¿input_ids
#                         inputs = {k: v for k, v in inputs.items() if k != 'input_ids'}
#                         inputs['decoder_input_ids'] = decoder_input_ids
#                         input_length = 0  # decoderä»ç©ºå¼€å§‹ç”Ÿæˆ
#                     else:
#                         input_length = 0
#                 else:
#                     # éBLIPæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
#                     inputs = self.processor(
#                         text=prompt,
#                         images=pil_images,
#                         return_tensors="pt"
#                     )
#                     inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
#                              for k, v in inputs.items()}
#                     input_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
#                 if self._first_call:
#                     inputs_info = self._extract_inputs_info(inputs, pil_images)
#                     self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
#                     print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„:")
#                     self._print_inputs_structure(inputs_info)
#                     print(f"  â€¢ è¾“å…¥é•¿åº¦ï¼ˆç”¨äºè§£ç ï¼‰: {input_length}")
                
#                 with self.torch.no_grad():
#                     # ç”Ÿæˆå‚æ•°
#                     generate_kwargs = {
#                         "max_new_tokens": max_new_tokens,
#                         "temperature": temperature,
#                         "top_p": top_p,
#                         "do_sample": temperature > 0,
#                     }
#                     # æ·»åŠ å…¶ä»–kwargsï¼Œä½†é¿å…è¦†ç›–é‡è¦å‚æ•°
#                     for key, value in kwargs.items():
#                         if key not in generate_kwargs:
#                             generate_kwargs[key] = value
                    
#                     if self._first_call:
#                         print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - Generateè°ƒç”¨å‚æ•°:")
#                         print(f"  â€¢ inputsé”®: {list(inputs.keys())}")
#                         print(f"  â€¢ generate_kwargs: {generate_kwargs}")
#                         print(f"  â€¢ input_length: {input_length} (decoderèµ·å§‹é•¿åº¦)")
                    
#                     outputs = self.model.generate(
#                         **inputs,
#                         **generate_kwargs
#                     )
                    
#                     if self._first_call:
#                         print(f"  â€¢ generateè¿”å›çš„outputsç±»å‹: {type(outputs)}")
#                         print(f"  â€¢ generateè¿”å›çš„outputså½¢çŠ¶: {outputs.shape if hasattr(outputs, 'shape') else 'N/A'}")
#                         if hasattr(outputs, '__len__') and len(outputs) > 0:
#                             print(f"  â€¢ generateè¿”å›çš„outputs[0]é•¿åº¦: {len(outputs[0])}")
#                         print(f"  â€¢ é¢„æœŸé•¿åº¦: max_new_tokens({max_new_tokens})")
#                         print(f"  â€¢ å®é™…ç”Ÿæˆé•¿åº¦: {len(outputs[0]) if hasattr(outputs, '__len__') and len(outputs) > 0 else 'N/A'}")
                
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
#                 if self._first_call:
#                     outputs_info = self._extract_outputs_info(outputs, input_length)
#                     # æ·»åŠ è¯¦ç»†çš„è¾“å‡ºè°ƒè¯•ä¿¡æ¯
#                     print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - Generateè¾“å‡ºæ£€æŸ¥:")
#                     print(f"  â€¢ outputsç±»å‹: {type(outputs)}")
#                     print(f"  â€¢ outputså½¢çŠ¶: {outputs.shape if hasattr(outputs, 'shape') else 'N/A'}")
#                     print(f"  â€¢ outputs[0]å½¢çŠ¶: {outputs[0].shape if hasattr(outputs[0], 'shape') else 'N/A'}")
#                     print(f"  â€¢ outputs[0]é•¿åº¦: {len(outputs[0]) if hasattr(outputs[0], '__len__') else 'N/A'}")
#                     print(f"  â€¢ input_length: {input_length}")
#                     print(f"  â€¢ æ˜¯å¦æœ‰æ–°ç”Ÿæˆå†…å®¹: {len(outputs[0]) > input_length}")
#                     if len(outputs[0]) > input_length:
#                         print(f"  â€¢ æ–°ç”Ÿæˆçš„tokenæ•°é‡: {len(outputs[0]) - input_length}")
#                         print(f"  â€¢ æ–°ç”Ÿæˆçš„token IDs: {outputs[0][input_length:].tolist()}")
#                     else:
#                         print(f"  âš ï¸  è­¦å‘Š: è¾“å‡ºé•¿åº¦({len(outputs[0])}) <= è¾“å…¥é•¿åº¦({input_length})ï¼Œæ¨¡å‹å¯èƒ½æ²¡æœ‰ç”Ÿæˆæ–°å†…å®¹ï¼")
#                         print(f"  â€¢ å®Œæ•´çš„outputs[0]: {outputs[0].tolist()}")
#                         print(f"  â€¢ è¾“å…¥çš„input_ids: {inputs['input_ids'][0].tolist()}")
                
#                 # å¯¹äºBLIPæ¨¡å‹ï¼Œinput_lengthä¸º0ï¼Œç›´æ¥è§£ç æ•´ä¸ªè¾“å‡º
#                 # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œå¦‚æœinput_length > 0ï¼Œåªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
#                 if input_length > 0 and len(outputs[0]) > input_length:
#                     # åªè§£ç æ–°ç”Ÿæˆçš„token IDs
#                     generated_ids = outputs[0][input_length:]
#                     generated_text = self.processor.decode(
#                         generated_ids,
#                         skip_special_tokens=True
#                     )
#                     if self._first_call:
#                         print(f"  â€¢ è§£ç çš„æ–°ç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
#                 else:
#                     # å¯¹äºBLIPæ¨¡å‹æˆ–æ— æ³•ç¡®å®šè¾“å…¥é•¿åº¦çš„æƒ…å†µï¼Œè§£ç æ•´ä¸ªè¾“å‡º
#                     generated_text = self.processor.decode(
#                         outputs[0],
#                         skip_special_tokens=True
#                     )
#                     if self._first_call:
#                         print(f"  â€¢ å®Œæ•´è§£ç æ–‡æœ¬: '{generated_text}'")
#                         if input_length == 0:
#                             print(f"  â€¢ æ³¨æ„: input_length=0ï¼Œä½¿ç”¨å®Œæ•´è§£ç ï¼ˆé€‚ç”¨äºBLIPç­‰ä»decoderå¼€å§‹ç”Ÿæˆçš„æ¨¡å‹ï¼‰")
                    
#                     # å¦‚æœinput_length > 0ï¼Œè¯´æ˜å¯èƒ½éœ€è¦ç§»é™¤è¾“å…¥éƒ¨åˆ†
#                     # ä½†å¯¹äºBLIPï¼Œinput_length=0ï¼Œæ‰€ä»¥ä¸éœ€è¦ç§»é™¤
#                     if input_length > 0 and prompt in generated_text:
#                         # æ‰¾åˆ°promptåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®å¹¶ç§»é™¤
#                         prompt_pos = generated_text.find(prompt)
#                         if prompt_pos == 0:
#                             # promptåœ¨å¼€å¤´ï¼Œç›´æ¥ç§»é™¤
#                             generated_text = generated_text[len(prompt):].strip()
#                             if self._first_call:
#                                 print(f"  â€¢ ç§»é™¤å¼€å¤´çš„promptå: '{generated_text}'")
#                         else:
#                             # promptåœ¨ä¸­é—´æˆ–æœ«å°¾ï¼Œå°è¯•ç§»é™¤
#                             generated_text = generated_text.replace(prompt, "").strip()
#                             if self._first_call:
#                                 print(f"  â€¢ æ›¿æ¢promptå: '{generated_text}'")
                    
#                     # å¦‚æœç”Ÿæˆçš„æ–‡æœ¬ä¸ºç©ºï¼Œè¯´æ˜å¯èƒ½æœ‰é—®é¢˜
#                     if not generated_text and self._first_call:
#                         print(f"  âš ï¸  è­¦å‘Š: ç”Ÿæˆçš„æ–‡æœ¬ä¸ºç©ºï¼å¯èƒ½éœ€è¦æ£€æŸ¥æ¨¡å‹generateçš„å‚æ•°æˆ–æ¨¡å‹æœ¬èº«")
            
#             # æ–¹æ³•2: ä½¿ç”¨chatæ¥å£ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
#             elif hasattr(self.model, 'chat') and pil_images:
#                 generation_method = "chat_interface"
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
#                 if self._first_call:
#                     inputs_info = {
#                         "method": "chat",
#                         "prompt": prompt,
#                         "images_count": len(pil_images) if pil_images else 0,
#                         "image_sizes": [img.size for img in pil_images] if pil_images else [],
#                         "has_processor": self.has_processor
#                     }
#                     self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
#                     print("\nğŸ“¥ Chatæ¥å£è¾“å…¥ä¿¡æ¯:")
#                     print(f"  â€¢ å›¾åƒæ•°é‡: {inputs_info['images_count']}")
#                     print(f"  â€¢ å›¾åƒå°ºå¯¸: {inputs_info['image_sizes']}")
#                     print(f"  â€¢ ä½¿ç”¨Processor: {inputs_info['has_processor']}")
                
#                 if self.has_processor:
#                     response, _ = self.model.chat(
#                         self.processor,
#                         query=prompt,
#                         history=None,
#                         images=pil_images,
#                         temperature=temperature,
#                         top_p=top_p,
#                         max_new_tokens=max_new_tokens,
#                     )
#                 else:
#                     response, _ = self.model.chat(
#                         self.tokenizer,
#                         query=prompt,
#                         history=None,
#                         images=pil_images,
#                     )
#                 generated_text = response
                
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
#                 if self._first_call:
#                     outputs_info = {
#                         "method": "chat",
#                         "response_type": type(response).__name__,
#                         "response_length": len(response) if isinstance(response, str) else "N/A"
#                     }
            
#             # æ–¹æ³•3: çº¯æ–‡æœ¬ç”Ÿæˆ
#             else:
#                 generation_method = "text_only"
#                 if self.has_processor:
#                     inputs = self.processor(text=prompt, return_tensors="pt")
#                 else:
#                     inputs = self.tokenizer(prompt, return_tensors="pt")
                
#                 inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
#                          for k, v in inputs.items()}
                
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
#                 if self._first_call:
#                     inputs_info = self._extract_inputs_info(inputs, None)
#                     self._print_first_call_info(prompt, None, max_new_tokens, temperature, top_p, kwargs)
#                     print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„:")
#                     self._print_inputs_structure(inputs_info)
                
#                 # ä¿å­˜è¾“å…¥é•¿åº¦ï¼Œç”¨äºåç»­åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
#                 input_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                
#                 with self.torch.no_grad():
#                     outputs = self.model.generate(
#                         **inputs,
#                         max_new_tokens=max_new_tokens,
#                         temperature=temperature,
#                         top_p=top_p,
#                         do_sample=temperature > 0,
#                         **kwargs
#                     )
                
#                 # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
#                 if self._first_call:
#                     outputs_info = self._extract_outputs_info(outputs, input_length)
                
#                 # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæ’é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
#                 if input_length > 0 and len(outputs[0]) > input_length:
#                     # åªè§£ç æ–°ç”Ÿæˆçš„token IDs
#                     generated_ids = outputs[0][input_length:]
#                     if self.has_processor:
#                         generated_text = self.processor.decode(
#                             generated_ids,
#                             skip_special_tokens=True
#                         )
#                     else:
#                         generated_text = self.tokenizer.decode(
#                             generated_ids,
#                             skip_special_tokens=True
#                         )
#                 else:
#                     # å¦‚æœæ— æ³•ç¡®å®šè¾“å…¥é•¿åº¦ï¼Œè§£ç æ•´ä¸ªè¾“å‡ºç„¶åç§»é™¤è¾“å…¥éƒ¨åˆ†
#                     if self.has_processor:
#                         generated_text = self.processor.decode(
#                             outputs[0],
#                             skip_special_tokens=True
#                         )
#                     else:
#                         generated_text = self.tokenizer.decode(
#                             outputs[0],
#                             skip_special_tokens=True
#                         )
#                     # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼ˆæ›´ç²¾ç¡®çš„æ–¹æ³•ï¼‰
#                     if prompt in generated_text:
#                         # æ‰¾åˆ°promptåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®å¹¶ç§»é™¤
#                         prompt_pos = generated_text.find(prompt)
#                         if prompt_pos == 0:
#                             # promptåœ¨å¼€å¤´ï¼Œç›´æ¥ç§»é™¤
#                             generated_text = generated_text[len(prompt):].strip()
#                         else:
#                             # promptåœ¨ä¸­é—´æˆ–æœ«å°¾ï¼Œå°è¯•ç§»é™¤
#                             generated_text = generated_text.replace(prompt, "").strip()
            
#             result = {
#                 "text": generated_text,
#                 "usage": {"prompt_tokens": 0, "completion_tokens": 0},
#                 "raw": {"generated_text": generated_text}
#             }
            
#             # é¦–æ¬¡è°ƒç”¨æ—¶è¾“å‡ºè¯¦ç»†ä¿¡æ¯
#             if self._first_call:
#                 self._print_first_output_info(result, outputs_info, generation_method)
#                 self._first_call = False
#                 self._first_output_printed = True
            
#             return result
        
#         except Exception as e:
#             raise RuntimeError(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
#     def _print_inputs_structure(self, inputs_info: Dict):
#         """æ‰“å°è¾“å…¥ç»“æ„ä¿¡æ¯"""
#         if "keys" in inputs_info:
#             print(f"  â€¢ è¾“å…¥é”®: {inputs_info['keys']}")
#         if "tensor_info" in inputs_info:
#             print(f"  â€¢ Tensorä¿¡æ¯:")
#             for key, info in inputs_info["tensor_info"].items():
#                 if "shape" in info:
#                     print(f"    - {key}: shape={info['shape']}, dtype={info['dtype']}, device={info['device']}")
#                 else:
#                     print(f"    - {key}: {info}")
#         if "images" in inputs_info:
#             print(f"  â€¢ å›¾åƒä¿¡æ¯:")
#             for i, img_info in enumerate(inputs_info["images"]):
#                 print(f"    - å›¾åƒ {i+1}: {img_info}")
    
#     def _print_first_call_info(self, prompt: str, images: Optional[List], max_new_tokens: int, 
#                                temperature: float, top_p: float, kwargs: Dict):
#         """æ‰“å°é¦–æ¬¡è°ƒç”¨æ—¶çš„è¯¦ç»†ä¿¡æ¯"""
#         print("\n" + "="*80)
#         print("ğŸ” é¦–æ¬¡æ¨¡å‹è°ƒç”¨ - è¾“å…¥ä¿¡æ¯")
#         print("="*80)
        
#         # æ¨¡å‹ä¿¡æ¯
#         print("\nğŸ“¦ æ¨¡å‹ä¿¡æ¯:")
#         print(f"  â€¢ æ¨¡å‹ID: {self.model_id}")
#         print(f"  â€¢ æ¨¡å‹ç±»å‹: {self.model_class}")
#         print(f"  â€¢ è®¾å¤‡: {self.device}")
#         print(f"  â€¢ æ˜¯å¦è§†è§‰æ¨¡å‹: {self.is_vision_model}")
#         print(f"  â€¢ ä½¿ç”¨Processor: {self.has_processor}")
#         if hasattr(self, 'model'):
#             print(f"  â€¢ æ¨¡å‹ç±»: {type(self.model).__name__}")
#             if hasattr(self.model, 'config'):
#                 config = self.model.config
#                 print(f"  â€¢ æ¨¡å‹é…ç½®ç±»å‹: {type(config).__name__}")
#                 if hasattr(config, 'vocab_size'):
#                     print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
#                 if hasattr(config, 'max_position_embeddings'):
#                     print(f"  â€¢ æœ€å¤§ä½ç½®ç¼–ç : {config.max_position_embeddings}")
        
#         # è¾“å…¥ä¿¡æ¯
#         print("\nğŸ“¥ è¾“å…¥ä¿¡æ¯:")
#         print(f"  â€¢ Prompt: {prompt[:200]}{'...' if len(prompt) > 200 else ''}")
#         print(f"  â€¢ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
#         if images:
#             print(f"  â€¢ å›¾åƒæ•°é‡: {len(images)}")
#             for i, img in enumerate(images):
#                 if isinstance(img, str):
#                     print(f"    - å›¾åƒ {i+1}: {img[:100]}{'...' if len(img) > 100 else ''}")
#                 elif hasattr(img, 'size'):
#                     print(f"    - å›¾åƒ {i+1}: PIL Image, å°ºå¯¸: {img.size}, æ¨¡å¼: {img.mode}")
#                 else:
#                     print(f"    - å›¾åƒ {i+1}: {type(img).__name__}")
#         else:
#             print(f"  â€¢ å›¾åƒæ•°é‡: 0")
        
#         # ç”Ÿæˆå‚æ•°
#         print("\nâš™ï¸  ç”Ÿæˆå‚æ•°:")
#         print(f"  â€¢ max_new_tokens: {max_new_tokens}")
#         print(f"  â€¢ temperature: {temperature}")
#         print(f"  â€¢ top_p: {top_p}")
#         if kwargs:
#             print(f"  â€¢ å…¶ä»–å‚æ•°: {kwargs}")
        
#         print("="*80 + "\n")
    
#     def _extract_inputs_info(self, inputs: Dict, images: Optional[List]) -> Dict:
#         """æå–è¾“å…¥ä¿¡æ¯çš„ç»“æ„åŒ–æ•°æ®"""
#         info = {
#             "keys": list(inputs.keys()),
#             "tensor_info": {}
#         }
        
#         for key, value in inputs.items():
#             if isinstance(value, self.torch.Tensor):
#                 info["tensor_info"][key] = {
#                     "shape": list(value.shape),
#                     "dtype": str(value.dtype),
#                     "device": str(value.device),
#                     "requires_grad": value.requires_grad
#                 }
#             else:
#                 info["tensor_info"][key] = {
#                     "type": type(value).__name__,
#                     "value": str(value)[:100] if not isinstance(value, (list, dict)) else f"{type(value).__name__} with {len(value)} items"
#                 }
        
#         if images:
#             info["images"] = []
#             for img in images:
#                 if hasattr(img, 'size'):
#                     info["images"].append({
#                         "type": "PIL.Image",
#                         "size": img.size,
#                         "mode": img.mode
#                     })
#                 else:
#                     info["images"].append({"type": type(img).__name__})
        
#         return info
    
#     def _extract_outputs_info(self, outputs, input_length: int) -> Dict:
#         """æå–è¾“å‡ºä¿¡æ¯çš„ç»“æ„åŒ–æ•°æ®"""
#         info = {
#             "output_type": type(outputs).__name__,
#             "input_length": input_length
#         }
        
#         if isinstance(outputs, self.torch.Tensor):
#             info["shape"] = list(outputs.shape)
#             info["dtype"] = str(outputs.dtype)
#             info["device"] = str(outputs.device)
#         elif isinstance(outputs, (list, tuple)) and len(outputs) > 0:
#             first_output = outputs[0] if isinstance(outputs, list) else outputs[0]
#             if isinstance(first_output, self.torch.Tensor):
#                 info["first_output_shape"] = list(first_output.shape)
#                 info["first_output_dtype"] = str(first_output.dtype)
#                 info["total_length"] = first_output.shape[0] if len(first_output.shape) > 0 else "N/A"
#                 info["generated_length"] = first_output.shape[0] - input_length if input_length > 0 else "N/A"
        
#         return info
    
#     def _print_first_output_info(self, result: Dict, outputs_info: Optional[Dict], generation_method: Optional[str]):
#         """æ‰“å°é¦–æ¬¡è°ƒç”¨æ—¶çš„è¾“å‡ºä¿¡æ¯"""
#         print("\n" + "="*80)
#         print("ğŸ“¤ é¦–æ¬¡æ¨¡å‹è°ƒç”¨ - è¾“å‡ºä¿¡æ¯")
#         print("="*80)
        
#         # ç”Ÿæˆæ–¹æ³•
#         if generation_method:
#             print(f"\nğŸ”§ ä½¿ç”¨çš„ç”Ÿæˆæ–¹æ³•: {generation_method}")
        
#         # è¾“å‡ºç»“æ„
#         print("\nğŸ“Š è¾“å‡ºç»“æ„:")
#         print(f"  â€¢ è¿”å›ç±»å‹: {type(result).__name__}")
#         print(f"  â€¢ è¿”å›é”®: {list(result.keys())}")
        
#         # ç”Ÿæˆçš„æ–‡æœ¬
#         if "text" in result:
#             text = result["text"]
#             print(f"\nğŸ’¬ ç”Ÿæˆçš„æ–‡æœ¬:")
#             print(f"  â€¢ å†…å®¹: {text[:200]}{'...' if len(text) > 200 else ''}")
#             print(f"  â€¢ é•¿åº¦: {len(text)} å­—ç¬¦")
        
#         # åŸå§‹è¾“å‡ºä¿¡æ¯
#         if outputs_info:
#             print(f"\nğŸ” åŸå§‹è¾“å‡ºä¿¡æ¯:")
#             if "output_type" in outputs_info:
#                 print(f"  â€¢ è¾“å‡ºç±»å‹: {outputs_info['output_type']}")
#             if "shape" in outputs_info:
#                 print(f"  â€¢ å½¢çŠ¶: {outputs_info['shape']}")
#             if "first_output_shape" in outputs_info:
#                 print(f"  â€¢ ç¬¬ä¸€ä¸ªè¾“å‡ºå½¢çŠ¶: {outputs_info['first_output_shape']}")
#             if "input_length" in outputs_info:
#                 print(f"  â€¢ è¾“å…¥é•¿åº¦: {outputs_info['input_length']}")
#             if "generated_length" in outputs_info:
#                 print(f"  â€¢ ç”Ÿæˆé•¿åº¦: {outputs_info['generated_length']}")
        
#         # Usageä¿¡æ¯
#         if "usage" in result:
#             print(f"\nğŸ“ˆ Tokenä½¿ç”¨:")
#             for key, value in result["usage"].items():
#                 print(f"  â€¢ {key}: {value}")
        
#         # Rawä¿¡æ¯ï¼ˆç®€è¦ï¼‰
#         if "raw" in result:
#             raw = result["raw"]
#             print(f"\nğŸ“¦ Rawå“åº”:")
#             print(f"  â€¢ ç±»å‹: {type(raw).__name__}")
#             if isinstance(raw, dict):
#                 print(f"  â€¢ é”®: {list(raw.keys())}")
        
#         print("="*80 + "\n")
    
#     def get_model_info(self) -> Dict[str, Any]:
#         return {
#             "name": self.model_id,
#             "type": "huggingface_local",
#             "device": self.device,
#             "model_class": self.model_class,
#             "is_vision_model": self.is_vision_model
#         }


# # HuggingFace Hub Inference APIé€‚é…å™¨
# class HuggingFaceHubAdapter(BaseModelAdapter):
#     """HuggingFace Hub Inference APIé€‚é…å™¨"""
    
#     def __init__(self, 
#                  model_id: str,
#                  api_token: Optional[str] = None,
#                  api_url: Optional[str] = None,
#                  timeout: float = 30.0,
#                  **kwargs):
#         """
#         Args:
#             model_id: HuggingFaceæ¨¡å‹ID
#             api_token: HuggingFace API tokenï¼ˆå¯é€‰ï¼Œç”¨äºç§æœ‰æ¨¡å‹ï¼‰
#             api_url: è‡ªå®šä¹‰API URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨HuggingFace Inference APIï¼‰
#             timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
#         """
#         self.model_id = model_id
#         self.api_token = api_token
#         self.timeout = timeout
        
#         if api_url:
#             self.api_url = api_url.rstrip("/")
#         else:
#             self.api_url = f"https://api-inference.huggingface.co/models/{model_id}"
        
#         try:
#             import requests
#             self.requests = requests
#         except ImportError:
#             raise ImportError("éœ€è¦å®‰è£…requests: pip install requests")
    
#     def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
#         """ç”Ÿæˆæ¨¡å‹å“åº”"""
#         headers = {}
#         if self.api_token:
#             headers["Authorization"] = f"Bearer {self.api_token}"
        
#         # æ„å»ºè¯·æ±‚payload
#         payload = {
#             "inputs": prompt,
#             "parameters": {
#                 "max_new_tokens": kwargs.get("max_tokens", kwargs.get("max_new_tokens", 1024)),
#                 "temperature": kwargs.get("temperature", 0.7),
#                 "top_p": kwargs.get("top_p", 0.9),
#             }
#         }
        
#         # å¤„ç†å›¾åƒ
#         if images:
#             # HuggingFace Inference APIæ”¯æŒå›¾åƒè¾“å…¥
#             # å¯¹äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œinputså¯ä»¥æ˜¯å­—å…¸
#             if len(images) == 1:
#                 img_path = images[0]
#                 if img_path.startswith("http") or img_path.startswith("data:image"):
#                     payload["inputs"] = {
#                         "text": prompt,
#                         "image": img_path
#                     }
#                 else:
#                     # è¯»å–æœ¬åœ°å›¾åƒå¹¶è½¬æ¢ä¸ºbase64
#                     from PIL import Image
#                     import base64
#                     from io import BytesIO
                    
#                     img = Image.open(img_path)
#                     buffered = BytesIO()
#                     img.save(buffered, format="JPEG")
#                     img_base64 = base64.b64encode(buffered.getvalue()).decode()
#                     payload["inputs"] = {
#                         "text": prompt,
#                         "image": f"data:image/jpeg;base64,{img_base64}"
#                     }
        
#         try:
#             response = self.requests.post(
#                 self.api_url,
#                 headers=headers,
#                 json=payload,
#                 timeout=self.timeout
#             )
#             response.raise_for_status()
#             result = response.json()
            
#             # è§£æå“åº”ï¼ˆHuggingFace APIè¿”å›æ ¼å¼å¯èƒ½ä¸åŒï¼‰
#             if isinstance(result, list) and len(result) > 0:
#                 if "generated_text" in result[0]:
#                     text = result[0]["generated_text"]
#                 elif "answer" in result[0]:
#                     text = result[0]["answer"]
#                 else:
#                     text = str(result[0])
#             elif isinstance(result, dict):
#                 text = result.get("generated_text", result.get("answer", str(result)))
#             else:
#                 text = str(result)
            
#             # ç§»é™¤è¾“å…¥éƒ¨åˆ†
#             if prompt in text:
#                 text = text.replace(prompt, "").strip()
            
#             return {
#                 "text": text,
#                 "usage": {},
#                 "raw": result
#             }
        
#         except Exception as e:
#             raise RuntimeError(f"HuggingFace APIè°ƒç”¨å¤±è´¥: {e}")
    
#     def get_model_info(self) -> Dict[str, Any]:
#         return {
#             "name": self.model_id,
#             "type": "huggingface_hub_api",
#             "api_url": self.api_url
#         }


# # æ³¨å†Œé»˜è®¤é€‚é…å™¨
# ModelAdapterFactory.register("openai", OpenAIAdapter)
# ModelAdapterFactory.register("huggingface", HuggingFaceAdapter)
# ModelAdapterFactory.register("hf", HuggingFaceAdapter)  # ç®€å†™
# ModelAdapterFactory.register("huggingface_hub", HuggingFaceHubAdapter)
# ModelAdapterFactory.register("hf_hub", HuggingFaceHubAdapter)  # ç®€å†™








#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é€‚é…å™¨ï¼šç»Ÿä¸€æ¥å£ï¼Œå…è®¸ç”¨æˆ·æ¥å…¥è‡ªå·±çš„æ¨¡å‹
"""

import abc
from typing import Any, Dict, List, Optional, Union


class BaseModelAdapter(abc.ABC):
    """æ¨¡å‹é€‚é…å™¨åŸºç±»"""
    
    @abc.abstractmethod
    def generate(self, 
                prompt: str,
                images: Optional[List[str]] = None,
                **kwargs) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¨¡å‹å“åº”
        
        Args:
            prompt: æ–‡æœ¬æç¤º
            images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–base64ç¼–ç åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokensç­‰ï¼‰
        
        Returns:
            {
                "text": "æ¨¡å‹è¾“å‡ºæ–‡æœ¬",
                "usage": {...},  # å¯é€‰çš„tokenä½¿ç”¨ä¿¡æ¯
                "raw": {...}     # å¯é€‰çš„åŸå§‹å“åº”
            }
        """
        raise NotImplementedError
    
    @abc.abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            {
                "name": "æ¨¡å‹åç§°",
                "type": "æ¨¡å‹ç±»å‹",
                "version": "ç‰ˆæœ¬å·"ï¼ˆå¯é€‰ï¼‰
            }
        """
        raise NotImplementedError


class ModelAdapterFactory:
    """æ¨¡å‹é€‚é…å™¨å·¥å‚"""
    
    _adapters = {}
    
    @classmethod
    def register(cls, name: str, adapter_class):
        """æ³¨å†Œé€‚é…å™¨ç±»"""
        cls._adapters[name] = adapter_class
    
    @classmethod
    def create(cls, name: str, **kwargs) -> BaseModelAdapter:
        """åˆ›å»ºé€‚é…å™¨å®ä¾‹"""
        if name not in cls._adapters:
            raise ValueError(f"æœªçŸ¥çš„é€‚é…å™¨ç±»å‹: {name}")
        return cls._adapters[name](**kwargs)
    
    @classmethod
    def list_adapters(cls) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„é€‚é…å™¨"""
        return list(cls._adapters.keys())


# ç¤ºä¾‹ï¼šOpenAIå…¼å®¹çš„é€‚é…å™¨
class OpenAIAdapter(BaseModelAdapter):
    """OpenAIå…¼å®¹çš„APIé€‚é…å™¨"""
    
    def __init__(self, api_key: str, base_url: str, model: str, **kwargs):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.kwargs = kwargs
    
    def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
        import requests
        
        messages = [{"role": "user", "content": []}]
        
        # æ·»åŠ æ–‡æœ¬
        messages[0]["content"].append({"type": "text", "text": prompt})
        
        # æ·»åŠ å›¾åƒï¼ˆå¦‚æœæä¾›ï¼‰
        if images:
            for img in images:
                if img.startswith("http"):
                    messages[0]["content"].append({"type": "image_url", "image_url": {"url": img}})
                else:
                    # å‡è®¾æ˜¯base64
                    messages[0]["content"].append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img}"}})
        
        payload = {
            "model": self.model,
            "messages": messages,
            **kwargs
        }
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            json=payload,
            headers=headers,
            timeout=kwargs.get("timeout", 30)
        )
        response.raise_for_status()
        result = response.json()
        
        return {
            "text": result["choices"][0]["message"]["content"],
            "usage": result.get("usage", {}),
            "raw": result
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "name": self.model,
            "type": "openai_compatible",
            "base_url": self.base_url
        }


class HuggingFaceAdapter(BaseModelAdapter):
    """HuggingFaceæœ¬åœ°æ¨¡å‹é€‚é…å™¨ï¼ˆæ”¯æŒæ–‡æœ¬å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰"""
    
    def __init__(self, 
                 model_id: str,
                 device: str = "cuda",
                 dtype: str = "auto",
                 trust_remote_code: bool = False,
                 load_in_8bit: bool = False,
                 load_in_4bit: bool = False,
                 max_new_tokens: int = 1024,
                 temperature: float = 0.7,
                 top_p: float = 0.9,
                 **kwargs):
        """
        Args:
            model_id: HuggingFaceæ¨¡å‹IDæˆ–æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ "Qwen/Qwen-VL-Chat"ï¼‰
            device: è®¾å¤‡ ("cuda", "cpu", "mps"ç­‰)
            dtype: æ•°æ®ç±»å‹ ("auto", "float16", "bfloat16"ç­‰)
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
            load_in_8bit: æ˜¯å¦ä½¿ç”¨8bité‡åŒ–
            load_in_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
        """
        self.model_id = model_id
        self.device = device
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        
        try:
            from transformers import (
                AutoConfig,
                AutoModelForCausalLM,
                AutoModelForVision2Seq,
                BlipForConditionalGeneration,
                AutoTokenizer,
                AutoProcessor
            )
            from PIL import Image
            import torch
        except ImportError:
            raise ImportError(
                "éœ€è¦å®‰è£…transformerså’Œtorch: pip install transformers torch pillow"
            )
        
        self.torch = torch
        self.Image = Image
        
        # åŠ è½½é…ç½®ä»¥ç¡®å®šæ¨¡å‹ç±»å‹
        config = AutoConfig.from_pretrained(
            model_id,
            trust_remote_code=trust_remote_code
        )
        
        # æ ¹æ®æ¨¡å‹æ¶æ„ç¡®å®šæ¨¡å‹ç±»å‹
        model_type = config.model_type.lower()
        architectures = getattr(config, 'architectures', [])
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹
        is_vision_model = any([
            'vision' in model_type,
            'vl' in model_type,
            'blip' in model_type,
            'clip' in model_type,
            'llava' in model_type,
            'qwen-vl' in model_type,
            any('Vision' in arch or 'VL' in arch or 'Blip' in arch or 'LLaVA' in arch 
                for arch in architectures)
        ])
        
        # åŠ è½½tokenizer/processor
        try:
            self.processor = AutoProcessor.from_pretrained(
                model_id, 
                trust_remote_code=trust_remote_code
            )
            self.has_processor = True
        except:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=trust_remote_code
            )
            self.has_processor = False
        
        # ç¡®å®šæ•°æ®ç±»å‹
        if dtype == "auto":
            if self.torch.cuda.is_available():
                dtype_obj = self.torch.float16
            else:
                dtype_obj = self.torch.float32
        elif dtype == "float16":
            dtype_obj = self.torch.float16
        elif dtype == "bfloat16":
            dtype_obj = self.torch.bfloat16
        else:
            dtype_obj = self.torch.float32
        
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†accelerateï¼ˆç”¨äºdevice_mapï¼‰
        try:
            import accelerate
            has_accelerate = True
        except ImportError:
            has_accelerate = False
        
        # å‡†å¤‡åŠ è½½å‚æ•°
        load_kwargs = {
            "trust_remote_code": trust_remote_code,
            "torch_dtype": dtype_obj,
        }
        
        # åªåœ¨æœ‰accelerateæˆ–deviceä¸ºcpuæ—¶è®¾ç½®device_map
        if device == "cpu":
            # CPUæ¨¡å¼ä¸éœ€è¦device_map
            pass
        elif has_accelerate:
            load_kwargs["device_map"] = device
        else:
            # æ²¡æœ‰accelerateï¼Œæ‰‹åŠ¨æŒ‡å®šè®¾å¤‡
            print(f"è­¦å‘Š: æœªå®‰è£…accelerateï¼Œå°†åœ¨åŠ è½½åæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ° {device}")
        
        if load_in_8bit:
            load_kwargs["load_in_8bit"] = True
        elif load_in_4bit:
            load_kwargs["load_in_4bit"] = True
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹å¼
        self.model = None
        self.model_class = None
        
        try:
            if 'blip' in model_type:
                # BLIPç³»åˆ—æ¨¡å‹ - æ ¹æ®ä»»åŠ¡é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ç±»
                # å…³é”®ï¼šBLIPæœ‰å¤šä¸ªå˜ä½“ï¼Œéœ€è¦æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ VQA æ¨¡å‹
                if 'vqa' in model_id.lower():
                    # ä½¿ç”¨ BlipForQuestionAnsweringï¼ˆVQAä¸“ç”¨ï¼‰
                    from transformers import BlipForQuestionAnswering
                    self.model = BlipForQuestionAnswering.from_pretrained(
                        model_id,
                        **load_kwargs
                    )
                    self.model_class = 'blip_vqa'
                    print(f"âœ… åŠ è½½ BLIP VQA æ¨¡å‹: BlipForQuestionAnswering")
                else:
                    # ä½¿ç”¨ BlipForConditionalGenerationï¼ˆCaption/ç”Ÿæˆä»»åŠ¡ï¼‰
                    self.model = BlipForConditionalGeneration.from_pretrained(
                        model_id,
                        **load_kwargs
                    )
                    self.model_class = 'blip_caption'
                    print(f"âœ… åŠ è½½ BLIP Caption æ¨¡å‹: BlipForConditionalGeneration")
                    
            elif is_vision_model:
                # å°è¯•ä½¿ç”¨AutoModelForVision2Seq
                try:
                    self.model = AutoModelForVision2Seq.from_pretrained(
                        model_id,
                        **load_kwargs
                    )
                    self.model_class = 'vision2seq'
                except Exception as e:
                    # æŸäº›VLæ¨¡å‹ä½¿ç”¨CausalLMæ¶æ„ï¼Œä½†éœ€è¦ç¡®ä¿ä¸æ˜¯BLIP
                    if 'blip' not in model_type:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_id,
                            **load_kwargs
                        )
                        self.model_class = 'causal_lm'
                    else:
                        raise e
            else:
                # çº¯æ–‡æœ¬æ¨¡å‹
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    **load_kwargs
                )
                self.model_class = 'causal_lm'
                
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}\næç¤º: å¦‚æœæ˜¯device_mapç›¸å…³é”™è¯¯ï¼Œè¯·å®‰è£…accelerate: pip install accelerate")
        
        # å¦‚æœæ²¡æœ‰ä½¿ç”¨device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹
        if device != "cpu" and not has_accelerate and "device_map" not in load_kwargs:
            self.model = self.model.to(device)
        
        self.model.eval()
        self.is_vision_model = is_vision_model
        
        # ç”¨äºè·Ÿè¸ªæ˜¯å¦æ˜¯é¦–æ¬¡è°ƒç”¨
        self._first_call = True
        self._first_output_printed = False
    
    def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        max_new_tokens = kwargs.get("max_tokens", kwargs.get("max_new_tokens", self.max_new_tokens))
        temperature = kwargs.get("temperature", self.temperature)
        top_p = kwargs.get("top_p", self.top_p)
        
        # å¤„ç†å›¾åƒ
        pil_images = None
        if images:
            pil_images = []
            for img_path in images:
                if isinstance(img_path, str):
                    if img_path.startswith("http"):
                        # ä»URLåŠ è½½
                        import requests
                        from io import BytesIO
                        response = requests.get(img_path)
                        pil_images.append(self.Image.open(BytesIO(response.content)))
                    elif img_path.startswith("data:image"):
                        # Base64ç¼–ç çš„å›¾åƒ
                        import base64
                        from io import BytesIO
                        header, encoded = img_path.split(",", 1)
                        img_data = base64.b64decode(encoded)
                        pil_images.append(self.Image.open(BytesIO(img_data)))
                    else:
                        # æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        pil_images.append(self.Image.open(img_path))
                else:
                    # å‡è®¾å·²ç»æ˜¯PIL Image
                    pil_images.append(img_path)
        
        # ç”¨äºä¿å­˜è¾“å‡ºä¿¡æ¯ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶ä½¿ç”¨ï¼‰
        outputs_info = None
        inputs_info = None
        generation_method = None
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨processorï¼ˆé€‚ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰
            if self.has_processor and pil_images:
                generation_method = "processor_with_images"
                
                # BLIP VQA çš„æ­£ç¡®å¤„ç†æ–¹å¼
                if self.model_class == 'blip_vqa':
                    # âœ… æ­£ç¡®ï¼šä½¿ç”¨ BlipForQuestionAnswering
                    # processor ä¼šæ­£ç¡®å¤„ç† image å’Œ question
                    
                    # æ ‡å‡†è°ƒç”¨æ–¹å¼
                    inputs = self.processor(
                        pil_images[0] if len(pil_images) == 1 else pil_images,  # å•å›¾åƒä¼ PIL.Imageï¼Œå¤šå›¾åƒä¼ list
                        prompt,  # questionä½œä¸ºç¬¬äºŒä¸ªå‚æ•°
                        return_tensors="pt"
                    )
                    inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
                             for k, v in inputs.items()}
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
                    if self._first_call:
                        inputs_info = self._extract_inputs_info(inputs, pil_images)
                        self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
                        print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„ (BLIP VQA):")
                        self._print_inputs_structure(inputs_info)
                    
                    with self.torch.no_grad():
                        # BLIP VQA æ¨èå‚æ•°
                        generate_kwargs = {
                            "max_length": kwargs.get("max_length", 20),  # VQAç­”æ¡ˆé€šå¸¸å¾ˆçŸ­
                        }
                        
                        # ä¸å»ºè®®å¯¹VQAä½¿ç”¨beam searchå’Œsamplingï¼Œç›´æ¥ç”Ÿæˆæœ€å¯èƒ½çš„ç­”æ¡ˆ
                        # ä½†å¦‚æœç”¨æˆ·åšæŒï¼Œä¹Ÿå¯ä»¥æ·»åŠ 
                        if "num_beams" in kwargs:
                            generate_kwargs["num_beams"] = kwargs["num_beams"]
                        
                        if self._first_call:
                            print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - BLIP VQA Generateè°ƒç”¨:")
                            print(f"  â€¢ æ¨¡å‹ç±»: {type(self.model).__name__}")
                            print(f"  â€¢ inputsé”®: {list(inputs.keys())}")
                            print(f"  â€¢ generate_kwargs: {generate_kwargs}")
                        
                        # æ ‡å‡†VQAç”Ÿæˆ
                        outputs = self.model.generate(
                            **inputs,
                            **generate_kwargs
                        )
                        
                        if self._first_call:
                            print(f"  â€¢ outputså½¢çŠ¶: {outputs.shape}")
                            print(f"  â€¢ outputs[0]: {outputs[0].tolist()}")
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
                    if self._first_call:
                        outputs_info = self._extract_outputs_info(outputs, 0)
                    
                    # ç›´æ¥è§£ç ç­”æ¡ˆ
                    generated_text = self.processor.decode(
                        outputs[0],
                        skip_special_tokens=True
                    )
                    
                    if self._first_call:
                        print(f"  â€¢ è§£ç åçš„ç­”æ¡ˆ: '{generated_text}'")
                
                # BLIP Caption æ¨¡å‹ï¼ˆä¸æ˜¯VQAï¼‰
                elif self.model_class == 'blip_caption':
                    # å¯¹äºCaptionæ¨¡å‹ï¼ŒåŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬
                    inputs = self.processor(
                        images=pil_images,
                        text=prompt,
                        return_tensors="pt"
                    )
                    inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
                             for k, v in inputs.items()}
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
                    if self._first_call:
                        inputs_info = self._extract_inputs_info(inputs, pil_images)
                        self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
                        print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„ (BLIP Caption):")
                        self._print_inputs_structure(inputs_info)
                    
                    # è®°å½•é—®é¢˜çš„é•¿åº¦ï¼Œç”¨äºåç»­æˆªå–ç­”æ¡ˆ
                    input_ids_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                    
                    with self.torch.no_grad():
                        # ç”Ÿæˆå‚æ•°
                        generate_kwargs = {
                            "max_length": kwargs.get("max_length", input_ids_length + 20),
                            "num_beams": kwargs.get("num_beams", 5),
                            "min_length": kwargs.get("min_length", 1),
                        }
                        
                        if self._first_call:
                            print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - BLIP Caption Generateè°ƒç”¨:")
                            print(f"  â€¢ input_ids_length: {input_ids_length}")
                            print(f"  â€¢ generate_kwargs: {generate_kwargs}")
                        
                        outputs = self.model.generate(
                            **inputs,
                            **generate_kwargs
                        )
                        
                        if self._first_call:
                            print(f"  â€¢ outputså½¢çŠ¶: {outputs.shape}")
                            print(f"  â€¢ outputs[0]é•¿åº¦: {len(outputs[0])}")
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
                    if self._first_call:
                        outputs_info = self._extract_outputs_info(outputs, input_ids_length)
                    
                    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    if len(outputs[0]) > input_ids_length:
                        answer_ids = outputs[0][input_ids_length:]
                        generated_text = self.processor.decode(
                            answer_ids,
                            skip_special_tokens=True
                        )
                    else:
                        generated_text = self.processor.decode(
                            outputs[0],
                            skip_special_tokens=True
                        )
                        if prompt.lower() in generated_text.lower():
                            generated_text = generated_text.lower().replace(prompt.lower(), "").strip()
                    
                    if self._first_call:
                        print(f"  â€¢ è§£ç åçš„æ–‡æœ¬: '{generated_text}'")
                else:
                    # éBLIPæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
                    inputs = self.processor(
                        text=prompt,
                        images=pil_images,
                        return_tensors="pt"
                    )
                    inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
                             for k, v in inputs.items()}
                    input_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
                    if self._first_call:
                        inputs_info = self._extract_inputs_info(inputs, pil_images)
                        self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
                        print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„:")
                        self._print_inputs_structure(inputs_info)
                        print(f"  â€¢ è¾“å…¥é•¿åº¦ï¼ˆç”¨äºè§£ç ï¼‰: {input_length}")
                    
                    with self.torch.no_grad():
                        # ç”Ÿæˆå‚æ•°
                        generate_kwargs = {
                            "max_new_tokens": max_new_tokens,
                            "temperature": temperature,
                            "top_p": top_p,
                            "do_sample": temperature > 0,
                        }
                        # æ·»åŠ å…¶ä»–kwargs
                        for key, value in kwargs.items():
                            if key not in generate_kwargs and key not in ['max_tokens']:
                                generate_kwargs[key] = value
                        
                        outputs = self.model.generate(
                            **inputs,
                            **generate_kwargs
                        )
                    
                    # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
                    if self._first_call:
                        outputs_info = self._extract_outputs_info(outputs, input_length)
                    
                    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    if input_length > 0 and len(outputs[0]) > input_length:
                        generated_ids = outputs[0][input_length:]
                        generated_text = self.processor.decode(
                            generated_ids,
                            skip_special_tokens=True
                        )
                    else:
                        generated_text = self.processor.decode(
                            outputs[0],
                            skip_special_tokens=True
                        )
                        if input_length > 0 and prompt in generated_text:
                            generated_text = generated_text.replace(prompt, "").strip()
            
            # æ–¹æ³•2: ä½¿ç”¨chatæ¥å£ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            elif hasattr(self.model, 'chat') and pil_images:
                generation_method = "chat_interface"
                # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
                if self._first_call:
                    inputs_info = {
                        "method": "chat",
                        "prompt": prompt,
                        "images_count": len(pil_images) if pil_images else 0,
                        "image_sizes": [img.size for img in pil_images] if pil_images else [],
                        "has_processor": self.has_processor
                    }
                    self._print_first_call_info(prompt, pil_images, max_new_tokens, temperature, top_p, kwargs)
                    print("\nğŸ“¥ Chatæ¥å£è¾“å…¥ä¿¡æ¯:")
                    print(f"  â€¢ å›¾åƒæ•°é‡: {inputs_info['images_count']}")
                    print(f"  â€¢ å›¾åƒå°ºå¯¸: {inputs_info['image_sizes']}")
                    print(f"  â€¢ ä½¿ç”¨Processor: {inputs_info['has_processor']}")
                
                if self.has_processor:
                    response, _ = self.model.chat(
                        self.processor,
                        query=prompt,
                        history=None,
                        images=pil_images,
                        temperature=temperature,
                        top_p=top_p,
                        max_new_tokens=max_new_tokens,
                    )
                else:
                    response, _ = self.model.chat(
                        self.tokenizer,
                        query=prompt,
                        history=None,
                        images=pil_images,
                    )
                generated_text = response
                
                # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
                if self._first_call:
                    outputs_info = {
                        "method": "chat",
                        "response_type": type(response).__name__,
                        "response_length": len(response) if isinstance(response, str) else "N/A"
                    }
            
            # æ–¹æ³•3: çº¯æ–‡æœ¬ç”Ÿæˆ
            else:
                generation_method = "text_only"
                if self.has_processor:
                    inputs = self.processor(text=prompt, return_tensors="pt")
                else:
                    inputs = self.tokenizer(prompt, return_tensors="pt")
                
                inputs = {k: v.to(self.device) if isinstance(v, self.torch.Tensor) else v 
                         for k, v in inputs.items()}
                
                # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜å¹¶æ‰“å°è¾“å…¥ä¿¡æ¯
                if self._first_call:
                    inputs_info = self._extract_inputs_info(inputs, None)
                    self._print_first_call_info(prompt, None, max_new_tokens, temperature, top_p, kwargs)
                    print("\nğŸ“¥ å¤„ç†åçš„è¾“å…¥ç»“æ„:")
                    self._print_inputs_structure(inputs_info)
                
                # ä¿å­˜è¾“å…¥é•¿åº¦
                input_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                
                with self.torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=temperature > 0,
                        **kwargs
                    )
                
                # é¦–æ¬¡è°ƒç”¨æ—¶ä¿å­˜è¾“å‡ºä¿¡æ¯
                if self._first_call:
                    outputs_info = self._extract_outputs_info(outputs, input_length)
                
                # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                if input_length > 0 and len(outputs[0]) > input_length:
                    generated_ids = outputs[0][input_length:]
                    if self.has_processor:
                        generated_text = self.processor.decode(
                            generated_ids,
                            skip_special_tokens=True
                        )
                    else:
                        generated_text = self.tokenizer.decode(
                            generated_ids,
                            skip_special_tokens=True
                        )
                else:
                    if self.has_processor:
                        generated_text = self.processor.decode(
                            outputs[0],
                            skip_special_tokens=True
                        )
                    else:
                        generated_text = self.tokenizer.decode(
                            outputs[0],
                            skip_special_tokens=True
                        )
                    if prompt in generated_text:
                        generated_text = generated_text.replace(prompt, "").strip()
            
            result = {
                "text": generated_text,
                "usage": {"prompt_tokens": 0, "completion_tokens": 0},
                "raw": {"generated_text": generated_text}
            }
            
            # é¦–æ¬¡è°ƒç”¨æ—¶è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            if self._first_call:
                self._print_first_output_info(result, outputs_info, generation_method)
                self._first_call = False
                self._first_output_printed = True
            
            return result
        
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
    def _print_inputs_structure(self, inputs_info: Dict):
        """æ‰“å°è¾“å…¥ç»“æ„ä¿¡æ¯"""
        if "keys" in inputs_info:
            print(f"  â€¢ è¾“å…¥é”®: {inputs_info['keys']}")
        if "tensor_info" in inputs_info:
            print(f"  â€¢ Tensorä¿¡æ¯:")
            for key, info in inputs_info["tensor_info"].items():
                if "shape" in info:
                    print(f"    - {key}: shape={info['shape']}, dtype={info['dtype']}, device={info['device']}")
                else:
                    print(f"    - {key}: {info}")
        if "images" in inputs_info:
            print(f"  â€¢ å›¾åƒä¿¡æ¯:")
            for i, img_info in enumerate(inputs_info["images"]):
                print(f"    - å›¾åƒ {i+1}: {img_info}")
    
    def _print_first_call_info(self, prompt: str, images: Optional[List], max_new_tokens: int, 
                               temperature: float, top_p: float, kwargs: Dict):
        """æ‰“å°é¦–æ¬¡è°ƒç”¨æ—¶çš„è¯¦ç»†ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ” é¦–æ¬¡æ¨¡å‹è°ƒç”¨ - è¾“å…¥ä¿¡æ¯")
        print("="*80)
        
        # æ¨¡å‹ä¿¡æ¯
        print("\nğŸ“¦ æ¨¡å‹ä¿¡æ¯:")
        print(f"  â€¢ æ¨¡å‹ID: {self.model_id}")
        print(f"  â€¢ æ¨¡å‹ç±»å‹: {self.model_class}")
        print(f"  â€¢ è®¾å¤‡: {self.device}")
        print(f"  â€¢ æ˜¯å¦è§†è§‰æ¨¡å‹: {self.is_vision_model}")
        print(f"  â€¢ ä½¿ç”¨Processor: {self.has_processor}")
        if hasattr(self, 'model'):
            print(f"  â€¢ æ¨¡å‹ç±»: {type(self.model).__name__}")
            if hasattr(self.model, 'config'):
                config = self.model.config
                print(f"  â€¢ æ¨¡å‹é…ç½®ç±»å‹: {type(config).__name__}")
                if hasattr(config, 'vocab_size'):
                    print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
                if hasattr(config, 'max_position_embeddings'):
                    print(f"  â€¢ æœ€å¤§ä½ç½®ç¼–ç : {config.max_position_embeddings}")
        
        # è¾“å…¥ä¿¡æ¯
        print("\nğŸ“¥ è¾“å…¥ä¿¡æ¯:")
        print(f"  â€¢ Prompt: {prompt[:200]}{'...' if len(prompt) > 200 else ''}")
        print(f"  â€¢ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        if images:
            print(f"  â€¢ å›¾åƒæ•°é‡: {len(images)}")
            for i, img in enumerate(images):
                if isinstance(img, str):
                    print(f"    - å›¾åƒ {i+1}: {img[:100]}{'...' if len(img) > 100 else ''}")
                elif hasattr(img, 'size'):
                    print(f"    - å›¾åƒ {i+1}: PIL Image, å°ºå¯¸: {img.size}, æ¨¡å¼: {img.mode}")
                else:
                    print(f"    - å›¾åƒ {i+1}: {type(img).__name__}")
        else:
            print(f"  â€¢ å›¾åƒæ•°é‡: 0")
        
        # ç”Ÿæˆå‚æ•°
        print("\nâš™ï¸  ç”Ÿæˆå‚æ•°:")
        print(f"  â€¢ max_new_tokens: {max_new_tokens}")
        print(f"  â€¢ temperature: {temperature}")
        print(f"  â€¢ top_p: {top_p}")
        if kwargs:
            print(f"  â€¢ å…¶ä»–å‚æ•°: {kwargs}")
        
        print("="*80 + "\n")
    
    def _extract_inputs_info(self, inputs: Dict, images: Optional[List]) -> Dict:
        """æå–è¾“å…¥ä¿¡æ¯çš„ç»“æ„åŒ–æ•°æ®"""
        info = {
            "keys": list(inputs.keys()),
            "tensor_info": {}
        }
        
        for key, value in inputs.items():
            if isinstance(value, self.torch.Tensor):
                info["tensor_info"][key] = {
                    "shape": list(value.shape),
                    "dtype": str(value.dtype),
                    "device": str(value.device),
                    "requires_grad": value.requires_grad
                }
            else:
                info["tensor_info"][key] = {
                    "type": type(value).__name__,
                    "value": str(value)[:100] if not isinstance(value, (list, dict)) else f"{type(value).__name__} with {len(value)} items"
                }
        
        if images:
            info["images"] = []
            for img in images:
                if hasattr(img, 'size'):
                    info["images"].append({
                        "type": "PIL.Image",
                        "size": img.size,
                        "mode": img.mode
                    })
                else:
                    info["images"].append({"type": type(img).__name__})
        
        return info
    
    def _extract_outputs_info(self, outputs, input_length: int) -> Dict:
        """æå–è¾“å‡ºä¿¡æ¯çš„ç»“æ„åŒ–æ•°æ®"""
        info = {
            "output_type": type(outputs).__name__,
            "input_length": input_length
        }
        
        if isinstance(outputs, self.torch.Tensor):
            info["shape"] = list(outputs.shape)
            info["dtype"] = str(outputs.dtype)
            info["device"] = str(outputs.device)
        elif isinstance(outputs, (list, tuple)) and len(outputs) > 0:
            first_output = outputs[0] if isinstance(outputs, list) else outputs[0]
            if isinstance(first_output, self.torch.Tensor):
                info["first_output_shape"] = list(first_output.shape)
                info["first_output_dtype"] = str(first_output.dtype)
                info["total_length"] = first_output.shape[0] if len(first_output.shape) > 0 else "N/A"
                info["generated_length"] = first_output.shape[0] - input_length if input_length > 0 else "N/A"
        
        return info
    
    def _print_first_output_info(self, result: Dict, outputs_info: Optional[Dict], generation_method: Optional[str]):
        """æ‰“å°é¦–æ¬¡è°ƒç”¨æ—¶çš„è¾“å‡ºä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“¤ é¦–æ¬¡æ¨¡å‹è°ƒç”¨ - è¾“å‡ºä¿¡æ¯")
        print("="*80)
        
        # ç”Ÿæˆæ–¹æ³•
        if generation_method:
            print(f"\nğŸ”§ ä½¿ç”¨çš„ç”Ÿæˆæ–¹æ³•: {generation_method}")
        
        # è¾“å‡ºç»“æ„
        print("\nğŸ“Š è¾“å‡ºç»“æ„:")
        print(f"  â€¢ è¿”å›ç±»å‹: {type(result).__name__}")
        print(f"  â€¢ è¿”å›é”®: {list(result.keys())}")
        
        # ç”Ÿæˆçš„æ–‡æœ¬
        if "text" in result:
            text = result["text"]
            print(f"\nğŸ’¬ ç”Ÿæˆçš„æ–‡æœ¬:")
            print(f"  â€¢ å†…å®¹: {text[:200]}{'...' if len(text) > 200 else ''}")
            print(f"  â€¢ é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # åŸå§‹è¾“å‡ºä¿¡æ¯
        if outputs_info:
            print(f"\nğŸ” åŸå§‹è¾“å‡ºä¿¡æ¯:")
            if "output_type" in outputs_info:
                print(f"  â€¢ è¾“å‡ºç±»å‹: {outputs_info['output_type']}")
            if "shape" in outputs_info:
                print(f"  â€¢ å½¢çŠ¶: {outputs_info['shape']}")
            if "first_output_shape" in outputs_info:
                print(f"  â€¢ ç¬¬ä¸€ä¸ªè¾“å‡ºå½¢çŠ¶: {outputs_info['first_output_shape']}")
            if "input_length" in outputs_info:
                print(f"  â€¢ è¾“å…¥é•¿åº¦: {outputs_info['input_length']}")
            if "generated_length" in outputs_info:
                print(f"  â€¢ ç”Ÿæˆé•¿åº¦: {outputs_info['generated_length']}")
        
        # Usageä¿¡æ¯
        if "usage" in result:
            print(f"\nğŸ“ˆ Tokenä½¿ç”¨:")
            for key, value in result["usage"].items():
                print(f"  â€¢ {key}: {value}")
        
        # Rawä¿¡æ¯ï¼ˆç®€è¦ï¼‰
        if "raw" in result:
            raw = result["raw"]
            print(f"\nğŸ“¦ Rawå“åº”:")
            print(f"  â€¢ ç±»å‹: {type(raw).__name__}")
            if isinstance(raw, dict):
                print(f"  â€¢ é”®: {list(raw.keys())}")
        
        print("="*80 + "\n")
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "name": self.model_id,
            "type": "huggingface_local",
            "device": self.device,
            "model_class": self.model_class,
            "is_vision_model": self.is_vision_model
        }


# HuggingFace Hub Inference APIé€‚é…å™¨
class HuggingFaceHubAdapter(BaseModelAdapter):
    """HuggingFace Hub Inference APIé€‚é…å™¨"""
    
    def __init__(self, 
                 model_id: str,
                 api_token: Optional[str] = None,
                 api_url: Optional[str] = None,
                 timeout: float = 30.0,
                 **kwargs):
        """
        Args:
            model_id: HuggingFaceæ¨¡å‹ID
            api_token: HuggingFace API tokenï¼ˆå¯é€‰ï¼Œç”¨äºç§æœ‰æ¨¡å‹ï¼‰
            api_url: è‡ªå®šä¹‰API URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨HuggingFace Inference APIï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.model_id = model_id
        self.api_token = api_token
        self.timeout = timeout
        
        if api_url:
            self.api_url = api_url.rstrip("/")
        else:
            self.api_url = f"https://api-inference.huggingface.co/models/{model_id}"
        
        try:
            import requests
            self.requests = requests
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…requests: pip install requests")
    
    def generate(self, prompt: str, images: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        headers = {}
        if self.api_token:
            headers["Authorization"] = f"Bearer {self.api_token}"
        
        # æ„å»ºè¯·æ±‚payload
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get("max_tokens", kwargs.get("max_new_tokens", 1024)),
                "temperature": kwargs.get("temperature", 0.7),
                "top_p": kwargs.get("top_p", 0.9),
            }
        }
        
        # å¤„ç†å›¾åƒ
        if images:
            # HuggingFace Inference APIæ”¯æŒå›¾åƒè¾“å…¥
            # å¯¹äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œinputså¯ä»¥æ˜¯å­—å…¸
            if len(images) == 1:
                img_path = images[0]
                if img_path.startswith("http") or img_path.startswith("data:image"):
                    payload["inputs"] = {
                        "text": prompt,
                        "image": img_path
                    }
                else:
                    # è¯»å–æœ¬åœ°å›¾åƒå¹¶è½¬æ¢ä¸ºbase64
                    from PIL import Image
                    import base64
                    from io import BytesIO
                    
                    img = Image.open(img_path)
                    buffered = BytesIO()
                    img.save(buffered, format="JPEG")
                    img_base64 = base64.b64encode(buffered.getvalue()).decode()
                    payload["inputs"] = {
                        "text": prompt,
                        "image": f"data:image/jpeg;base64,{img_base64}"
                    }
        
        try:
            response = self.requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            result = response.json()
            
            # è§£æå“åº”ï¼ˆHuggingFace APIè¿”å›æ ¼å¼å¯èƒ½ä¸åŒï¼‰
            if isinstance(result, list) and len(result) > 0:
                if "generated_text" in result[0]:
                    text = result[0]["generated_text"]
                elif "answer" in result[0]:
                    text = result[0]["answer"]
                else:
                    text = str(result[0])
            elif isinstance(result, dict):
                text = result.get("generated_text", result.get("answer", str(result)))
            else:
                text = str(result)
            
            # ç§»é™¤è¾“å…¥éƒ¨åˆ†
            if prompt in text:
                text = text.replace(prompt, "").strip()
            
            return {
                "text": text,
                "usage": {},
                "raw": result
            }
        
        except Exception as e:
            raise RuntimeError(f"HuggingFace APIè°ƒç”¨å¤±è´¥: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "name": self.model_id,
            "type": "huggingface_hub_api",
            "api_url": self.api_url
        }


# æ³¨å†Œé»˜è®¤é€‚é…å™¨
ModelAdapterFactory.register("openai", OpenAIAdapter)
ModelAdapterFactory.register("huggingface", HuggingFaceAdapter)
ModelAdapterFactory.register("hf", HuggingFaceAdapter)  # ç®€å†™
ModelAdapterFactory.register("huggingface_hub", HuggingFaceHubAdapter)
ModelAdapterFactory.register("hf_hub", HuggingFaceHubAdapter)  # ç®€å†™