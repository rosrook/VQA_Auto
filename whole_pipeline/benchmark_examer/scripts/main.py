# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# Benchmarkæµ‹è¯•ç³»ç»Ÿä¸»å…¥å£
# """

# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# Benchmarkæµ‹è¯•ç³»ç»Ÿä¸»å…¥å£
# """

# import json
# import argparse
# import sys
# from pathlib import Path
# from typing import Dict, Any, List

# # æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
# current_dir = Path(__file__).parent
# sys.path.insert(0, str(current_dir))

# from ..core.model_adapter import BaseModelAdapter, ModelAdapterFactory
# from ..core.benchmark_manager import BenchmarkManager
# from ..core.test_executor import TestExecutor
# from ..core.result_analyzer import ResultAnalyzer


# def load_model_config(config_path: str) -> Dict[str, Any]:
#     """åŠ è½½æ¨¡å‹é…ç½®"""
#     with open(config_path, 'r', encoding='utf-8') as f:
#         return json.load(f)


# def create_model_adapter(config: Dict[str, Any]) -> BaseModelAdapter:
#     """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹é€‚é…å™¨"""
#     adapter_type = config.get("type", "openai")
#     params = config.get("params", {})
    
#     # å¦‚æœæ˜¯è‡ªå®šä¹‰é€‚é…å™¨ï¼Œéœ€è¦å…ˆæ³¨å†Œ
#     if "custom_adapter_class" in config:
#         adapter_class = config["custom_adapter_class"]
#         ModelAdapterFactory.register(adapter_type, adapter_class)
    
#     return ModelAdapterFactory.create(adapter_type, **params)


# def main():
#     parser = argparse.ArgumentParser(description="Benchmarkæµ‹è¯•ç³»ç»Ÿ")
#     parser.add_argument("--model-config", type=str, required=True,
#                        help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
#     parser.add_argument("--model-type", type=str, default=None,
#                        help="æ¨¡å‹ç±»å‹ï¼ˆç”¨äºç­›é€‰benchmarkï¼Œå¯é€‰ï¼‰")
#     parser.add_argument("--benchmark-config", type=str, default=None,
#                        help="Benchmarké…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‡å®šè¦æµ‹è¯•çš„benchmarkï¼‰")
#     parser.add_argument("--benchmark-names", type=str, nargs="+", default=None,
#                        help="æŒ‡å®šè¦æµ‹è¯•çš„benchmarkåç§°åˆ—è¡¨ï¼ˆå¦‚: GQA CLEVRï¼‰")
#     parser.add_argument("--taxonomy-file", type=str, default=None,
#                        help="Benchmarkåˆ†ç±»æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨available_benchmarks_with_internal_taxonomy.jsonï¼‰")
#     parser.add_argument("--requirements", type=str, default=None,
#                        help="éœ€æ±‚é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©benchmarkï¼‰")
#     parser.add_argument("--output", type=str, default="benchmark_results.json",
#                        help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
#     parser.add_argument("--max-samples", type=int, default=None,
#                        help="æ¯ä¸ªbenchmarkæœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
#     parser.add_argument("--accuracy-threshold", type=float, default=0.7,
#                        help="å‡†ç¡®ç‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºè¡¨ç°ä¸å¥½")
#     parser.add_argument("--list-benchmarks", action="store_true",
#                        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„benchmarkå¹¶é€€å‡º")
    
#     args = parser.parse_args()
    
#     # åŠ è½½æ¨¡å‹é…ç½®
#     print("="*60)
#     print("åŠ è½½æ¨¡å‹é…ç½®...")
#     print("="*60)
#     model_config = load_model_config(args.model_config)
#     model_adapter = create_model_adapter(model_config)
#     model_info = model_adapter.get_model_info()
#     print(f"âœ“ æ¨¡å‹: {model_info.get('name')} ({model_info.get('type')})")
    
#     # åˆå§‹åŒ–Benchmarkç®¡ç†å™¨
#     print("\n" + "="*60)
#     print("åˆå§‹åŒ–Benchmarkç®¡ç†å™¨...")
#     print("="*60)
    
#     taxonomy_file = args.taxonomy_file
#     if not taxonomy_file:
#         # é»˜è®¤è·¯å¾„
#         taxonomy_file = str(Path(__file__).parent.parent / "benchmarks" / "available_benchmarks_with_internal_taxonomy.json")
    
#     benchmark_manager = BenchmarkManager(
#         taxonomy_file=taxonomy_file,
#         model_type=args.model_type
#     )
    
#     # åˆ—å‡ºæ‰€æœ‰å¯ç”¨benchmark
#     if args.list_benchmarks:
#         print("\nå¯ç”¨Benchmarkåˆ—è¡¨:")
#         print("="*60)
#         available = benchmark_manager.list_available_benchmarks()
#         for bench in available:
#             print(f"\nåç§°: {bench['name']}")
#             print(f"  HuggingFace ID: {bench['hf_id']}")
#             print(f"  é…ç½®: {bench.get('config', 'default')}")
#             print(f"  é»˜è®¤Split: {bench.get('default_split', 'N/A')}")
#             print(f"  å¯ç”¨Splits: {', '.join(bench.get('available_splits', []))}")
#             print(f"  åˆ†ç±»å­—æ®µ: {', '.join(bench.get('native_taxonomy_fields', []))}")
#         return
    
#     # æ ¹æ®éœ€æ±‚é€‰æ‹©benchmark
#     requirements = None
#     if args.requirements:
#         with open(args.requirements, 'r', encoding='utf-8') as f:
#             requirements = json.load(f)
#         print(f"æ ¹æ®éœ€æ±‚é€‰æ‹©benchmark: {requirements}")
    
#     # å¦‚æœæŒ‡å®šäº†benchmarkåç§°ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™æ ¹æ®éœ€æ±‚ç­›é€‰
#     selected_benchmarks = benchmark_manager.select_benchmarks(
#         requirements=requirements,
#         benchmark_names=args.benchmark_names
#     )
    
#     if not selected_benchmarks:
#         print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„benchmark")
#         return
    
#     print(f"âœ“ é€‰æ‹©äº† {len(selected_benchmarks)} ä¸ªbenchmark")
#     for bench in selected_benchmarks:
#         info = bench.get_info()
#         print(f"  - {info['name']}: {info['num_tasks']} ä¸ªä»»åŠ¡")
    
#     # æ‰§è¡Œæµ‹è¯•
#     print("\n" + "="*60)
#     print("å¼€å§‹æ‰§è¡Œæµ‹è¯•...")
#     print("="*60)
    
#     test_executor = TestExecutor(model_adapter)
#     result_analyzer = ResultAnalyzer(accuracy_threshold=args.accuracy_threshold)
    
#     all_results = []
    
#     for benchmark in selected_benchmarks:
#         print(f"\næµ‹è¯• Benchmark: {benchmark.name}")
#         results = test_executor.run_benchmark(
#             benchmark=benchmark,
#             max_samples=args.max_samples,
#             verbose=True
#         )
        
#         # åˆ†æç»“æœ
#         summary = result_analyzer.analyze_results(benchmark.name, results)
        
#         all_results.append({
#             "benchmark_name": summary.benchmark_name,
#             "total_tasks": summary.total_tasks,
#             "correct_count": summary.correct_count,
#             "accuracy": summary.accuracy,
#             "average_score": summary.average_score,
#             "failed_cases": summary.failed_cases,
#             "all_results": [
#                 {
#                     "task_id": r.task_id,
#                     "question": r.question,
#                     "ground_truth": r.ground_truth,
#                     "model_answer": r.model_answer,
#                     "is_correct": r.is_correct,
#                     "score": r.score,
#                     "metadata": r.metadata
#                 }
#                 for r in results
#             ]
#         })
    
#     # è¯†åˆ«è¡¨ç°ä¸å¥½çš„benchmark
#     summaries = [
#         ResultAnalyzer.BenchmarkSummary(
#             benchmark_name=r["benchmark_name"],
#             total_tasks=r["total_tasks"],
#             correct_count=r["correct_count"],
#             accuracy=r["accuracy"],
#             average_score=r["average_score"],
#             failed_cases=r["failed_cases"]
#         )
#         for r in all_results
#     ]
    
#     poor_performers = result_analyzer.identify_poor_performers(summaries)
    
#     # æ•´ç†æœ€ç»ˆç»“æœ
#     # æ ¼å¼ï¼šæ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ªbenchçš„æµ‹è¯•ç»“æœå’Œæ¨¡å‹è¡¨ç°ä¸å¥½çš„benchæ¡ˆä¾‹
#     output_data = []
    
#     for result in all_results:
#         bench_result = {
#             "benchmark_name": result["benchmark_name"],
#             "total_tasks": result["total_tasks"],
#             "correct_count": result["correct_count"],
#             "accuracy": result["accuracy"],
#             "average_score": result["average_score"],
#             "is_poor_performer": result["benchmark_name"] in poor_performers,
#             "failed_cases": result["failed_cases"],  # è¡¨ç°ä¸å¥½çš„æ¡ˆä¾‹
#             "all_results": result["all_results"]
#         }
#         output_data.append(bench_result)
    
#     # æ·»åŠ æ€»ä½“æ‘˜è¦
#     summary_data = {
#         "model_info": model_info,
#         "summary": {
#             "total_benchmarks": len(all_results),
#             "poor_performers": poor_performers,
#             "overall_accuracy": sum(s.accuracy for s in summaries) / len(summaries) if summaries else 0.0
#         },
#         "benchmark_results": output_data
#     }
    
#     # ä¿å­˜ç»“æœ
#     with open(args.output, 'w', encoding='utf-8') as f:
#         json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
#     print("\n" + "="*60)
#     print("æµ‹è¯•å®Œæˆï¼")
#     print("="*60)
#     print(f"ç»“æœä¿å­˜åˆ°: {args.output}")
#     print(f"\næ€»ä½“ç»Ÿè®¡:")
#     print(f"  æµ‹è¯•çš„Benchmarkæ•°é‡: {len(all_results)}")
#     print(f"  è¡¨ç°ä¸å¥½çš„Benchmark: {len(poor_performers)}")
#     if poor_performers:
#         print(f"  è¡¨ç°ä¸å¥½çš„Benchmarkåˆ—è¡¨: {', '.join(poor_performers)}")
#     print(f"  å¹³å‡å‡†ç¡®ç‡: {summary_info['summary']['overall_accuracy']:.2%}")
    
#     # æ˜¾ç¤ºæ¯ä¸ªbenchmarkçš„æ‘˜è¦
#     print(f"\nå„Benchmarkç»“æœ:")
#     for result in all_results:
#         print(f"  {result['benchmark_name']}:")
#         print(f"    å‡†ç¡®ç‡: {result['accuracy']:.2%}")
#         print(f"    å¤±è´¥æ¡ˆä¾‹æ•°: {len(result['failed_cases'])}")


# if __name__ == "__main__":
#     main()





# # python scripts/main.py \
# #     --model-config configs/example_hf_model_config.json \
# #     --benchmark-names GQA \
# #     --output results.json





























#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Benchmarkæµ‹è¯•ç³»ç»Ÿä¸»å…¥å£
"""

import json
import argparse
import sys
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from ..core.model_adapter import BaseModelAdapter, ModelAdapterFactory
from ..core.benchmark_manager import BenchmarkManager
from ..core.test_executor import TestExecutor
from ..core.result_analyzer import ResultAnalyzer, BenchmarkSummary


def load_model_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def create_model_adapter(config: Dict[str, Any]) -> BaseModelAdapter:
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹é€‚é…å™¨"""
    adapter_type = config.get("type", "openai")
    params = config.get("params", {})
    
    # å¦‚æœæ˜¯è‡ªå®šä¹‰é€‚é…å™¨ï¼Œéœ€è¦å…ˆæ³¨å†Œ
    if "custom_adapter_class" in config:
        adapter_class = config["custom_adapter_class"]
        ModelAdapterFactory.register(adapter_type, adapter_class)
    
    return ModelAdapterFactory.create(adapter_type, **params)


def main():
    parser = argparse.ArgumentParser(description="Benchmarkæµ‹è¯•ç³»ç»Ÿ")
    parser.add_argument("--model-config", type=str, required=True,
                       help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--model-type", type=str, default=None,
                       help="æ¨¡å‹ç±»å‹ï¼ˆç”¨äºç­›é€‰benchmarkï¼Œå¯é€‰ï¼‰")
    parser.add_argument("--benchmark-config", type=str, default=None,
                       help="Benchmarké…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‡å®šè¦æµ‹è¯•çš„benchmarkï¼‰")
    parser.add_argument("--benchmark-names", type=str, nargs="+", default=None,
                       help="æŒ‡å®šè¦æµ‹è¯•çš„benchmarkåç§°åˆ—è¡¨ï¼ˆå¦‚: GQA CLEVRï¼‰")
    parser.add_argument("--taxonomy-file", type=str, default=None,
                       help="Benchmarkåˆ†ç±»æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨available_benchmarks_with_internal_taxonomy.jsonï¼‰")
    parser.add_argument("--requirements", type=str, default=None,
                       help="éœ€æ±‚é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©benchmarkï¼‰")
    parser.add_argument("--output", type=str, default="benchmark_results.json",
                       help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æ¯ä¸ªbenchmarkæœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    parser.add_argument("--batch-size", type=int, default=1,
                       help="æ‰¹å¤„ç†å¤§å°ï¼ˆç”¨äºæ‰¹é‡å¤„ç†ä»»åŠ¡ï¼Œé»˜è®¤1è¡¨ç¤ºé€ä¸ªå¤„ç†ï¼‰")
    parser.add_argument("--accuracy-threshold", type=float, default=0.7,
                       help="å‡†ç¡®ç‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºè¡¨ç°ä¸å¥½")
    parser.add_argument("--list-benchmarks", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„benchmarkå¹¶é€€å‡º")
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹é…ç½®
    print("="*60)
    print("åŠ è½½æ¨¡å‹é…ç½®...")
    print("="*60)
    model_config = load_model_config(args.model_config)
    model_adapter = create_model_adapter(model_config)
    model_info = model_adapter.get_model_info()
    print(f"âœ“ æ¨¡å‹: {model_info.get('name')} ({model_info.get('type')})")
    
    # åˆå§‹åŒ–Benchmarkç®¡ç†å™¨
    print("\n" + "="*60)
    print("åˆå§‹åŒ–Benchmarkç®¡ç†å™¨...")
    print("="*60)
    
    taxonomy_file = args.taxonomy_file
    if not taxonomy_file:
        # é»˜è®¤è·¯å¾„
        taxonomy_file = str(Path(__file__).parent.parent / "benchmarks" / "available_benchmarks_with_internal_taxonomy.json")
    
    benchmark_manager = BenchmarkManager(
        taxonomy_file=taxonomy_file,
        model_type=args.model_type
    )
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨benchmark
    if args.list_benchmarks:
        print("\nå¯ç”¨Benchmarkåˆ—è¡¨:")
        print("="*60)
        available = benchmark_manager.list_available_benchmarks()
        for bench in available:
            print(f"\nåç§°: {bench['name']}")
            print(f"  HuggingFace ID: {bench['hf_id']}")
            print(f"  é…ç½®: {bench.get('config', 'default')}")
            print(f"  é»˜è®¤Split: {bench.get('default_split', 'N/A')}")
            print(f"  å¯ç”¨Splits: {', '.join(bench.get('available_splits', []))}")
            print(f"  åˆ†ç±»å­—æ®µ: {', '.join(bench.get('native_taxonomy_fields', []))}")
        return
    
    # æ ¹æ®éœ€æ±‚é€‰æ‹©benchmark
    requirements = None
    if args.requirements:
        with open(args.requirements, 'r', encoding='utf-8') as f:
            requirements = json.load(f)
        print(f"æ ¹æ®éœ€æ±‚é€‰æ‹©benchmark: {requirements}")
    
    # è·å–è¦æµ‹è¯•çš„benchmarkåç§°åˆ—è¡¨ï¼ˆä¸åŠ è½½æ•°æ®ï¼‰
    selected_benchmark_names = benchmark_manager.select_benchmark_names(
        requirements=requirements,
        benchmark_names=args.benchmark_names
    )
    
    if not selected_benchmark_names:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„benchmark")
        return
    
    print(f"âœ“ é€‰æ‹©äº† {len(selected_benchmark_names)} ä¸ªbenchmark: {', '.join(selected_benchmark_names)}")
    
    # æ‰§è¡Œæµ‹è¯•ï¼ˆæŒ‰éœ€åŠ è½½ï¼Œè¾¹åŠ è½½è¾¹æµ‹è¯•ï¼‰
    print("\n" + "="*60)
    print("å¼€å§‹æ‰§è¡Œæµ‹è¯•ï¼ˆæµå¼åŠ è½½æ¨¡å¼ï¼‰...")
    print("="*60)
    
    test_executor = TestExecutor(model_adapter, verbose=True)
    result_analyzer = ResultAnalyzer(accuracy_threshold=args.accuracy_threshold)
    
    all_results = []
    
    # é€ä¸ªåŠ è½½å’Œæµ‹è¯•benchmarkï¼ˆé¿å…å†…å­˜å ç”¨è¿‡å¤§ï¼‰
    for bench_name in selected_benchmark_names:
        print(f"\n{'='*60}")
        print(f"åŠ è½½å¹¶æµ‹è¯• Benchmark: {bench_name}")
        print(f"{'='*60}")
        
        # æŒ‰éœ€åŠ è½½benchmarkï¼ˆä¸åŠ è½½æ•°æ®ï¼Œä½¿ç”¨æµå¼ï¼‰
        benchmark = benchmark_manager.load_benchmark(bench_name, load_data=False)
        
        if benchmark is None:
            print(f"âœ— æ— æ³•åŠ è½½benchmark: {bench_name}")
            continue
        
        print(f"âœ“ BenchmarkåŠ è½½æˆåŠŸ: {bench_name}")
        
        # è¿è¡Œæµ‹è¯•ï¼ˆæµå¼å¤„ç†ï¼‰
        print(f"\n  ğŸ“ Benchmarkä¿¡æ¯:")
        bench_info = benchmark.get_info()
        print(f"     åç§°: {bench_info['name']}")
        print(f"     HF ID: {bench_info.get('hf_id', 'N/A')}")
        print(f"     Split: {bench_info.get('split', 'N/A')}")
        print(f"     æ¨¡å¼: {'æµå¼åŠ è½½' if bench_info.get('use_streaming') else 'æ‰¹é‡åŠ è½½'}")
        
        try:
            results = test_executor.run_benchmark(
                benchmark=benchmark,
                max_samples=args.max_samples,
                batch_size=args.batch_size,
                verbose=True
            )
            
            if not results:
                print(f"  âš ï¸  è­¦å‘Š: æ²¡æœ‰è·å¾—ä»»ä½•æµ‹è¯•ç»“æœ")
                continue
            
            print(f"\n  ğŸ“Š æ”¶é›†åˆ° {len(results)} ä¸ªæµ‹è¯•ç»“æœ")
        
        except KeyboardInterrupt:
            print(f"\n  âš ï¸  ç”¨æˆ·ä¸­æ–­äº† {bench_name} çš„æµ‹è¯•")
            break
        
        except Exception as e:
            print(f"\n  âœ— æµ‹è¯• {bench_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # åˆ†æç»“æœ
        print(f"\n  ğŸ“ˆ åˆ†æç»“æœ...")
        summary = result_analyzer.analyze_results(benchmark.name, results)
        
        print(f"     æ€»ä»»åŠ¡æ•°: {summary.total_tasks}")
        print(f"     æ­£ç¡®ç­”æ¡ˆ: {summary.correct_count}")
        print(f"     å‡†ç¡®ç‡: {summary.accuracy:.2%}")
        print(f"     å¹³å‡åˆ†æ•°: {summary.average_score:.3f}")
        print(f"     å¤±è´¥æ¡ˆä¾‹: {len(summary.failed_cases)}")
        
        all_results.append({
            "benchmark_name": summary.benchmark_name,
            "total_tasks": summary.total_tasks,
            "correct_count": summary.correct_count,
            "accuracy": summary.accuracy,
            "average_score": summary.average_score,
            "failed_cases": summary.failed_cases,
            "all_results": [
                {
                    "task_id": r.task_id,
                    "question": r.question,
                    "ground_truth": r.ground_truth,
                    "model_answer": r.model_answer,
                    "is_correct": r.is_correct,
                    "score": r.score,
                    "metadata": r.metadata
                }
                for r in results
            ]
        })
    
    # è¯†åˆ«è¡¨ç°ä¸å¥½çš„benchmark
    summaries = [
        BenchmarkSummary(
            benchmark_name=r["benchmark_name"],
            total_tasks=r["total_tasks"],
            correct_count=r["correct_count"],
            accuracy=r["accuracy"],
            average_score=r["average_score"],
            failed_cases=r["failed_cases"]
        )
        for r in all_results
    ]
    
    poor_performers = result_analyzer.identify_poor_performers(summaries)
    
    # æ•´ç†æœ€ç»ˆç»“æœ
    # æ ¼å¼ï¼šæ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ªbenchçš„æµ‹è¯•ç»“æœå’Œæ¨¡å‹è¡¨ç°ä¸å¥½çš„benchæ¡ˆä¾‹
    output_data = []
    
    for result in all_results:
        bench_result = {
            "benchmark_name": result["benchmark_name"],
            "total_tasks": result["total_tasks"],
            "correct_count": result["correct_count"],
            "accuracy": result["accuracy"],
            "average_score": result["average_score"],
            "is_poor_performer": result["benchmark_name"] in poor_performers,
            "failed_cases": result["failed_cases"],  # è¡¨ç°ä¸å¥½çš„æ¡ˆä¾‹
            "all_results": result["all_results"]
        }
        output_data.append(bench_result)
    
    # æ·»åŠ æ€»ä½“æ‘˜è¦
    summary_data = {
        "model_info": model_info,
        "summary": {
            "total_benchmarks": len(all_results),
            "poor_performers": poor_performers,
            "overall_accuracy": sum(s.accuracy for s in summaries) / len(summaries) if summaries else 0.0
        },
        "benchmark_results": output_data
    }
    
    # ä¿å­˜ç»“æœ
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print(f"ç»“æœä¿å­˜åˆ°: {args.output}")
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  æµ‹è¯•çš„Benchmarkæ•°é‡: {len(all_results)}")
    print(f"  è¡¨ç°ä¸å¥½çš„Benchmark: {len(poor_performers)}")
    if poor_performers:
        print(f"  è¡¨ç°ä¸å¥½çš„Benchmarkåˆ—è¡¨: {', '.join(poor_performers)}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {summary_data['summary']['overall_accuracy']:.2%}")
    
    # æ˜¾ç¤ºæ¯ä¸ªbenchmarkçš„æ‘˜è¦
    print(f"\nå„Benchmarkç»“æœ:")
    for result in all_results:
        print(f"  {result['benchmark_name']}:")
        print(f"    å‡†ç¡®ç‡: {result['accuracy']:.2%}")
        print(f"    å¤±è´¥æ¡ˆä¾‹æ•°: {len(result['failed_cases'])}")


if __name__ == "__main__":
    main()





# python scripts/main.py \
#     --model-config configs/example_hf_model_config.json \
#     --benchmark-names GQA \
#     --output results.json
#     --batch-size 10 \
#     --max-samples 20