# æ¨¡å‹è°ƒç”¨ä»£ç æ‰§è¡Œæµç¨‹è¯¦è§£

## ğŸ“‹ æ•´ä½“æµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·å¯åŠ¨æµ‹è¯•
    â†“
TestExecutor.run_benchmark()  [å…¥å£ç‚¹]
    â†“
é€‰æ‹©å¤„ç†æ¨¡å¼ï¼ˆæµå¼/æ‰¹é‡ï¼‰
    â†“
_process_task_batch()  [æ ¸å¿ƒå¤„ç†å‡½æ•°]
    â†“
å¯¹æ¯ä¸ªä»»åŠ¡ï¼š
    â”œâ”€ _build_prompt()  [æ„å»ºprompt]
    â”œâ”€ model_adapter.generate()  [è°ƒç”¨æ¨¡å‹]
    â”œâ”€ æå–ç­”æ¡ˆ
    â”œâ”€ benchmark.evaluate_answer()  [è¯„ä¼°ç­”æ¡ˆ]
    â””â”€ æ”¶é›†ç»“æœ
```

---

## ğŸ” è¯¦ç»†æµç¨‹è§£æ

### 1ï¸âƒ£ å…¥å£ç‚¹ï¼š`run_benchmark()`

**æ–‡ä»¶ä½ç½®**: `core/test_executor.py` ç¬¬149è¡Œ

**ä½œç”¨**: æµ‹è¯•æ‰§è¡Œçš„å…¥å£å‡½æ•°ï¼Œå†³å®šä½¿ç”¨æµå¼è¿˜æ˜¯æ‰¹é‡å¤„ç†æ¨¡å¼

**å…³é”®ä»£ç **:
```python
def run_benchmark(self, benchmark, max_samples, batch_size, verbose):
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼åŠ è½½
    use_streaming = hasattr(benchmark, 'get_dataset_iterator') and 
                    hasattr(benchmark, '_use_streaming') and 
                    benchmark._use_streaming
    
    if use_streaming:
        # æµå¼å¤„ç†æ¨¡å¼
        results = self._run_benchmark_streaming(...)
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        tasks = benchmark.get_tasks()
        results = self._process_tasks(benchmark, tasks_iter, batch_size)
```

**æµç¨‹åˆ†æ”¯**:
- **æµå¼æ¨¡å¼**: æ•°æ®é›†å¾ˆå¤§æ—¶ï¼Œé€é¡¹åŠ è½½å’Œå¤„ç†
- **æ‰¹é‡æ¨¡å¼**: æ•°æ®é›†è¾ƒå°æ—¶ï¼Œä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ä»»åŠ¡

---

### 2ï¸âƒ£ ä»»åŠ¡å¤„ç†ï¼š`_process_task_batch()`

**æ–‡ä»¶ä½ç½®**: `core/test_executor.py` ç¬¬417è¡Œ

**ä½œç”¨**: å¤„ç†ä¸€æ‰¹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«ä¸€ä¸ªé—®é¢˜å’Œå›¾åƒ

**æ‰§è¡Œæ­¥éª¤**:

#### æ­¥éª¤1: æ„å»ºPrompt (ç¬¬424è¡Œ)
```python
prompt = self._build_prompt(task)
```

#### æ­¥éª¤2: è°ƒç”¨æ¨¡å‹ (ç¬¬432-435è¡Œ)
```python
model_response = self.model_adapter.generate(
    prompt=prompt,
    images=task.images
)
```

#### æ­¥éª¤3: æå–ç­”æ¡ˆ (ç¬¬457è¡Œ)
```python
model_answer = model_response.get("text", "")
```

#### æ­¥éª¤4: è¯„ä¼°ç­”æ¡ˆ (ç¬¬460-464è¡Œ)
```python
result = benchmark.evaluate_answer(
    model_answer=model_answer,
    ground_truth=task.ground_truth,
    task=task
)
```

---

### 3ï¸âƒ£ Promptæ„å»ºï¼š`_build_prompt()`

**æ–‡ä»¶ä½ç½®**: `core/test_executor.py` ç¬¬494è¡Œ

**ä½œç”¨**: å°†ä»»åŠ¡ä¸­çš„é—®é¢˜è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„promptæ ¼å¼

**å½“å‰å®ç°**:
```python
def _build_prompt(self, task: BenchmarkTask) -> str:
    prompt = task.question  # ç›´æ¥ä½¿ç”¨é—®é¢˜æ–‡æœ¬
    return prompt
```

**å¯æ‰©å±•æ€§**: 
- å¯ä»¥æ ¹æ®ä¸åŒæ¨¡å‹ç±»å‹è°ƒæ•´promptæ ¼å¼
- ä¾‹å¦‚ï¼š`f"Question: {task.question} Answer:"`

---

### 4ï¸âƒ£ æ¨¡å‹è°ƒç”¨ï¼š`model_adapter.generate()`

**æ–‡ä»¶ä½ç½®**: `core/model_adapter.py` ç¬¬835è¡Œ

**è¿™æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒï¼** è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹ï¼š

#### 4.1 å›¾åƒé¢„å¤„ç† (ç¬¬841-865è¡Œ)

```python
# å¤„ç†ä¸åŒç±»å‹çš„å›¾åƒè¾“å…¥
pil_images = []
for img_path in images:
    if isinstance(img_path, str):
        if img_path.startswith("http"):
            # ä»URLåŠ è½½å›¾åƒ
        elif img_path.startswith("data:image"):
            # Base64ç¼–ç çš„å›¾åƒ
        else:
            # æœ¬åœ°æ–‡ä»¶è·¯å¾„
            pil_images.append(Image.open(img_path))
    else:
        # å·²ç»æ˜¯PIL Imageå¯¹è±¡
        pil_images.append(img_path)
```

#### 4.2 æ¨¡å‹ç”Ÿæˆï¼ˆä¸‰ç§æ–¹æ³•ï¼‰

##### æ–¹æ³•1: ä½¿ç”¨Processorï¼ˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰ (ç¬¬868-914è¡Œ)

**é€‚ç”¨äº**: BLIPã€CLIPç­‰ä½¿ç”¨processorçš„æ¨¡å‹

```python
if self.has_processor and pil_images:
    # 1. ä½¿ç”¨processorå¤„ç†æ–‡æœ¬å’Œå›¾åƒ
    inputs = self.processor(
        text=prompt,
        images=pil_images,
        return_tensors="pt"
    )
    
    # 2. ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    inputs = {k: v.to(self.device) if isinstance(v, Tensor) else v 
             for k, v in inputs.items()}
    
    # 3. ä¿å­˜è¾“å…¥é•¿åº¦ï¼ˆç”¨äºåç»­åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    input_length = inputs['input_ids'].shape[1]
    
    # 4. æ¨¡å‹ç”Ÿæˆ
    with torch.no_grad():
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=temperature > 0,
        )
    
    # 5. åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    if input_length > 0 and len(outputs[0]) > input_length:
        generated_ids = outputs[0][input_length:]  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = self.processor.decode(
            generated_ids,
            skip_special_tokens=True
        )
```

**å…³é”®ç‚¹**: 
- ä½¿ç”¨ `outputs[0][input_length:]` åªè§£ç æ–°ç”Ÿæˆçš„token
- é¿å…é‡å¤åŒ…å«è¾“å…¥promptçš„é—®é¢˜

##### æ–¹æ³•2: ä½¿ç”¨Chatæ¥å£ (ç¬¬916-935è¡Œ)

**é€‚ç”¨äº**: æ”¯æŒchatæ¥å£çš„æ¨¡å‹ï¼ˆå¦‚Qwen-VLï¼‰

```python
elif hasattr(self.model, 'chat') and pil_images:
    response, _ = self.model.chat(
        self.processor,
        query=prompt,
        history=None,
        images=pil_images,
        temperature=temperature,
        top_p=top_p,
        max_new_tokens=max_new_tokens,
    )
    generated_text = response
```

##### æ–¹æ³•3: çº¯æ–‡æœ¬ç”Ÿæˆ (ç¬¬937-990è¡Œ)

**é€‚ç”¨äº**: çº¯æ–‡æœ¬æ¨¡å‹æˆ–æ²¡æœ‰å›¾åƒçš„æƒ…å†µ

```python
else:
    # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬
    inputs = self.tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(self.device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    outputs = self.model.generate(**inputs, ...)
    
    # è§£ç ï¼ˆåŒæ ·åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    generated_text = self.tokenizer.decode(outputs[0][input_length:], ...)
```

#### 4.3 è¿”å›ç»“æœ (ç¬¬994-998è¡Œ)

```python
return {
    "text": generated_text,  # æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç­”æ¡ˆ
    "usage": {"prompt_tokens": 0, "completion_tokens": 0},
    "raw": {"generated_text": generated_text}  # åŸå§‹å“åº”ï¼ˆå¯é€‰ï¼‰
}
```

---

### 5ï¸âƒ£ ç­”æ¡ˆè¯„ä¼°ï¼š`benchmark.evaluate_answer()`

**æ–‡ä»¶ä½ç½®**: `benchmarks/huggingface_benchmark.py` ç¬¬797è¡Œ

**ä½œç”¨**: å°†æ¨¡å‹ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒï¼Œç»™å‡ºæ­£ç¡®æ€§å’Œåˆ†æ•°

**ç¤ºä¾‹å®ç°**:
```python
def evaluate_answer(self, model_answer, ground_truth, task):
    model_answer_clean = str(model_answer).strip().lower()
    ground_truth_clean = str(ground_truth).strip().lower()
    
    is_correct = model_answer_clean == ground_truth_clean
    score = 1.0 if is_correct else 0.0
    
    return BenchmarkResult(
        task_id=task.task_id,
        question=task.question,
        ground_truth=ground_truth,
        model_answer=model_answer,
        is_correct=is_correct,
        score=score,
        ...
    )
```

---

## ğŸ”„ å®Œæ•´æ‰§è¡Œç¤ºä¾‹

å‡è®¾æœ‰ä¸€ä¸ªGQAä»»åŠ¡ï¼š
- **é—®é¢˜**: "What color is the car?"
- **å›¾åƒ**: ä¸€å¼ åŒ…å«çº¢è‰²æ±½è½¦çš„å›¾ç‰‡
- **æ ‡å‡†ç­”æ¡ˆ**: "red"

### æ‰§è¡Œæµç¨‹ï¼š

1. **run_benchmark()** 
   - é€‰æ‹©æ‰¹é‡å¤„ç†æ¨¡å¼
   - åŠ è½½æ‰€æœ‰ä»»åŠ¡

2. **_process_task_batch()** - å¤„ç†è¿™ä¸ªä»»åŠ¡
   
3. **_build_prompt()**
   - è¾“å…¥: `task.question = "What color is the car?"`
   - è¾“å‡º: `prompt = "What color is the car?"`

4. **model_adapter.generate()**
   - è¾“å…¥å›¾åƒè½¬æ¢ä¸ºPIL Image
   - Processorå¤„ç†: `processor(text="What color is the car?", images=[PIL_Image])`
   - æ¨¡å‹ç”Ÿæˆ: `model.generate(**inputs)` â†’ token IDs
   - è§£ç : `processor.decode(generated_ids)` â†’ `"red"`
   - è¿”å›: `{"text": "red", "usage": {...}, "raw": {...}}`

5. **æå–ç­”æ¡ˆ**
   - `model_answer = "red"`

6. **evaluate_answer()**
   - æ¯”è¾ƒ: `"red" == "red"` â†’ `True`
   - è¿”å›: `BenchmarkResult(is_correct=True, score=1.0)`

7. **æ”¶é›†ç»“æœ**
   - æ·»åŠ åˆ°resultsåˆ—è¡¨
   - è¿”å›ç»™è°ƒç”¨è€…

---

## ğŸ¯ å…³é”®è®¾è®¡è¦ç‚¹

1. **é€‚é…å™¨æ¨¡å¼**: ä½¿ç”¨ `model_adapter` æŠ½è±¡ä¸åŒæ¨¡å‹çš„æ¥å£
2. **æµå¼å¤„ç†**: æ”¯æŒå¤§æ•°æ®é›†çš„æµå¼åŠ è½½
3. **é”™è¯¯å¤„ç†**: å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
4. **ç­”æ¡ˆè§£ç **: åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œé¿å…é‡å¤prompt
5. **å¯æ‰©å±•æ€§**: Promptæ ¼å¼å¯ä»¥æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´

---

## ğŸ“ è°ƒè¯•è¾“å‡º

åœ¨verboseæ¨¡å¼ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°ï¼š

```
[1/10] å¤„ç†ä»»åŠ¡: n161313...
  ğŸ“ Prompt: What color is the car?
  ğŸ–¼ï¸  å›¾åƒæ•°é‡: 1
  ğŸ” æ¨¡å‹è¿”å›å€¼ç±»å‹: <class 'dict'>
  ğŸ” æ¨¡å‹è¿”å›å€¼çš„é”®: ['text', 'usage', 'raw']
  ğŸ” æ¨¡å‹è¿”å›å€¼å†…å®¹:
    - text: red
    - usage: {'prompt_tokens': 0, 'completion_tokens': 0}
  âœ“ ç­”æ¡ˆ: red | GT: red
```

---

## ğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥

1. **ç­”æ¡ˆé‡å¤äº†é—®é¢˜**: æ£€æŸ¥è§£ç é€»è¾‘ï¼Œç¡®ä¿ä½¿ç”¨ `outputs[0][input_length:]`
2. **å›¾åƒåŠ è½½å¤±è´¥**: æ£€æŸ¥å›¾åƒè·¯å¾„æ ¼å¼ï¼ˆURL/Base64/æœ¬åœ°è·¯å¾„ï¼‰
3. **Promptæ ¼å¼ä¸å¯¹**: ä¿®æ”¹ `_build_prompt()` æ–¹æ³•
4. **æ¨¡å‹è°ƒç”¨å¤±è´¥**: æ£€æŸ¥æ¨¡å‹é€‚é…å™¨çš„åˆå§‹åŒ–å‚æ•°

