"""
å¤§è§„æ¨¡å›¾æ–‡æ•°æ®èšç±»ç³»ç»Ÿ - å†…å­˜ä¼˜åŒ–ç‰ˆ
æ”¯æŒç™¾ä¸‡çº§æ•°æ®å¤„ç†ï¼Œé€šè¿‡æµå¼è¯»å–ã€æ‰¹é‡è®¡ç®—ã€ç‰¹å¾ç¼“å­˜é¿å…å†…å­˜çˆ†ç‚¸
"""

import json
import numpy as np
import h5py
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Iterator, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
from pathlib import Path
import gc
import base64
from io import BytesIO
from PIL import Image
from PIL import UnidentifiedImageError
import torch

# ==================== å…³é”®ä¿®å¤ï¼šåœ¨å¯¼å…¥transformersä¹‹å‰å¤„ç†cv2é—®é¢˜ ====================
# transformerså†…éƒ¨ä¼šå°è¯•å¯¼å…¥cv2ï¼Œå¦‚æœcv2å¯¼å…¥å¤±è´¥ï¼ˆå¦‚ç¼ºå°‘libGL.so.1ï¼‰ï¼Œä¼šå¯¼è‡´transformerså¯¼å…¥å¤±è´¥
# æˆ‘ä»¬éœ€è¦åœ¨transformerså¯¼å…¥ä¹‹å‰å°±åˆ›å»ºå¥½å‡cv2æ¨¡å—

import os
import sys
import types

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…cv2åŠ è½½OpenGL
os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
os.environ['QT_QPA_PLATFORM'] = 'offscreen'
os.environ['DISPLAY'] = ''

# å°è¯•å¯¼å…¥imageioä½œä¸ºWebPçš„å¤‡é€‰æ–¹æ¡ˆ
try:
    import imageio
    HAS_IMAGEIO = True
except ImportError:
    HAS_IMAGEIO = False

# å°è¯•å¯¼å…¥cv2ï¼Œå¦‚æœå¤±è´¥åˆ™åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å‡cv2æ¨¡å—
HAS_CV2 = False
try:
    import cv2
    HAS_CV2 = True
    print("[INFO] cv2å¯¼å…¥æˆåŠŸ")
except (ImportError, OSError, RuntimeError) as e:
    # æ•è·æ‰€æœ‰å¯èƒ½çš„é”™è¯¯ï¼ˆImportError, OSError, RuntimeErrorï¼‰
    HAS_CV2 = False
    error_msg = str(e)
    error_type = type(e).__name__
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯libGLç›¸å…³é”™è¯¯
    is_libgl_error = 'libGL' in error_msg or 'libGL.so' in error_msg
    
    if is_libgl_error or error_type == 'OSError':
        print(f"[WARN] cv2å¯¼å…¥å¤±è´¥ï¼ˆ{error_type}ï¼‰: {error_msg[:100]}")
        print(f"      åˆ›å»ºå‡çš„cv2æ¨¡å—ä¾›transformersä½¿ç”¨...")
    else:
        print(f"[WARN] cv2å¯¼å…¥å¤±è´¥ï¼ˆ{error_type}ï¼‰: {error_msg[:100]}")
    
    # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å‡cv2æ¨¡å—ï¼ŒåŒ…å«transformerså¯èƒ½éœ€è¦çš„æ‰€æœ‰å±æ€§
    fake_cv2 = types.ModuleType('cv2')
    
    # æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
    fake_cv2.__version__ = '4.5.0'
    
    # æ·»åŠ transformerså¯èƒ½ä½¿ç”¨çš„å¸¸é‡å’Œå‡½æ•°
    fake_cv2.IMREAD_COLOR = 1
    fake_cv2.IMREAD_GRAYSCALE = 0
    fake_cv2.IMREAD_UNCHANGED = -1
    fake_cv2.COLOR_BGR2RGB = 4
    fake_cv2.COLOR_RGB2BGR = 4
    fake_cv2.COLOR_BGR2GRAY = 6
    fake_cv2.COLOR_GRAY2RGB = 8
    
    # æ·»åŠ å‡½æ•°ï¼ˆè™½ç„¶ä¸ä¼šè¢«è°ƒç”¨ï¼Œä½†é¿å…AttributeErrorï¼‰
    def fake_imread(*args, **kwargs):
        raise NotImplementedError("cv2 is not available (fake module)")
    
    def fake_imwrite(*args, **kwargs):
        raise NotImplementedError("cv2 is not available (fake module)")
    
    def fake_cvtColor(*args, **kwargs):
        raise NotImplementedError("cv2 is not available (fake module)")
    
    fake_cv2.imread = fake_imread
    fake_cv2.imwrite = fake_imwrite
    fake_cv2.cvtColor = fake_cvtColor
    
    # å°†å‡æ¨¡å—æ³¨å†Œåˆ°sys.modulesï¼Œè¿™æ ·transformerså¯¼å…¥æ—¶å°±ä¼šä½¿ç”¨å®ƒ
    sys.modules['cv2'] = fake_cv2
    
    if is_libgl_error:
        print(f"      [INFO] å·²åˆ›å»ºå‡çš„cv2æ¨¡å—ï¼Œtransformerså¯ä»¥ç»§ç»­å¯¼å…¥")
        print(f"      [æç¤º] å¦‚éœ€ä½¿ç”¨cv2åŠŸèƒ½ï¼Œè¯·å®‰è£…: apt-get install libgl1-mesa-glx libglib2.0-0")

# ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥transformersäº†
from transformers import CLIPProcessor, CLIPModel
from sklearn.preprocessing import StandardScaler

from sklearn.cluster import MiniBatchKMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

import warnings
warnings.filterwarnings('ignore')


# ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================
@dataclass
class DataSample:
    """è½»é‡çº§æ•°æ®æ ·æœ¬ç»“æ„"""
    idx: int
    data_type: str
    cluster_id: int = -1
    # æ³¨æ„ï¼šä¸åœ¨å†…å­˜ä¸­ä¿å­˜å®Œæ•´contentå’Œfeatureï¼Œé€šè¿‡idxç´¢å¼•


# ==================== 1. ç‰¹å¾ç¼“å­˜ç®¡ç† ====================
class FeatureCache:
    """ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨ - ä½¿ç”¨HDF5å­˜å‚¨å¤§è§„æ¨¡ç‰¹å¾"""
    
    def __init__(self, cache_dir: str, feature_dim: int = 1546):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.feature_dim = feature_dim
        self.cache_files = {}
    
    def create_cache(self, cache_name: str, n_samples: int):
        """åˆ›å»ºç‰¹å¾ç¼“å­˜æ–‡ä»¶"""
        cache_path = self.cache_dir / f"{cache_name}_features.h5"
        
        with h5py.File(cache_path, 'w') as f:
            f.create_dataset(
                'features',
                shape=(n_samples, self.feature_dim),
                dtype='float32',
                chunks=(min(1000, n_samples), self.feature_dim)
            )
            f.create_dataset('processed', shape=(n_samples,), dtype='bool')
        
        self.cache_files[cache_name] = cache_path
        print(f"  âœ… åˆ›å»ºç¼“å­˜: {cache_path} ({n_samples} x {self.feature_dim})")
    
    def write_batch(self, cache_name: str, indices: np.ndarray, features: np.ndarray):
        """å†™å…¥æ‰¹é‡ç‰¹å¾"""
        cache_path = self.cache_files[cache_name]
        
        with h5py.File(cache_path, 'r+') as f:
            f['features'][indices] = features
            f['processed'][indices] = True
    
    def read_all(self, cache_name: str) -> np.ndarray:
        """è¯»å–æ‰€æœ‰ç‰¹å¾"""
        cache_path = self.cache_files[cache_name]
        
        with h5py.File(cache_path, 'r') as f:
            features = f['features'][:]
        
        return features
    
    def cleanup(self):
        """æ¸…ç†ç¼“å­˜æ–‡ä»¶"""
        for cache_path in self.cache_files.values():
            if cache_path.exists():
                cache_path.unlink()
        print(f"  ğŸ§¹ æ¸…ç†ç¼“å­˜å®Œæˆ")



# ==================== 4. ç‰¹å¾æå–æ¨¡å—ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰====================
class FeatureExtractor(ABC):
    """ç‰¹å¾æå–å™¨æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def extract_batch(self, samples: List[Dict], data_type: str) -> np.ndarray:
        """æ‰¹é‡æå–ç‰¹å¾ï¼ˆç›´æ¥å¤„ç†åŸå§‹æ•°æ®ï¼‰"""
        pass


# class SimpleTextFeatureExtractor(FeatureExtractor):
#     """ç®€å•æ–‡æœ¬ç‰¹å¾æå–å™¨"""
    
#     def __init__(self, embedding_dim: int = 768):
#         self.embedding_dim = embedding_dim
    
#     def extract_batch(self, samples: List[Dict], data_type: str) -> np.ndarray:
#         """æ‰¹é‡æå–æ–‡æœ¬ç‰¹å¾"""
#         texts = [self._get_text(sample, data_type) for sample in samples]
#         features = np.array([self._text_to_embedding(text) for text in texts])
#         return features
    
#     def _get_text(self, sample: Dict, data_type: str) -> str:
#         """æå–æ–‡æœ¬å†…å®¹ï¼ˆæ™ºèƒ½æ£€æµ‹å­—æ®µï¼‰"""
#         # ç®€åŒ–ï¼šæŸ¥æ‰¾æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µå¹¶æ‹¼æ¥
#         text_parts = []
#         for key, value in sample.items():
#             if isinstance(value, str) and len(value) > 0:
#                 # æ’é™¤å›¾åƒè·¯å¾„
#                 if not any(ext in value.lower() for ext in ['.jpg', '.png', '.jpeg']):
#                     text_parts.append(value)
#         return " ".join(text_parts)
    
#     def _text_to_embedding(self, text: str) -> np.ndarray:
#         """æ–‡æœ¬è½¬åµŒå…¥ï¼ˆå“ˆå¸Œæ¨¡æ‹Ÿï¼‰"""
#         np.random.seed(hash(text) % (2**32))
#         embedding = np.random.randn(self.embedding_dim)
#         return normalize(embedding.reshape(1, -1))[0]


class VQAFeatureExtractor:
    """
    VQAæ•°æ®å¤šæ¨¡æ€ç‰¹å¾æå–å™¨
    
    ç‰¹å¾æå–ç­–ç•¥:
    1. å›¾åƒç‰¹å¾: CLIPè§†è§‰ç¼–ç å™¨æå–å…¨å±€ç‰¹å¾
    2. é—®é¢˜ç‰¹å¾: CLIPæ–‡æœ¬ç¼–ç å™¨ + é—®é¢˜ç±»å‹/é•¿åº¦ç­‰ç»Ÿè®¡ç‰¹å¾
    3. ç­”æ¡ˆç‰¹å¾: CLIPæ–‡æœ¬ç¼–ç å™¨ + ç­”æ¡ˆé•¿åº¦/ç±»å‹ç­‰ç»Ÿè®¡ç‰¹å¾
    4. äº¤äº’ç‰¹å¾: é—®ç­”å¯¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€å¤šè½®å¯¹è¯ç‰¹å¾
    5. å¤šæ¨¡æ€å¯¹é½ç‰¹å¾: å›¾åƒ-é—®é¢˜ã€å›¾åƒ-ç­”æ¡ˆçš„è·¨æ¨¡æ€ç›¸ä¼¼åº¦
    """
    
    def __init__(self, 
                 model_name: str = "openai/clip-vit-base-patch32",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 normalize: bool = True,
                 feature_config: Dict[str, bool] = None):
        """
        Args:
            model_name: CLIPæ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
            normalize: æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾
            feature_config: ç‰¹å¾é€‰æ‹©é…ç½®å­—å…¸ï¼Œæ§åˆ¶ä½¿ç”¨å“ªäº›ç‰¹å¾
                å¯é€‰é…ç½®é¡¹:
                - 'image': å›¾åƒç‰¹å¾ (512ç»´)
                - 'question': é—®é¢˜è¯­ä¹‰ç‰¹å¾ (512ç»´)
                - 'answer': ç­”æ¡ˆè¯­ä¹‰ç‰¹å¾ (512ç»´)
                - 'statistical': ç»Ÿè®¡ç‰¹å¾ (10ç»´)
                - 'interaction': äº¤äº’ç‰¹å¾ (9ç»´)
                é»˜è®¤: ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
                ç¤ºä¾‹: {'image': False, 'question': True, 'answer': False, 'statistical': False, 'interaction': False}
                     è¡¨ç¤ºåªä½¿ç”¨é—®é¢˜ç‰¹å¾è¿›è¡Œèšç±»
        """
        self.device = device
        self.normalize = normalize
        
        # ç‰¹å¾é…ç½®ï¼ˆé»˜è®¤å…¨éƒ¨å¯ç”¨ï¼‰
        default_config = {
            'image': True,
            'question': True,
            'answer': True,
            'statistical': True,
            'interaction': True
        }
        if feature_config is None:
            feature_config = default_config
        else:
            # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
            for key in default_config:
                if key not in feature_config:
                    feature_config[key] = default_config[key]
        
        self.feature_config = feature_config
        
        # åŠ è½½CLIPæ¨¡å‹
        print(f"ğŸ”„ åŠ è½½CLIPæ¨¡å‹: {model_name}")
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.model.eval()
        
        # ç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.scaler = StandardScaler() if normalize else None
        
        # æ‰“å°ç‰¹å¾é…ç½®
        enabled_features = [k for k, v in self.feature_config.items() if v]
        disabled_features = [k for k, v in self.feature_config.items() if not v]
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")
        print(f"ğŸ“Š ç‰¹å¾é…ç½®:")
        print(f"   å¯ç”¨: {', '.join(enabled_features) if enabled_features else 'æ— '}")
        if disabled_features:
            print(f"   ç¦ç”¨: {', '.join(disabled_features)}")
    
    def extract_batch(self, samples: List[Dict]) -> np.ndarray:
        """
        æ‰¹é‡æå–VQAæ ·æœ¬çš„å¤šæ¨¡æ€ç‰¹å¾
        
        Args:
            samples: VQAæ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬åŒ…å«:
                - dialogue: [{question: str, answer: str}, ...]
                - image_buffer_list: [{buffer: str, image_id: str}, ...]
                - task_type: str
                - source: str
        
        Returns:
            ç‰¹å¾çŸ©é˜µ (n_samples, feature_dim)
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç‰¹å¾æå–: {len(samples)} ä¸ªVQAæ ·æœ¬")
        print(f"{'='*70}\n")
        
        all_features = []
        
        for idx, sample in enumerate(samples):
            if (idx + 1) % 10 == 0:
                print(f"è¿›åº¦: {idx + 1}/{len(samples)}")
            
            features = self._extract_single_sample(sample, idx)
            all_features.append(features)
        
        # å †å æ‰€æœ‰ç‰¹å¾
        feature_matrix = np.vstack(all_features)
        
        # æ ‡å‡†åŒ–
        if self.normalize and len(samples) > 1:
            print(f"\nğŸ”§ æ ‡å‡†åŒ–ç‰¹å¾...")
            feature_matrix = self.scaler.fit_transform(feature_matrix)
        
        print(f"\n{'='*70}")
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ!")
        print(f"   ç‰¹å¾ç»´åº¦: {feature_matrix.shape}")
        print(f"   ç‰¹å¾èŒƒå›´: [{feature_matrix.min():.3f}, {feature_matrix.max():.3f}]")
        print(f"{'='*70}\n")
        
        return feature_matrix
    
    def _extract_single_sample(self, sample: Dict, idx: int) -> np.ndarray:
        """
        æå–å•ä¸ªæ ·æœ¬çš„ç»¼åˆç‰¹å¾ï¼ˆæ ¹æ®feature_configé€‰æ‹©æ€§ç»„åˆï¼‰
        """
        feature_parts = []
        
        # 1. æå–å›¾åƒç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼Œæˆ–è€…äº¤äº’ç‰¹å¾éœ€è¦ï¼‰
        image_features = None
        if self.feature_config.get('image', True) or self.feature_config.get('interaction', True):
            image_features = self._extract_image_features(sample, idx)
            if self.feature_config.get('image', True):
                feature_parts.append(image_features)
        
        # 2. æå–å¯¹è¯ç‰¹å¾ï¼ˆç»†åŒ–åçš„ç‰ˆæœ¬ï¼‰
        dialogue_features_dict = self._extract_dialogue_features(sample, idx)
        
        # 2a. é—®é¢˜ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.feature_config.get('question', True) and 'question' in dialogue_features_dict:
            feature_parts.append(dialogue_features_dict['question'])
        
        # 2b. ç­”æ¡ˆç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.feature_config.get('answer', True) and 'answer' in dialogue_features_dict:
            feature_parts.append(dialogue_features_dict['answer'])
        
        # 3. æå–ç»Ÿè®¡ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.feature_config.get('statistical', True):
            stat_features = self._extract_statistical_features(sample)
            feature_parts.append(stat_features)
        
        # 4. æå–å¤šæ¨¡æ€äº¤äº’ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.feature_config.get('interaction', True):
            # éœ€è¦å›¾åƒå’Œå¯¹è¯ç‰¹å¾æ¥è®¡ç®—äº¤äº’ç‰¹å¾
            # å¦‚æœå›¾åƒç‰¹å¾æœªå¯ç”¨ï¼Œä»ç„¶éœ€è¦è®¡ç®—ï¼ˆä½†ä¸åŠ å…¥æœ€ç»ˆç‰¹å¾ï¼‰
            if image_features is None:
                image_features = self._extract_image_features(sample, idx)
            
            # ç»„åˆå¯¹è¯ç‰¹å¾ï¼ˆç”¨äºäº¤äº’ç‰¹å¾è®¡ç®—ï¼‰
            dialogue_features_combined = np.concatenate([
                dialogue_features_dict.get('question', np.zeros(512)),
                dialogue_features_dict.get('answer', np.zeros(512))
            ])
            
            interaction_features = self._extract_interaction_features(
                sample, image_features, dialogue_features_combined
            )
            feature_parts.append(interaction_features)
        
        # 5. åˆå¹¶æ‰€æœ‰å¯ç”¨çš„ç‰¹å¾
        if not feature_parts:
            raise ValueError("è‡³å°‘éœ€è¦å¯ç”¨ä¸€ä¸ªç‰¹å¾ç±»å‹ï¼è¯·æ£€æŸ¥feature_configé…ç½®ã€‚")
        
        combined = np.concatenate(feature_parts)
        return combined
    
    def _extract_image_features(self, sample: Dict, idx: int) -> np.ndarray:
        """æå–å›¾åƒç‰¹å¾"""
        images = self._load_images(sample)
        
        if not images:
            # æ²¡æœ‰å›¾åƒæ—¶è¿”å›é›¶å‘é‡
            print(f"[WARN] æ ·æœ¬ {idx} æœªåŠ è½½åˆ°ä»»ä½•å›¾åƒï¼Œè¿”å›é›¶å‘é‡ã€‚")
            return np.zeros(512)
        
        with torch.no_grad():
            # å¤„ç†å¤šå¼ å›¾ç‰‡: å–å¹³å‡
            image_embeddings = []
            for img in images:
                inputs = self.processor(images=img, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                embedding = self.model.get_image_features(**inputs)
                image_embeddings.append(embedding.cpu().numpy())
            
            # å¹³å‡æ± åŒ–
            avg_embedding = np.mean(image_embeddings, axis=0).flatten()
        
        return avg_embedding
    
    def _extract_dialogue_features(self, sample: Dict, idx: int) -> Dict[str, np.ndarray]:
        """
        æå–å¯¹è¯ç‰¹å¾ (é—®é¢˜+ç­”æ¡ˆçš„è¯­ä¹‰ç‰¹å¾)
        
        Returns:
            å­—å…¸ï¼ŒåŒ…å«:
            - 'question': é—®é¢˜è¯­ä¹‰ç‰¹å¾ (512ç»´)
            - 'answer': ç­”æ¡ˆè¯­ä¹‰ç‰¹å¾ (512ç»´)
        """
        dialogue = sample.get('dialogue', [])
        
        result = {}
        
        if not dialogue:
            if self.feature_config.get('question', True):
                result['question'] = np.zeros(512)
            if self.feature_config.get('answer', True):
                result['answer'] = np.zeros(512)
            return result
        
        with torch.no_grad():
            # æå–é—®é¢˜ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.feature_config.get('question', True):
                questions = [qa.get('question', '') for qa in dialogue]
                question_embeddings = self._encode_texts(questions)
                # å¯¹å¤šè½®å¯¹è¯å–å¹³å‡
                result['question'] = np.mean(question_embeddings, axis=0)
            
            # æå–ç­”æ¡ˆç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.feature_config.get('answer', True):
                answers = [qa.get('answer', '') for qa in dialogue]
                answer_embeddings = self._encode_texts(answers)
                # å¯¹å¤šè½®å¯¹è¯å–å¹³å‡
                result['answer'] = np.mean(answer_embeddings, axis=0)
        
        return result
    
    def _extract_statistical_features(self, sample: Dict) -> np.ndarray:
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        dialogue = sample.get('dialogue', [])
        
        features = []
        
        # 1. å¯¹è¯è½®æ•°
        features.append(len(dialogue))
        
        # 2. é—®é¢˜ç»Ÿè®¡
        if dialogue:
            questions = [qa.get('question', '') for qa in dialogue]
            features.extend([
                np.mean([len(q.split()) for q in questions]),  # å¹³å‡é—®é¢˜é•¿åº¦
                np.std([len(q.split()) for q in questions]),   # é—®é¢˜é•¿åº¦æ ‡å‡†å·®
                np.mean([len(q) for q in questions]),          # å¹³å‡å­—ç¬¦æ•°
            ])
        else:
            features.extend([0, 0, 0])
        
        # 3. ç­”æ¡ˆç»Ÿè®¡
        if dialogue:
            answers = [qa.get('answer', '') for qa in dialogue]
            features.extend([
                np.mean([len(a.split()) for a in answers]),    # å¹³å‡ç­”æ¡ˆé•¿åº¦
                np.std([len(a.split()) for a in answers]),     # ç­”æ¡ˆé•¿åº¦æ ‡å‡†å·®
                np.mean([len(a) for a in answers]),            # å¹³å‡å­—ç¬¦æ•°
            ])
        else:
            features.extend([0, 0, 0])
        
        # 4. å›¾åƒæ•°é‡
        image_list = sample.get('image_buffer_list', [])
        features.append(len(image_list))
        
        # 5. ä»»åŠ¡ç±»å‹ç¼–ç  (one-hotç®€åŒ–ç‰ˆ)
        task_type = sample.get('task_type', '')
        task_indicators = [
            int('vqa' in task_type.lower()),
            int('caption' in task_type.lower()),
            int('turn' in task_type.lower()),
        ]
        features.extend(task_indicators)
        
        return np.array(features, dtype=np.float32)
    
    def _extract_interaction_features(self, sample: Dict, 
                                     image_features: np.ndarray,
                                     dialogue_features: np.ndarray) -> np.ndarray:
        """æå–å¤šæ¨¡æ€äº¤äº’ç‰¹å¾"""
        features = []
        
        dialogue = sample.get('dialogue', [])
        images = self._load_images(sample)
        has_image = len(images) > 0
        
        if not dialogue or not has_image:
            # ä¸ç‰¹å¾å‘½åä¿æŒä¸€è‡´çš„ 9 ç»´å ä½
            if not dialogue:
                print("[WARN] å¯¹è¯ä¸ºç©ºï¼Œäº¤äº’ç‰¹å¾è¿”å›é›¶å‘é‡ã€‚")
            if not has_image:
                print("[WARN] å›¾åƒæœªåŠ è½½æˆåŠŸï¼Œäº¤äº’ç‰¹å¾è¿”å›é›¶å‘é‡ã€‚")
            return np.zeros(9, dtype=np.float32)
        
        # åˆ†ç¦»é—®é¢˜å’Œç­”æ¡ˆç‰¹å¾
        question_features = dialogue_features[:512]
        answer_features = dialogue_features[512:]
        
        # 1. å›¾åƒ-é—®é¢˜ç›¸ä¼¼åº¦
        img_q_sim = self._cosine_similarity(image_features, question_features)
        features.append(img_q_sim)
        
        # 2. å›¾åƒ-ç­”æ¡ˆç›¸ä¼¼åº¦
        img_a_sim = self._cosine_similarity(image_features, answer_features)
        features.append(img_a_sim)
        
        # 3. é—®é¢˜-ç­”æ¡ˆç›¸ä¼¼åº¦
        q_a_sim = self._cosine_similarity(question_features, answer_features)
        features.append(q_a_sim)
        
        # 4. é—®é¢˜ä¸­æ˜¯å¦åŒ…å«å›¾åƒå¼•ç”¨æ ‡è®°
        questions = [qa.get('question', '') for qa in dialogue]
        ##### here111
        # å…¼å®¹æ—§æ ¼å¼ "<image>" å’Œæ–°æ ¼å¼ "[Image_{idx}]"
        has_image_ref = any(('<image>' in q) or ('[Image_' in q) for q in questions)
        ##### here222
        features.append(float(has_image_ref))
        
        # 5. å¤šè½®å¯¹è¯è¿è´¯æ€§ (ç›¸é‚»QAå¯¹çš„ç›¸ä¼¼åº¦)
        if len(dialogue) > 1:
            coherence_scores = []
            for i in range(len(dialogue) - 1):
                curr_text = dialogue[i]['question'] + ' ' + dialogue[i]['answer']
                next_text = dialogue[i+1]['question'] + ' ' + dialogue[i+1]['answer']
                
                with torch.no_grad():
                    curr_emb = self._encode_texts([curr_text])[0]
                    next_emb = self._encode_texts([next_text])[0]
                    sim = self._cosine_similarity(curr_emb, next_emb)
                    coherence_scores.append(sim)
            
            features.append(np.mean(coherence_scores))
            features.append(np.std(coherence_scores))
        else:
            features.extend([0, 0])
        
        # 6. é—®é¢˜å¤æ‚åº¦æŒ‡æ ‡
        avg_question_words = np.mean([len(qa['question'].split()) for qa in dialogue])
        features.append(avg_question_words / 50)  # å½’ä¸€åŒ–
        
        # 7. ç­”æ¡ˆè¯¦ç»†åº¦æŒ‡æ ‡
        avg_answer_words = np.mean([len(qa['answer'].split()) for qa in dialogue])
        features.append(avg_answer_words / 100)  # å½’ä¸€åŒ–
        
        # 8. é—®ç­”æ¯”ä¾‹
        if avg_question_words > 0:
            features.append(avg_answer_words / avg_question_words)
        else:
            features.append(0)
        
        return np.array(features, dtype=np.float32)
    
    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        if not texts:
            return np.zeros((1, 512))
        
        inputs = self.processor(text=texts, return_tensors="pt", 
                               padding=True, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            embeddings = self.model.get_text_features(**inputs)
        
        return embeddings.cpu().numpy()
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        vec1 = vec1.flatten()
        vec2 = vec2.flatten()
        
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(np.dot(vec1, vec2) / (norm1 * norm2))
    
    def _load_images(self, sample: Dict) -> List[Image.Image]:
        """åŠ è½½æ ·æœ¬ä¸­çš„æ‰€æœ‰å›¾åƒ"""
        images = []
        image_buffer_list = sample.get('image_buffer_list', [])
        
        for i, img_data in enumerate(image_buffer_list):
            buffer = img_data.get('buffer', '')
            if not buffer:
                print(f"[WARN] æ ·æœ¬ä¸­ç¬¬ {i} ä¸ª image_buffer ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue
            
            img = self._base64_to_image(buffer)
            if img is None:
                # æ‰“å°å‡ºé”™ä¿¡æ¯ï¼ˆé¿å…æ‰“å°æ•´ä¸ª base64ï¼Œåªç»™å‰ç¼€ï¼‰
                buf_preview = str(buffer)
                if len(buf_preview) > 60:
                    buf_preview = buf_preview[:60] + "...[truncated]"
                print(f"[ERROR] æ— æ³•ä»ç¬¬ {i} ä¸ª image_buffer è§£ç å›¾åƒï¼Œbuffer å‰ç¼€: {buf_preview}")
                continue
            
            images.append(img)
        
        return images
    
    def _base64_to_image(self, img_b64: Any) -> Optional[Image.Image]:
        """Base64è½¬PIL Imageï¼Œå…¼å®¹ bytes / b"..." å­—ç¬¦ä¸²ç­‰å¤šç§æ ¼å¼ï¼Œæ”¯æŒWebPæ ¼å¼"""
        img_bytes = None
        original_input_type = type(img_b64).__name__
        original_input_length = len(str(img_b64)) if img_b64 else 0
        data_source = "unknown"  # è®°å½•æ•°æ®æ¥æº
        
        try:
            # ========== æ­¥éª¤1: æ£€æµ‹è¾“å…¥æ˜¯å¦å·²ç»æ˜¯äºŒè¿›åˆ¶æ•°æ® ==========
            # å¦‚æœè¾“å…¥æ˜¯bytesç±»å‹ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ï¼ˆä¸æ˜¯base64ç¼–ç çš„å­—ç¬¦ä¸²ï¼‰
            if isinstance(img_b64, bytes):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶å¤´ï¼ˆRIFF/WEBP, JPEG, PNGç­‰ï¼‰
                if len(img_b64) >= 12:
                    if (img_b64[:4] == b'RIFF' and img_b64[8:12] == b'WEBP') or \
                       img_b64[:2] == b'\xff\xd8' or \
                       img_b64[:8] == b'\x89PNG\r\n\x1a\n':
                        # å·²ç»æ˜¯äºŒè¿›åˆ¶å›¾ç‰‡æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                        print(f"[DEBUG] æ£€æµ‹åˆ°è¾“å…¥æ˜¯äºŒè¿›åˆ¶å›¾ç‰‡æ•°æ®ï¼ˆ{original_input_type}ï¼‰ï¼Œè·³è¿‡Base64è§£ç ")
                        img_bytes = img_b64
                        data_source = "direct_binary"
                    else:
                        # å¯èƒ½æ˜¯base64ç¼–ç çš„bytesï¼Œå°è¯•è§£ç ä¸ºå­—ç¬¦ä¸²
                        try:
                            img_b64 = img_b64.decode("utf-8")
                        except Exception:
                            img_b64 = img_b64.decode("latin-1", errors="ignore")
                else:
                    # å¤ªçŸ­ï¼Œå°è¯•è§£ç ä¸ºå­—ç¬¦ä¸²
                    try:
                        img_b64 = img_b64.decode("utf-8")
                    except Exception:
                        img_b64 = img_b64.decode("latin-1", errors="ignore")
            else:
                img_b64 = str(img_b64)

            # ========== æ­¥éª¤2: å¦‚æœå·²ç»æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œè·³è¿‡Base64è§£ç  ==========
            if img_bytes is not None:
                pass  # å·²ç»è®¾ç½®å¥½ï¼Œç»§ç»­åç»­å¤„ç†
            else:
                # ========== æ­¥éª¤2a: å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è¾“å…¥ ==========
                # æ£€æŸ¥æ˜¯å¦æ˜¯Pythonå­—ç¬¦ä¸²è¡¨ç¤ºçš„äºŒè¿›åˆ¶æ•°æ®ï¼ˆb'...' æˆ–åŒ…å«è½¬ä¹‰åºåˆ—ï¼‰
                is_python_bytes_literal = False
                if img_b64.startswith("b'") and img_b64.endswith("'"):
                    # è¿™æ˜¯ b'...' æ ¼å¼çš„å­—ç¬¦ä¸²è¡¨ç¤º
                    try:
                        import ast
                        img_bytes = ast.literal_eval(img_b64)
                        data_source = "python_bytes_literal"
                        is_python_bytes_literal = True
                        print(f"[DEBUG] æ£€æµ‹åˆ°Python byteså­—é¢é‡ï¼ˆb'...'ï¼‰ï¼Œç›´æ¥è§£æ")
                    except Exception as eval_error:
                        print(f"[WARN] è§£æb'...'æ ¼å¼å¤±è´¥: {eval_error}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                        img_b64 = img_b64[2:-1]  # å»æ‰ b' å’Œ '
                elif img_b64.startswith('b"') and img_b64.endswith('"'):
                    # è¿™æ˜¯ b"..." æ ¼å¼çš„å­—ç¬¦ä¸²è¡¨ç¤º
                    try:
                        import ast
                        img_bytes = ast.literal_eval(img_b64)
                        data_source = "python_bytes_literal"
                        is_python_bytes_literal = True
                        print(f"[DEBUG] æ£€æµ‹åˆ°Python byteså­—é¢é‡ï¼ˆb\"...\"ï¼‰ï¼Œç›´æ¥è§£æ")
                    except Exception as eval_error:
                        print(f"[WARN] è§£æb\"...\"æ ¼å¼å¤±è´¥: {eval_error}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                        img_b64 = img_b64[2:-1]  # å»æ‰ b" å’Œ "
                
                if not is_python_bytes_literal:
                    # å¤„ç† data URI
                    if ',' in img_b64 and img_b64.startswith('data:'):
                        img_b64 = img_b64.split(',', 1)[1]
                    
                    # æ¸…ç†å¹¶ä¿®å¤padding
                    img_b64_cleaned = img_b64.strip()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„base64å­—ç¬¦
                    base64_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
                    is_likely_base64 = len(img_b64_cleaned) > 0 and all(c in base64_chars or c.isspace() for c in img_b64_cleaned[:100])
                    
                    if not is_likely_base64:
                        print(f"[WARN] è¾“å…¥å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„Base64å­—ç¬¦ä¸²:")
                        print(f"  - è¾“å…¥ç±»å‹: {original_input_type}")
                        print(f"  - è¾“å…¥é•¿åº¦: {original_input_length}")
                        print(f"  - æ¸…ç†åé•¿åº¦: {len(img_b64_cleaned)}")
                        print(f"  - å‰100å­—ç¬¦: {img_b64_cleaned[:100]}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯Pythonå­—ç¬¦ä¸²è¡¨ç¤ºçš„äºŒè¿›åˆ¶æ•°æ®ï¼ˆåŒ…å«\xè½¬ä¹‰åºåˆ—ï¼‰
                        has_escape_sequences = '\\x' in img_b64_cleaned or '\\n' in img_b64_cleaned or '\\t' in img_b64_cleaned
                        
                        if has_escape_sequences:
                            print(f"  - æ£€æµ‹åˆ°è½¬ä¹‰åºåˆ—ï¼Œå°è¯•è§£æä¸ºPythonå­—ç¬¦ä¸²è¡¨ç¤ºçš„äºŒè¿›åˆ¶æ•°æ®...")
                            try:
                                # ä½¿ç”¨ast.literal_evalå®‰å…¨åœ°è§£æå­—ç¬¦ä¸²
                                import ast
                                # å¦‚æœå­—ç¬¦ä¸²ä»¥b'æˆ–b"å¼€å¤´ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                                if img_b64_cleaned.startswith("b'") or img_b64_cleaned.startswith('b"'):
                                    # å·²ç»æ˜¯b'...'æ ¼å¼ï¼Œç›´æ¥è§£æ
                                    img_bytes = ast.literal_eval(img_b64_cleaned)
                                    data_source = "python_string_literal"
                                    print(f"  - [æˆåŠŸ] è§£æPythonå­—ç¬¦ä¸²å­—é¢é‡ï¼Œå¾—åˆ° {len(img_bytes)} bytes")
                                else:
                                    # å°è¯•æ·»åŠ bå‰ç¼€
                                    try:
                                        img_bytes = ast.literal_eval('b"' + img_b64_cleaned.replace('"', '\\"') + '"')
                                        data_source = "python_string_literal"
                                        print(f"  - [æˆåŠŸ] è§£æPythonå­—ç¬¦ä¸²å­—é¢é‡ï¼ˆæ·»åŠ bå‰ç¼€ï¼‰ï¼Œå¾—åˆ° {len(img_bytes)} bytes")
                                    except:
                                        # å°è¯•ä½¿ç”¨codecs.decodeå¤„ç†è½¬ä¹‰åºåˆ—
                                        import codecs
                                        img_bytes = codecs.decode(img_b64_cleaned, 'unicode_escape').encode('latin-1')
                                        data_source = "unicode_escape_decoded"
                                        print(f"  - [æˆåŠŸ] ä½¿ç”¨unicode_escapeè§£ç ï¼Œå¾—åˆ° {len(img_bytes)} bytes")
                            except Exception as parse_error:
                                print(f"  - [å¤±è´¥] è§£æå¤±è´¥: {type(parse_error).__name__}: {parse_error}")
                                # æœ€åå°è¯•ï¼šç›´æ¥ä½¿ç”¨latin-1ç¼–ç 
                                try:
                                    img_bytes = img_b64_cleaned.encode('latin-1')
                                    data_source = "latin1_encoded"
                                    print(f"  - [å›é€€] ä½¿ç”¨latin-1ç¼–ç ï¼Œå¾—åˆ° {len(img_bytes)} bytes")
                                except:
                                    pass
                        else:
                            # æ²¡æœ‰è½¬ä¹‰åºåˆ—ï¼Œå°è¯•ç›´æ¥ç¼–ç 
                            print(f"  - å°è¯•ç›´æ¥ä½œä¸ºäºŒè¿›åˆ¶æ•°æ®å¤„ç†...")
                            try:
                                img_bytes = img_b64_cleaned.encode('latin-1')
                                data_source = "latin1_encoded"
                            except:
                                pass
                
                if img_bytes is None:
                    # å°è¯•Base64è§£ç 
                    padding = len(img_b64_cleaned) % 4
                    if padding:
                        img_b64_cleaned += '=' * (4 - padding)
                    
                    try:
                        img_bytes = base64.b64decode(img_b64_cleaned, validate=True)
                        data_source = "base64_decoded"
                    except Exception as decode_error:
                        print(f"[ERROR] Base64è§£ç å¤±è´¥:")
                        print(f"  - è¾“å…¥ç±»å‹: {original_input_type}")
                        print(f"  - è¾“å…¥é•¿åº¦: {original_input_length}")
                        print(f"  - Base64å­—ç¬¦ä¸²é•¿åº¦: {len(img_b64_cleaned)}")
                        print(f"  - é”™è¯¯ç±»å‹: {type(decode_error).__name__}")
                        print(f"  - é”™è¯¯ä¿¡æ¯: {decode_error}")
                        print(f"  - Base64å‰ç¼€(å‰200å­—ç¬¦): {img_b64_cleaned[:200] if len(img_b64_cleaned) > 200 else img_b64_cleaned}")
                        return None
            
            # ========== æ­¥éª¤3: è¯¦ç»†æ ¼å¼æ£€æµ‹ ==========
            file_size = len(img_bytes)
            file_header = img_bytes[:16] if len(img_bytes) >= 16 else img_bytes
            header_hex = ' '.join(f'{b:02x}' for b in file_header[:12])
            header_ascii = ''.join(chr(b) if 32 <= b < 127 else '.' for b in file_header[:12])
            
            # æ£€æµ‹å›¾ç‰‡æ ¼å¼
            is_webp = False
            is_jpeg = False
            is_png = False
            webp_subformat = None
            
            print(f"[DEBUG] å›¾ç‰‡æ•°æ®æ£€æµ‹:")
            print(f"  - æ•°æ®æ¥æº: {data_source if img_bytes is not None else 'Base64è§£ç '}")
            print(f"  - æ–‡ä»¶å¤§å°: {file_size} bytes")
            print(f"  - æ–‡ä»¶å¤´(hex): {header_hex}")
            print(f"  - æ–‡ä»¶å¤´(ascii): {header_ascii}")
            
            if file_size >= 12:
                if img_bytes[:4] == b'RIFF' and img_bytes[8:12] == b'WEBP':
                    is_webp = True
                    # æ£€æµ‹WebPå­æ ¼å¼
                    if file_size >= 16:
                        chunk_type = img_bytes[12:16]
                        if chunk_type == b'VP8 ':
                            webp_subformat = 'VP8 (lossy)'
                        elif chunk_type == b'VP8L':
                            webp_subformat = 'VP8L (lossless)'
                        elif chunk_type == b'VP8X':
                            webp_subformat = 'VP8X (extended)'
                        else:
                            webp_subformat = f'Unknown chunk: {chunk_type}'
                    print(f"  - æ ¼å¼: WebP ({webp_subformat})")
                elif img_bytes[:2] == b'\xff\xd8':
                    is_jpeg = True
                    print(f"  - æ ¼å¼: JPEG")
                elif img_bytes[:8] == b'\x89PNG\r\n\x1a\n':
                    is_png = True
                    print(f"  - æ ¼å¼: PNG")
                else:
                    print(f"  - æ ¼å¼: æœªçŸ¥ï¼ˆå‰4å­—èŠ‚: {img_bytes[:4]}ï¼‰")
            else:
                print(f"  - æ ¼å¼: æ•°æ®å¤ªçŸ­ï¼ˆ{file_size} < 12 bytesï¼‰")
            
            # ========== æ­¥éª¤4: å°è¯•ç”¨PILæ‰“å¼€ ==========
            try:
                img_buffer = BytesIO(img_bytes)
                img = Image.open(img_buffer)
                # å¦‚æœæ˜¯RGBAæˆ–å…¶ä»–æ¨¡å¼ï¼Œè½¬æ¢ä¸ºRGB
                if img.mode != 'RGB':
                    img = img.convert("RGB")
                return img
            except (UnidentifiedImageError, OSError, IOError) as pil_error:
                # PILæ— æ³•è¯†åˆ«ï¼Œè¾“å‡ºè¯¦ç»†è¯Šæ–­ä¿¡æ¯
                print(f"[DEBUG] PILæ— æ³•è¯†åˆ«å›¾ç‰‡:")
                print(f"  - æ–‡ä»¶å¤§å°: {file_size} bytes")
                print(f"  - æ–‡ä»¶å¤´(hex): {header_hex}")
                print(f"  - æ–‡ä»¶å¤´(ascii): {header_ascii}")
                print(f"  - æ ¼å¼æ£€æµ‹: WebP={is_webp}, JPEG={is_jpeg}, PNG={is_png}")
                if is_webp:
                    print(f"  - WebPå­æ ¼å¼: {webp_subformat}")
                print(f"  - PILé”™è¯¯ç±»å‹: {type(pil_error).__name__}")
                print(f"  - PILé”™è¯¯ä¿¡æ¯: {pil_error}")
                
                # å°è¯•ç”¨å¤‡é€‰æ–¹æ¡ˆï¼ˆç‰¹åˆ«æ˜¯WebPæ ¼å¼ï¼‰
                if is_webp:
                    print(f"  - å°è¯•å¤‡é€‰æ–¹æ¡ˆåŠ è½½WebP...")
                    
                    # æ–¹æ¡ˆ1: å°è¯•ä½¿ç”¨imageio
                    if HAS_IMAGEIO:
                        try:
                            print(f"    [å°è¯•1] ä½¿ç”¨imageioåŠ è½½...")
                            img_array = imageio.imread(img_bytes, format='webp')
                            img = Image.fromarray(img_array)
                            if img.mode != 'RGB':
                                img = img.convert("RGB")
                            print(f"    [æˆåŠŸ] imageioæˆåŠŸåŠ è½½WebP")
                            return img
                        except Exception as imageio_error:
                            print(f"    [å¤±è´¥] imageioåŠ è½½å¤±è´¥: {type(imageio_error).__name__}: {imageio_error}")
                    
                    # æ–¹æ¡ˆ2: å°è¯•ä½¿ç”¨cv2 (OpenCV)
                    if HAS_CV2:
                        try:
                            print(f"    [å°è¯•2] ä½¿ç”¨cv2åŠ è½½...")
                            # cv2.imdecodeéœ€è¦numpyæ•°ç»„
                            nparr = np.frombuffer(img_bytes, dtype=np.uint8)
                            img_array = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                            if img_array is not None:
                                # cv2ä½¿ç”¨BGRæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºRGB
                                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                                img = Image.fromarray(img_array)
                                print(f"    [æˆåŠŸ] cv2æˆåŠŸåŠ è½½WebP")
                                return img
                            else:
                                print(f"    [å¤±è´¥] cv2.imdecodeè¿”å›None")
                        except Exception as cv2_error:
                            print(f"    [å¤±è´¥] cv2åŠ è½½å¤±è´¥: {type(cv2_error).__name__}: {cv2_error}")
                    
                    # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥
                    print(f"  [æ€»ç»“] æ‰€æœ‰WebPåŠ è½½æ–¹æ¡ˆå‡å¤±è´¥")
                    if not HAS_IMAGEIO and not HAS_CV2:
                        print(f"      å»ºè®®å®‰è£…: pip install imageio imageio-ffmpeg æˆ– pip install opencv-python-headless")
                    raise pil_error
                else:
                    # ä¸æ˜¯WebPæ ¼å¼ï¼Œç›´æ¥æŠ›å‡ºåŸå§‹é”™è¯¯
                    print(f"  [æ€»ç»“] æœªçŸ¥å›¾ç‰‡æ ¼å¼ï¼Œæ— æ³•åŠ è½½")
                    raise pil_error
            
        except Exception as e:
            # æœ€ç»ˆé”™è¯¯å¤„ç†ï¼Œè¾“å‡ºæ‰€æœ‰è¯Šæ–­ä¿¡æ¯
            error_type = type(e).__name__
            error_msg = str(e)
            
            print(f"[ERROR] _base64_to_image æœ€ç»ˆå¤±è´¥:")
            print(f"  - é”™è¯¯ç±»å‹: {error_type}")
            print(f"  - é”™è¯¯ä¿¡æ¯: {error_msg}")
            print(f"  - è¾“å…¥ç±»å‹: {original_input_type}")
            print(f"  - è¾“å…¥é•¿åº¦: {original_input_length}")
            
            if img_bytes is not None:
                print(f"  - è§£ç åå¤§å°: {len(img_bytes)} bytes")
                print(f"  - æ–‡ä»¶å¤´(hex): {' '.join(f'{b:02x}' for b in img_bytes[:12])}")
                print(f"  - æ–‡ä»¶å¤´(ascii): {''.join(chr(b) if 32 <= b < 127 else '.' for b in img_bytes[:12])}")
                
                # å†æ¬¡æ£€æµ‹æ ¼å¼
                if len(img_bytes) >= 12:
                    if img_bytes[:4] == b'RIFF' and img_bytes[8:12] == b'WEBP':
                        print(f"  - æ ¼å¼: WebP")
                        if len(img_bytes) >= 16:
                            chunk_type = img_bytes[12:16]
                            print(f"  - WebP chunk: {chunk_type}")
                    elif img_bytes[:2] == b'\xff\xd8':
                        print(f"  - æ ¼å¼: JPEG")
                    elif img_bytes[:8] == b'\x89PNG\r\n\x1a\n':
                        print(f"  - æ ¼å¼: PNG")
                    else:
                        print(f"  - æ ¼å¼: æœªçŸ¥")
            else:
                print(f"  - Base64è§£ç å¤±è´¥ï¼Œæ— æ³•è·å–å›¾ç‰‡æ•°æ®")
            
            return None
    
    def get_feature_names(self) -> List[str]:
        """
        è·å–ç‰¹å¾åç§°(ç”¨äºåˆ†æ)ï¼Œæ ¹æ®feature_configè¿”å›å®é™…ä½¿ç”¨çš„ç‰¹å¾åç§°
        """
        names = []
        
        # å›¾åƒç‰¹å¾
        if self.feature_config.get('image', True):
            names.extend([f"image_feat_{i}" for i in range(512)])
        
        # å¯¹è¯ç‰¹å¾
        if self.feature_config.get('question', True):
            names.extend([f"question_feat_{i}" for i in range(512)])
        
        if self.feature_config.get('answer', True):
            names.extend([f"answer_feat_{i}" for i in range(512)])
        
        # ç»Ÿè®¡ç‰¹å¾
        if self.feature_config.get('statistical', True):
            names.extend([
                'dialogue_turns',
                'avg_question_words', 'std_question_words', 'avg_question_chars',
                'avg_answer_words', 'std_answer_words', 'avg_answer_chars',
                'num_images',
                'is_vqa', 'is_caption', 'is_multiturn'
            ])
        
        # äº¤äº’ç‰¹å¾
        if self.feature_config.get('interaction', True):
            names.extend([
                'img_question_sim', 'img_answer_sim', 'question_answer_sim',
                'has_image_reference', 'dialogue_coherence_mean', 'dialogue_coherence_std',
                'question_complexity', 'answer_detail', 'answer_question_ratio'
            ])
        
        return names


# ==================== 2. æµå¼æ•°æ®åŠ è½½å™¨ ====================
class VQADataLoader:
    """VQAæ•°æ®æµå¼åŠ è½½å™¨"""
    
    def __init__(self, data_path: str, batch_size: int = 100):
        self.data_path = Path(data_path)
        self.batch_size = batch_size
    
    def stream_batches(self):
        """æµå¼è¯»å–JSONLæ•°æ®"""
        batch = []
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    sample = json.loads(line)
                    batch.append(sample)
                    
                    if len(batch) >= self.batch_size:
                        yield batch
                        batch = []
        
        # è¿”å›æœ€åä¸€æ‰¹
        if batch:
            yield batch
    
    def count_samples(self) -> int:
        """ç»Ÿè®¡æ ·æœ¬æ€»æ•°"""
        count = 0
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    count += 1
        return count


# ==================== 3. èšç±»ç®—æ³• ====================
class ClusteringAlgorithm(ABC):
    """èšç±»ç®—æ³•æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def fit_predict(self, features: np.ndarray, **kwargs) -> np.ndarray:
        """æ‰§è¡Œèšç±»"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """è·å–ç®—æ³•åç§°"""
        pass


class AutoMiniBatchKMeans(ClusteringAlgorithm):
    """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°çš„Mini-Batch KMeans"""
    
    def __init__(self, random_state: int = 42, batch_size: int = 1000):
        self.random_state = random_state
        self.batch_size = batch_size
        self.model = None
        self.best_k = None
        self.scores = {}
    
    def fit_predict(self, features: np.ndarray, 
                    n_clusters: Optional[int] = None,
                    min_clusters: int = 5, 
                    max_clusters: int = 20,
                    auto_select: bool = True) -> np.ndarray:
        """
        æ‰§è¡Œèšç±»,æ”¯æŒè‡ªåŠ¨é€‰æ‹©ç°‡æ•°
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ
            n_clusters: æŒ‡å®šç°‡æ•°(å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©)
            min_clusters: æœ€å°ç°‡æ•°
            max_clusters: æœ€å¤§ç°‡æ•°
            auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°
        """
        
        # å¦‚æœæŒ‡å®šäº†ç°‡æ•°,ç›´æ¥èšç±»
        if n_clusters is not None:
            return self._cluster_with_k(features, n_clusters)
        
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°
        if not auto_select:
            n_clusters = min_clusters
            return self._cluster_with_k(features, n_clusters)
        
        print(f"\n  ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•° (èŒƒå›´: {min_clusters}-{max_clusters})...")
        
        # é‡‡æ ·ç”¨äºå¿«é€Ÿè¯„ä¼°
        n_samples = len(features)
        sample_size = min(10000, n_samples)
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        sample_features = features[sample_indices]
        
        # å°è¯•ä¸åŒçš„kå€¼
        max_clusters = min(max_clusters, sample_size // 2)
        best_score = -1
        best_k = min_clusters
        
        for k in range(min_clusters, max_clusters + 1):
            try:
                # ä¸´æ—¶æ¨¡å‹
                model = MiniBatchKMeans(
                    n_clusters=k,
                    random_state=self.random_state,
                    batch_size=min(self.batch_size, sample_size),
                    max_iter=100
                )
                labels = model.fit_predict(sample_features)
                
                # æ£€æŸ¥æ˜¯å¦äº§ç”Ÿäº†æœ‰æ•ˆçš„ç°‡
                unique_labels = len(np.unique(labels))
                if unique_labels <= 1:
                    continue
                
                # è®¡ç®—è½®å»“ç³»æ•°
                score = silhouette_score(
                    sample_features, 
                    labels, 
                    sample_size=min(5000, sample_size)
                )
                
                # åŒæ—¶è€ƒè™‘Calinski-HarabaszæŒ‡æ•°(å¥–åŠ±ç´§å¯†ä¸”åˆ†ç¦»çš„ç°‡)
                ch_score = calinski_harabasz_score(sample_features, labels)
                
                # ç»„åˆå¾—åˆ†(å¯è°ƒæ•´æƒé‡)
                combined_score = 0.7 * score + 0.3 * (ch_score / 10000)
                
                self.scores[k] = {
                    'silhouette': score,
                    'calinski_harabasz': ch_score,
                    'combined': combined_score
                }
                
                print(f"    k={k}: silhouette={score:.3f}, CH={ch_score:.1f}, combined={combined_score:.3f}")
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_k = k
                    
            except Exception as e:
                print(f"    k={k}: è¯„ä¼°å¤±è´¥ ({e})")
                continue
        
        self.best_k = best_k
        print(f"  âœ… é€‰æ‹©æœ€ä¼˜ç°‡æ•°: k={best_k} (å¾—åˆ†: {best_score:.3f})")
        
        # ä½¿ç”¨æœ€ä¼˜kå€¼è¿›è¡Œæœ€ç»ˆèšç±»
        return self._cluster_with_k(features, best_k)
    
    def _cluster_with_k(self, features: np.ndarray, k: int) -> np.ndarray:
        """ä½¿ç”¨æŒ‡å®škå€¼èšç±»"""
        print(f"  ğŸ¯ ä½¿ç”¨ k={k} è¿›è¡Œèšç±»...")
        
        self.model = MiniBatchKMeans(
            n_clusters=k,
            random_state=self.random_state,
            batch_size=self.batch_size,
            max_iter=300,
            n_init=10
        )
        
        labels = self.model.fit_predict(features)
        
        # ç»Ÿè®¡ç°‡å¤§å°
        unique, counts = np.unique(labels, return_counts=True)
        print(f"  ğŸ“Š ç°‡åˆ†å¸ƒ:")
        for cluster_id, count in zip(unique, counts):
            print(f"    ç°‡ {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count/len(labels)*100:.1f}%)")
        
        return labels
    
    def get_name(self) -> str:
        return "MiniBatchKMeans"


class HierarchicalClustering(ClusteringAlgorithm):
    """å±‚æ¬¡èšç±»(é€‚åˆä¸­å°è§„æ¨¡æ•°æ®)ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°"""
    
    def __init__(self, linkage: str = 'ward', distance_threshold: float = None):
        self.linkage = linkage
        self.distance_threshold = distance_threshold
        self.model = None
        self.best_k = None
        self.scores = {}
    
    def fit_predict(self, features: np.ndarray, 
                    n_clusters: Optional[int] = None,
                    min_clusters: int = 5,
                    max_clusters: int = 20,
                    auto_select: bool = True,
                    **kwargs) -> np.ndarray:
        """
        æ‰§è¡Œå±‚æ¬¡èšç±»ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ
            n_clusters: æŒ‡å®šç°‡æ•°(å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©)
            min_clusters: æœ€å°ç°‡æ•°
            max_clusters: æœ€å¤§ç°‡æ•°
            auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°
        """
        
        if len(features) > 10000:
            print(f"  âš ï¸  æ•°æ®é‡è¾ƒå¤§({len(features)}),å±‚æ¬¡èšç±»å¯èƒ½è¾ƒæ…¢")
        
        # å¦‚æœæŒ‡å®šäº†ç°‡æ•°,ç›´æ¥èšç±»
        if n_clusters is not None:
            return self._cluster_with_k(features, n_clusters)
        
        # å¦‚æœä½¿ç”¨distance_thresholdï¼Œç›´æ¥èšç±»
        if self.distance_threshold is not None:
            return self._cluster_with_threshold(features)
        
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°
        if not auto_select:
            n_clusters = min_clusters
            return self._cluster_with_k(features, n_clusters)
        
        print(f"\n  ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•° (èŒƒå›´: {min_clusters}-{max_clusters})...")
        
        # é‡‡æ ·ç”¨äºå¿«é€Ÿè¯„ä¼°ï¼ˆå±‚æ¬¡èšç±»è®¡ç®—é‡å¤§ï¼‰
        n_samples = len(features)
        sample_size = min(5000, n_samples)  # å±‚æ¬¡èšç±»é‡‡æ ·æ›´å°
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        sample_features = features[sample_indices]
        
        # å°è¯•ä¸åŒçš„kå€¼
        max_clusters = min(max_clusters, sample_size // 2)
        best_score = -1
        best_k = min_clusters
        
        for k in range(min_clusters, max_clusters + 1):
            try:
                # ä¸´æ—¶æ¨¡å‹
                model = AgglomerativeClustering(
                    n_clusters=k,
                    linkage=self.linkage
                )
                labels = model.fit_predict(sample_features)
                
                # æ£€æŸ¥æ˜¯å¦äº§ç”Ÿäº†æœ‰æ•ˆçš„ç°‡
                unique_labels = len(np.unique(labels))
                if unique_labels <= 1:
                    continue
                
                # è®¡ç®—è½®å»“ç³»æ•°
                score = silhouette_score(
                    sample_features, 
                    labels, 
                    sample_size=min(3000, sample_size)
                )
                
                # åŒæ—¶è€ƒè™‘Calinski-HarabaszæŒ‡æ•°
                ch_score = calinski_harabasz_score(sample_features, labels)
                
                # ç»„åˆå¾—åˆ†(å¯è°ƒæ•´æƒé‡)
                combined_score = 0.7 * score + 0.3 * (ch_score / 10000)
                
                self.scores[k] = {
                    'silhouette': score,
                    'calinski_harabasz': ch_score,
                    'combined': combined_score
                }
                
                print(f"    k={k}: silhouette={score:.3f}, CH={ch_score:.1f}, combined={combined_score:.3f}")
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_k = k
                    
            except Exception as e:
                print(f"    k={k}: è¯„ä¼°å¤±è´¥ ({e})")
                continue
        
        self.best_k = best_k
        print(f"  âœ… é€‰æ‹©æœ€ä¼˜ç°‡æ•°: k={best_k} (å¾—åˆ†: {best_score:.3f})")
        
        # ä½¿ç”¨æœ€ä¼˜kå€¼è¿›è¡Œæœ€ç»ˆèšç±»
        return self._cluster_with_k(features, best_k)
    
    def _cluster_with_k(self, features: np.ndarray, k: int) -> np.ndarray:
        """ä½¿ç”¨æŒ‡å®škå€¼èšç±»"""
        print(f"  ğŸ¯ ä½¿ç”¨ k={k} è¿›è¡Œå±‚æ¬¡èšç±»...")
        
        self.model = AgglomerativeClustering(
            n_clusters=k,
            linkage=self.linkage
        )
        
        labels = self.model.fit_predict(features)
        
        # ç»Ÿè®¡ç°‡å¤§å°
        unique, counts = np.unique(labels, return_counts=True)
        print(f"  ğŸ“Š ç°‡åˆ†å¸ƒ:")
        for cluster_id, count in zip(unique, counts):
            print(f"    ç°‡ {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count/len(labels)*100:.1f}%)")
        
        return labels
    
    def _cluster_with_threshold(self, features: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨distance_thresholdèšç±»"""
        print(f"  ğŸ¯ ä½¿ç”¨ distance_threshold={self.distance_threshold} è¿›è¡Œå±‚æ¬¡èšç±»...")
        
        self.model = AgglomerativeClustering(
            distance_threshold=self.distance_threshold,
            linkage=self.linkage,
            n_clusters=None
        )
        
        labels = self.model.fit_predict(features)
        
        n_clusters = len(np.unique(labels))
        print(f"  ğŸ“Š ç”Ÿæˆäº† {n_clusters} ä¸ªç°‡")
        
        return labels
    
    def get_name(self) -> str:
        return "Hierarchical"


class DensityClustering(ClusteringAlgorithm):
    """åŸºäºå¯†åº¦çš„èšç±»(DBSCAN)ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°"""
    
    def __init__(self, eps: float = 0.5, min_samples: int = 5):
        self.eps = eps
        self.min_samples = min_samples
        self.model = None
        self.best_eps = None
        self.best_min_samples = None
        self.scores = {}
    
    def fit_predict(self, features: np.ndarray,
                    eps: Optional[float] = None,
                    min_samples: Optional[int] = None,
                    eps_range: Tuple[float, float] = (0.1, 2.0),
                    eps_steps: int = 10,
                    min_samples_range: Tuple[int, int] = (3, 10),
                    auto_select: bool = True,
                    **kwargs) -> np.ndarray:
        """
        æ‰§è¡ŒDBSCANèšç±»ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°
        
        Args:
            features: ç‰¹å¾çŸ©é˜µ
            eps: æŒ‡å®šepså‚æ•°(å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©)
            min_samples: æŒ‡å®šmin_sampleså‚æ•°(å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©)
            eps_range: epsæœç´¢èŒƒå›´ (min, max)
            eps_steps: epsæœç´¢æ­¥æ•°
            min_samples_range: min_samplesæœç´¢èŒƒå›´ (min, max)
            auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°
        """
        
        # å¦‚æœæŒ‡å®šäº†å‚æ•°,ç›´æ¥èšç±»
        if eps is not None and min_samples is not None:
            self.eps = eps
            self.min_samples = min_samples
            return self._cluster_with_params(features)
        
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°
        if not auto_select:
            if eps is not None:
                self.eps = eps
            if min_samples is not None:
                self.min_samples = min_samples
            return self._cluster_with_params(features)
        
        print(f"\n  ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜DBSCANå‚æ•°...")
        
        # é‡‡æ ·ç”¨äºå¿«é€Ÿè¯„ä¼°
        n_samples = len(features)
        sample_size = min(10000, n_samples)
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        sample_features = features[sample_indices]
        
        # æ™ºèƒ½ä¼°è®¡epsèŒƒå›´ï¼šä½¿ç”¨k-è·ç¦»å›¾æ–¹æ³•
        print(f"    ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒä»¥ä¼°è®¡epsèŒƒå›´...")
        
        # æ–¹æ³•1: è®¡ç®—æœ€è¿‘é‚»è·ç¦»çš„ç»Ÿè®¡ä¿¡æ¯
        from sklearn.neighbors import NearestNeighbors
        n_neighbors = min(min_samples_range[1] + 1, sample_size - 1)
        if n_neighbors < 2:
            n_neighbors = 2
        
        try:
            neighbors = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=-1)
            neighbors_fit = neighbors.fit(sample_features)
            distances, indices = neighbors_fit.kneighbors(sample_features)
            
            # è·å–ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»ï¼ˆk=min_samplesï¼‰
            k_distances = distances[:, min_samples_range[0]:min_samples_range[1]+1].mean(axis=1)
            k_distances_sorted = np.sort(k_distances)
            
            # ä½¿ç”¨åˆ†ä½æ•°æ¥ä¼°è®¡epsèŒƒå›´
            # é€šå¸¸epsåº”è¯¥åœ¨ç¬¬50-90ç™¾åˆ†ä½ä¹‹é—´
            eps_min_estimate = np.percentile(k_distances_sorted, 25)
            eps_max_estimate = np.percentile(k_distances_sorted, 90)
            
            # å¦‚æœä¼°è®¡çš„èŒƒå›´å¤ªå°ï¼Œä½¿ç”¨ç‰¹å¾æ ‡å‡†å·®ä½œä¸ºå‚è€ƒ
            feature_std = np.std(sample_features)
            if eps_max_estimate < feature_std * 0.1:
                eps_max_estimate = feature_std * 0.5
            
            # è°ƒæ•´epsèŒƒå›´
            eps_min = max(eps_range[0], eps_min_estimate * 0.5)
            eps_max = min(eps_range[1], eps_max_estimate * 2.0)
            
            # ç¡®ä¿èŒƒå›´åˆç†
            if eps_max <= eps_min:
                eps_max = eps_min * 3.0
            
            eps_range = (eps_min, eps_max)
            print(f"    âœ… æ ¹æ®k-è·ç¦»å›¾ä¼°è®¡epsèŒƒå›´: {eps_range[0]:.3f} - {eps_range[1]:.3f}")
            print(f"       (åŸå§‹èŒƒå›´: {eps_range[0]:.3f} - {eps_range[1]:.3f})")
            print(f"       k-è·ç¦»ç»Ÿè®¡: min={k_distances_sorted[0]:.3f}, "
                  f"median={np.median(k_distances_sorted):.3f}, "
                  f"max={k_distances_sorted[-1]:.3f}")
        except Exception as e:
            print(f"    âš ï¸  k-è·ç¦»å›¾åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹èŒƒå›´")
            # å¦‚æœk-è·ç¦»å›¾å¤±è´¥ï¼Œä½¿ç”¨ç‰¹å¾æ ‡å‡†å·®
            feature_std = np.std(sample_features)
            if eps_range[1] > feature_std * 3:
                eps_range = (eps_range[0], min(eps_range[1], feature_std * 2.0))
                print(f"    [è°ƒæ•´] æ ¹æ®ç‰¹å¾æ ‡å‡†å·®è°ƒæ•´epsèŒƒå›´: {eps_range[0]:.3f} - {eps_range[1]:.3f}")
        
        print(f"    epsèŒƒå›´: {eps_range[0]:.3f} - {eps_range[1]:.3f} (æ­¥æ•°: {eps_steps})")
        print(f"    min_samplesèŒƒå›´: {min_samples_range[0]} - {min_samples_range[1]}")
        
        # ç”Ÿæˆå‚æ•°ç½‘æ ¼
        eps_values = np.linspace(eps_range[0], eps_range[1], eps_steps)
        min_samples_values = range(min_samples_range[0], min_samples_range[1] + 1)
        
        best_score = -1
        best_eps = eps_values[0]
        best_min_samples = min_samples_values[0]
        
        total_combinations = len(eps_values) * len(min_samples_values)
        current_combination = 0
        
        for eps_val in eps_values:
            for min_samples_val in min_samples_values:
                current_combination += 1
                try:
                    # ä¸´æ—¶æ¨¡å‹
                    model = DBSCAN(eps=eps_val, min_samples=min_samples_val, n_jobs=-1)
                    labels = model.fit_predict(sample_features)
                    
                    # ç»Ÿè®¡ç»“æœ
                    unique_labels = np.unique(labels)
                    n_clusters = len(unique_labels[unique_labels != -1])  # æ’é™¤å™ªå£°ç‚¹
                    n_noise = (labels == -1).sum()
                    
                    # æ£€æŸ¥æ˜¯å¦äº§ç”Ÿäº†æœ‰æ•ˆçš„ç°‡
                    if n_clusters <= 1:
                        continue
                    
                    # å¦‚æœå™ªå£°ç‚¹å¤ªå¤šï¼Œè·³è¿‡
                    noise_ratio = n_noise / len(labels)
                    if noise_ratio > 0.7:  # å™ªå£°ç‚¹è¶…è¿‡70%ï¼Œè·³è¿‡ï¼ˆæ”¾å®½é˜ˆå€¼ï¼‰
                        continue
                    
                    # å¦‚æœç°‡å¤ªå°‘ï¼Œä¹Ÿè·³è¿‡ï¼ˆå¯èƒ½æ˜¯epså¤ªå¤§ï¼‰
                    if n_clusters == 0:
                        continue
                    
                    # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆåªå¯¹éå™ªå£°ç‚¹ï¼‰
                    non_noise_mask = labels != -1
                    if non_noise_mask.sum() < 2:
                        continue
                    
                    try:
                        score = silhouette_score(
                            sample_features[non_noise_mask],
                            labels[non_noise_mask],
                            sample_size=min(5000, non_noise_mask.sum())
                        )
                    except:
                        continue
                    
                    # åŒæ—¶è€ƒè™‘ç°‡æ•°å’Œå™ªå£°æ¯”ä¾‹
                    # å¥–åŠ±ï¼šæ›´å¤šç°‡ã€æ›´å°‘å™ªå£°ã€æ›´é«˜è½®å»“ç³»æ•°
                    cluster_bonus = min(n_clusters / 20.0, 1.0)  # ç°‡æ•°å¥–åŠ±ï¼ˆæœ€å¤š20ä¸ªç°‡ï¼‰
                    noise_penalty = noise_ratio  # å™ªå£°æƒ©ç½š
                    
                    # ç»„åˆå¾—åˆ†ï¼ˆè°ƒæ•´æƒé‡ï¼Œæ›´é‡è§†è½®å»“ç³»æ•°å’Œç°‡æ•°ï¼‰
                    # å¦‚æœå™ªå£°ç‚¹å¤ªå¤šï¼Œå¤§å¹…æƒ©ç½š
                    if noise_ratio > 0.3:
                        combined_score = 0.4 * score + 0.2 * cluster_bonus - 0.4 * noise_penalty
                    else:
                        combined_score = 0.6 * score + 0.3 * cluster_bonus - 0.1 * noise_penalty
                    
                    param_key = f"eps={eps_val:.3f},min_samples={min_samples_val}"
                    self.scores[param_key] = {
                        'silhouette': score,
                        'n_clusters': n_clusters,
                        'noise_ratio': noise_ratio,
                        'combined': combined_score
                    }
                    
                    if current_combination % 5 == 0 or combined_score > best_score:
                        print(f"    [{current_combination}/{total_combinations}] "
                              f"eps={eps_val:.3f}, min_samples={min_samples_val}: "
                              f"clusters={n_clusters}, noise={noise_ratio:.2%}, "
                              f"score={score:.3f}, combined={combined_score:.3f}")
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_eps = eps_val
                        best_min_samples = min_samples_val
                        
                except Exception as e:
                    if current_combination % 10 == 0:
                        print(f"    [{current_combination}/{total_combinations}] "
                              f"eps={eps_val:.3f}, min_samples={min_samples_val}: è¯„ä¼°å¤±è´¥ ({e})")
                    continue
        
        self.best_eps = best_eps
        self.best_min_samples = best_min_samples
        print(f"  âœ… é€‰æ‹©æœ€ä¼˜å‚æ•°: eps={best_eps:.3f}, min_samples={best_min_samples} (å¾—åˆ†: {best_score:.3f})")
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°è¿›è¡Œæœ€ç»ˆèšç±»
        self.eps = best_eps
        self.min_samples = best_min_samples
        return self._cluster_with_params(features)
    
    def _cluster_with_params(self, features: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨æŒ‡å®šå‚æ•°èšç±»"""
        print(f"  ğŸ¯ ä½¿ç”¨ eps={self.eps:.3f}, min_samples={self.min_samples} è¿›è¡ŒDBSCANèšç±»...")
        
        self.model = DBSCAN(eps=self.eps, min_samples=self.min_samples, n_jobs=-1)
        labels = self.model.fit_predict(features)
        
        # ç»Ÿè®¡å™ªå£°ç‚¹å’Œç°‡
        unique_labels = np.unique(labels)
        n_clusters = len(unique_labels[unique_labels != -1])
        n_noise = (labels == -1).sum()
        
        print(f"  ğŸ“Š ç”Ÿæˆäº† {n_clusters} ä¸ªç°‡, {n_noise} ä¸ªå™ªå£°ç‚¹ ({n_noise/len(labels)*100:.1f}%)")
        
        # ç»Ÿè®¡ç°‡å¤§å°
        if n_clusters > 0:
            non_noise_labels = labels[labels != -1]
            if len(non_noise_labels) > 0:
                unique, counts = np.unique(non_noise_labels, return_counts=True)
                print(f"  ğŸ“Š ç°‡åˆ†å¸ƒ:")
                for cluster_id, count in zip(unique, counts):
                    print(f"    ç°‡ {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count/len(labels)*100:.1f}%)")
        
        return labels
    
    def get_name(self) -> str:
        return "DBSCAN"


# ==================== 4. VQAèšç±»ç®¡é“ ====================
class VQAClusteringPipeline:
    """VQAæ•°æ®èšç±»ç®¡é“"""
    
    def __init__(
        self,
        feature_extractor,  # VQAFeatureExtractorå®ä¾‹
        clustering_algorithm: ClusteringAlgorithm,
        cache_dir: str = "./vqa_cache",
        batch_size: int = 100,
        clustering_params: Dict = None
    ):
        self.feature_extractor = feature_extractor
        self.clustering_algorithm = clustering_algorithm
        self.cache_dir = cache_dir
        self.batch_size = batch_size
        self.clustering_params = clustering_params or {}
        
        # ç‰¹å¾ç¼“å­˜
        self.feature_cache = FeatureCache(
            cache_dir, 
            feature_dim=self.feature_extractor.get_feature_names().__len__()
        )
    
    def run(self, input_jsonl: str, output_path: str):
        """æ‰§è¡Œå®Œæ•´èšç±»æµç¨‹"""
        print("\n" + "=" * 70)
        print("ğŸš€ VQAæ•°æ®èšç±»æµç¨‹")
        print("=" * 70)
        
        # é˜¶æ®µ1: ç»Ÿè®¡æ ·æœ¬æ•°
        print("\n[é˜¶æ®µ 1/4] ç»Ÿè®¡æ•°æ®...")
        n_samples = self._count_samples(input_jsonl)
        print(f"  ğŸ“Š æ€»æ ·æœ¬æ•°: {n_samples}")
        
        # é˜¶æ®µ2: æ‰¹é‡æå–ç‰¹å¾
        print("\n[é˜¶æ®µ 2/4] æ‰¹é‡æå–ç‰¹å¾...")
        self._extract_features(input_jsonl, n_samples)
        
        # é˜¶æ®µ3: æ‰§è¡Œèšç±»
        print("\n[é˜¶æ®µ 3/4] æ‰§è¡Œèšç±»...")
        labels = self._perform_clustering()
        
        # é˜¶æ®µ4: ä¿å­˜ç»“æœ
        print("\n[é˜¶æ®µ 4/4] ä¿å­˜ç»“æœ...")
        self._save_results(input_jsonl, labels, output_path)
        
        # æ¸…ç†ç¼“å­˜
        self.feature_cache.cleanup()
        
        print("\n" + "=" * 70)
        print("âœ… èšç±»å®Œæˆ!")
        print("=" * 70 + "\n")
    
    def _count_samples(self, data_path: str) -> int:
        """ç»Ÿè®¡æ ·æœ¬æ•°"""
        loader = VQADataLoader(data_path, self.batch_size)
        return loader.count_samples()
    
    def _extract_features(self, data_path: str, n_samples: int):
        """æ‰¹é‡æå–ç‰¹å¾å¹¶ç¼“å­˜"""
        # åˆ›å»ºç¼“å­˜
        self.feature_cache.create_cache('vqa', n_samples)
        
        loader = VQADataLoader(data_path, self.batch_size)
        
        batch_idx = 0
        global_start_idx = 0
        
        for batch_samples in loader.stream_batches():
            batch_idx += 1
            batch_size = len(batch_samples)
            
            print(f"  æ‰¹æ¬¡ {batch_idx}: å¤„ç† {batch_size} ä¸ªæ ·æœ¬...")
            
            # æå–ç‰¹å¾
            features = self.feature_extractor.extract_batch(batch_samples)
            
            # å†™å…¥ç¼“å­˜
            indices = np.arange(global_start_idx, global_start_idx + batch_size)
            self.feature_cache.write_batch('vqa', indices, features)
            
            global_start_idx += batch_size
            
            # å†…å­˜ç®¡ç†
            del features
            gc.collect()
        
        print(f"  âœ… ç‰¹å¾æå–å®Œæˆ")
    
    def _perform_clustering(self) -> np.ndarray:
        """æ‰§è¡Œèšç±»"""
        # ä»ç¼“å­˜è¯»å–æ‰€æœ‰ç‰¹å¾
        print(f"  ğŸ“¥ ä»ç¼“å­˜åŠ è½½ç‰¹å¾...")
        features = self.feature_cache.read_all('vqa')
        print(f"  âœ… åŠ è½½å®Œæˆ: {features.shape}")
        
        # æ‰§è¡Œèšç±»
        print(f"  ğŸ¯ ä½¿ç”¨ç®—æ³•: {self.clustering_algorithm.get_name()}")
        labels = self.clustering_algorithm.fit_predict(features, **self.clustering_params)
        
        # è¯„ä¼°èšç±»è´¨é‡
        self._evaluate_clustering(features, labels)
        
        return labels
    
    def _evaluate_clustering(self, features: np.ndarray, labels: np.ndarray):
        """è¯„ä¼°èšç±»è´¨é‡"""
        print(f"\n  ğŸ“Š èšç±»è´¨é‡è¯„ä¼°:")
        
        unique_labels = np.unique(labels)
        n_clusters = len(unique_labels[unique_labels != -1])  # æ’é™¤å™ªå£°ç‚¹
        
        if n_clusters <= 1:
            print(f"    âš ï¸  åªæœ‰1ä¸ªç°‡,æ— æ³•è®¡ç®—è´¨é‡æŒ‡æ ‡")
            return
        
        # é‡‡æ ·è¯„ä¼°(å¤§æ•°æ®é›†)
        if len(features) > 10000:
            sample_size = 10000
            sample_indices = np.random.choice(len(features), sample_size, replace=False)
            features_sample = features[sample_indices]
            labels_sample = labels[sample_indices]
        else:
            features_sample = features
            labels_sample = labels
        
        try:
            # è½®å»“ç³»æ•°
            sil_score = silhouette_score(features_sample, labels_sample)
            print(f"    - Silhouette Score: {sil_score:.4f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
            
            # Calinski-HarabaszæŒ‡æ•°
            ch_score = calinski_harabasz_score(features_sample, labels_sample)
            print(f"    - Calinski-Harabasz: {ch_score:.2f} (è¶Šå¤§è¶Šå¥½)")
            
            # Davies-BouldinæŒ‡æ•°
            db_score = davies_bouldin_score(features_sample, labels_sample)
            print(f"    - Davies-Bouldin: {db_score:.4f} (è¶Šå°è¶Šå¥½)")
            
        except Exception as e:
            print(f"    âš ï¸  è¯„ä¼°å¤±è´¥: {e}")
    
    def _save_results(self, data_path: str, labels: np.ndarray, output_path: str):
        """ä¿å­˜èšç±»ç»“æœ"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºèšç±»ç»“æœå­—å…¸
        clusters = defaultdict(list)
        
        loader = VQADataLoader(data_path, self.batch_size)
        
        sample_idx = 0
        for batch_samples in loader.stream_batches():
            for sample in batch_samples:
                cluster_id = int(labels[sample_idx])
                
                clusters[cluster_id].append({
                    'sample_id': sample_idx,
                    'cluster_id': cluster_id,
                    'data': sample
                })
                
                sample_idx += 1
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_data = {
            'metadata': {
                'total_samples': len(labels),
                'n_clusters': len(clusters),
                'algorithm': self.clustering_algorithm.get_name()
            },
            'clusters': []
        }
        
        for cluster_id in sorted(clusters.keys()):
            samples = clusters[cluster_id]
            output_data['clusters'].append({
                'cluster_id': cluster_id,
                'size': len(samples),
                'percentage': len(samples) / len(labels) * 100,
                'samples': samples
            })
        
        # ä¿å­˜ä¸ºJSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n  ğŸ“ˆ èšç±»ç»Ÿè®¡:")
        for cluster_info in output_data['clusters']:
            print(f"    ç°‡ {cluster_info['cluster_id']}: "
                  f"{cluster_info['size']} æ ·æœ¬ ({cluster_info['percentage']:.1f}%)")


# ==================== 5. ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    
    # 1. åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    print("ğŸ”§ åˆå§‹åŒ–ç‰¹å¾æå–å™¨...")
    
    # ========== ç‰¹å¾é…ç½®ç¤ºä¾‹ ==========
    # æ–¹æ¡ˆA: ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼ˆé»˜è®¤ï¼‰
    # feature_config = None  # æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
    
    # æ–¹æ¡ˆB: åªä½¿ç”¨é—®é¢˜ç‰¹å¾è¿›è¡Œèšç±»
    feature_config = {
        'image': False,
        'question': False,    
        'answer': True,
        'statistical': False,
        'interaction': False
    }
    
    # æ–¹æ¡ˆC: ä½¿ç”¨é—®é¢˜å’Œç­”æ¡ˆç‰¹å¾
    # feature_config = {
    #     'image': False,
    #     'question': True,
    #     'answer': True,
    #     'statistical': False,
    #     'interaction': False
    # }
    
    # æ–¹æ¡ˆD: ä½¿ç”¨å›¾åƒå’Œé—®é¢˜ç‰¹å¾
    # feature_config = {
    #     'image': True,
    #     'question': True,
    #     'answer': False,
    #     'statistical': False,
    #     'interaction': True  # äº¤äº’ç‰¹å¾éœ€è¦å›¾åƒå’Œå¯¹è¯ç‰¹å¾
    # }
    
    feature_extractor = VQAFeatureExtractor(
        model_name="openai/clip-vit-base-patch32",
        device="cuda",  # æˆ– "cpu"
        normalize=True,
        feature_config=feature_config  # ä¼ å…¥ç‰¹å¾é…ç½®
    )
    
    # 2. é€‰æ‹©èšç±»ç®—æ³•
    # æ–¹æ¡ˆA: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°çš„KMeans
    clustering_algo = AutoMiniBatchKMeans(random_state=42, batch_size=1000)
    clustering_params = {
        'n_clusters': None,  # Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
        'min_clusters': 1,
        'max_clusters': 20,
        'auto_select': True
    }
    
    # æ–¹æ¡ˆB: æŒ‡å®šç°‡æ•°çš„KMeans
    # clustering_algo = AutoMiniBatchKMeans(random_state=42, batch_size=1000)
    # clustering_params = {'n_clusters': 10}
    
    # # æ–¹æ¡ˆC: å±‚æ¬¡èšç±»ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç°‡æ•°ï¼‰
    # clustering_algo = HierarchicalClustering(linkage='ward')
    # clustering_params = {
    #     'n_clusters': None,  # Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    #     'min_clusters': 1,
    #     'max_clusters': 20,
    #     'auto_select': True
    # }
    
    # æ–¹æ¡ˆC1: å±‚æ¬¡èšç±»ï¼ˆæŒ‡å®šç°‡æ•°ï¼‰
    # clustering_algo = HierarchicalClustering(linkage='ward')
    # clustering_params = {'n_clusters': 10}
    
    # æ–¹æ¡ˆD: DBSCAN(åŸºäºå¯†åº¦ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°)
    # clustering_algo = DensityClustering(eps=0.5, min_samples=5)  # åˆå§‹å€¼ï¼Œä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–
    # clustering_params = {
    #     'eps': None,  # Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    #     'min_samples': None,  # Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    #     'eps_range': (0.1, 2.0),  # epsæœç´¢èŒƒå›´
    #     'eps_steps': 10,  # epsæœç´¢æ­¥æ•°
    #     'min_samples_range': (3, 10),  # min_samplesæœç´¢èŒƒå›´
    #     'auto_select': True
    # }
    
    # æ–¹æ¡ˆD1: DBSCAN(æŒ‡å®šå‚æ•°)
    # clustering_algo = DensityClustering(eps=0.5, min_samples=5)
    # clustering_params = {'eps': 0.5, 'min_samples': 5}
    
    # 3. åˆå§‹åŒ–èšç±»ç®¡é“
    pipeline = VQAClusteringPipeline(
        feature_extractor=feature_extractor,
        clustering_algorithm=clustering_algo,
        cache_dir="./vqa_cache",
        batch_size=100,
        clustering_params=clustering_params
    )
    
    # 4. æ‰§è¡Œèšç±»
    pipeline.run(
        input_jsonl="/user/zhuxuzhou/a_cluster_test/converted_clean_content_standardized_final.jsonl",  #  "/user/zhuxuzhou/a_cluster_test/converted_clean_content.jsonl",
        output_path="/user/zhuxuzhou/a_whole_pipeline/cluster/vqa_clustered_results_final.json"
    )
    
    print("\nâœ¨ å…¨éƒ¨å®Œæˆ!")