"""
è®­ç»ƒå™¨
é›†æˆdataå’Œmodelæ¨¡å—ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒåŠŸèƒ½
"""
import logging
import torch
import torch.nn as nn
from torch.optim import AdamW, Adam, SGD
from torch.optim.lr_scheduler import (
    StepLR, CosineAnnealingLR, ReduceLROnPlateau,
    LinearLR, ExponentialLR
)
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import time
from tqdm import tqdm

# å¯¼å…¥dataå’Œmodelæ¨¡å—ï¼ˆä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼‰
from data.data_pipeline import DataPipeline
from models.model_loader import load_model
from models.model_utils import freeze_model, print_model_summary, get_model_info
from training.callbacks import (
    Callback, EarlyStoppingCallback, ModelCheckpointCallback,
    LearningRateSchedulerCallback, TensorBoardCallback,
    ProgressBarCallback, CSVLoggerCallback
)
from training.evaluator import Evaluator, VQAEvaluator

logger = logging.getLogger(__name__)

# åˆ›å»ºè°ƒè¯•æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
_debug_log_file = None
_debug_log_handler = None

def setup_debug_logger(log_dir: str = "logs"):
    """è®¾ç½®è°ƒè¯•æ—¥å¿—æ–‡ä»¶"""
    global _debug_log_file, _debug_log_handler
    
    import os
    from pathlib import Path
    from datetime import datetime
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè°ƒè¯•æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    _debug_log_file = log_path / f"debug_{timestamp}.log"
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
    _debug_log_handler = logging.FileHandler(_debug_log_file, mode='w', encoding='utf-8')
    _debug_log_handler.setLevel(logging.DEBUG)
    
    # è®¾ç½®æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    _debug_log_handler.setFormatter(formatter)
    
    # åˆ›å»ºè°ƒè¯•logger
    debug_logger = logging.getLogger('debug')
    debug_logger.setLevel(logging.DEBUG)
    debug_logger.addHandler(_debug_log_handler)
    debug_logger.propagate = False  # ä¸ä¼ æ’­åˆ°root loggerï¼Œé¿å…åˆ·å±
    
    logger.info(f"è°ƒè¯•æ—¥å¿—æ–‡ä»¶: {_debug_log_file}")
    return debug_logger

def get_debug_logger():
    """è·å–è°ƒè¯•logger"""
    debug_logger = logging.getLogger('debug')
    if not debug_logger.handlers:
        # å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        setup_debug_logger()
    return debug_logger


class Trainer:
    """è®­ç»ƒå™¨ç±»"""
    
    def __init__(
        self,
        model: nn.Module,
        train_dataloader: torch.utils.data.DataLoader,
        val_dataloader: Optional[torch.utils.data.DataLoader] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        device: Optional[str] = None,
        callbacks: Optional[List[Callback]] = None,
        evaluator: Optional[Evaluator] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            optimizer: ä¼˜åŒ–å™¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šä½¿ç”¨é»˜è®¤AdamWï¼‰
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
            device: è®¾å¤‡
            callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
            evaluator: è¯„ä¼°å™¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šåˆ›å»ºé»˜è®¤çš„ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
                - num_epochs: è®­ç»ƒè½®æ•°
                - gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
                - max_grad_norm: æ¢¯åº¦è£å‰ª
                - fp16: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                - save_dir: ä¿å­˜ç›®å½•
        """
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        
        # è®¾å¤‡
        self.device = device or next(model.parameters()).device
        self.model = self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        if optimizer is None:
            self.optimizer = AdamW(model.parameters(), lr=3e-5)
        else:
            self.optimizer = optimizer
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = scheduler
        
        # å›è°ƒå‡½æ•°
        self.callbacks = callbacks or []
        
        # è¯„ä¼°å™¨
        self.evaluator = evaluator or Evaluator(model, device=self.device)
        
        # è®­ç»ƒå‚æ•°
        self.num_epochs = kwargs.get('num_epochs', 3)
        self.gradient_accumulation_steps = kwargs.get('gradient_accumulation_steps', 1)
        self.max_grad_norm = kwargs.get('max_grad_norm', None)
        self.fp16 = kwargs.get('fp16', False)
        self.save_dir = kwargs.get('save_dir', 'checkpoints')
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.should_stop = False
        self.history = []
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if self.fp16:
            try:
                from torch.cuda.amp import autocast, GradScaler
                self.scaler = GradScaler()
                self.use_amp = True
                logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰")
            except ImportError:
                logger.warning("æ— æ³•å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œéœ€è¦CUDAæ”¯æŒ")
                self.use_amp = False
        else:
            self.use_amp = False
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        Path(self.save_dir).mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®è°ƒè¯•æ—¥å¿—ï¼ˆå†™å…¥æ–‡ä»¶ï¼Œä¸åˆ·å±ï¼‰
        self.debug_logger = setup_debug_logger(log_dir=str(Path(self.save_dir) / "debug_logs"))
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataloader.dataset)}")
        if val_dataloader:
            logger.info(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataloader.dataset)}")
        
        # éªŒè¯ç¬¬ä¸€ä¸ªbatch
        self.validate_first_batch()
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹è®­ç»ƒ")
        logger.info("=" * 60)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print_model_summary(self.model)
        
        # è°ƒç”¨è®­ç»ƒå¼€å§‹å›è°ƒ
        self._call_callbacks('on_train_begin')
        
        try:
            for epoch in range(self.num_epochs):
                self.current_epoch = epoch
                
                # è°ƒç”¨epochå¼€å§‹å›è°ƒ
                self._call_callbacks('on_epoch_begin', epoch=epoch)
                
                # è®­ç»ƒä¸€ä¸ªepoch
                train_logs = self._train_epoch()
                
                # éªŒè¯ï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
                val_logs = {}
                if self.val_dataloader:
                    val_logs = self._validate()
                
                # åˆå¹¶æ—¥å¿—
                epoch_logs = {**train_logs, **val_logs}
                epoch_logs['epoch'] = epoch
                
                # è®°å½•å†å²
                self.history.append(epoch_logs)
                
                # è°ƒç”¨epochç»“æŸå›è°ƒ
                self._call_callbacks('on_epoch_end', epoch=epoch, logs=epoch_logs)
                
                # æ£€æŸ¥æ˜¯å¦æ—©åœ
                if self.should_stop:
                    logger.info("è®­ç»ƒæå‰åœæ­¢")
                    break
        
        except KeyboardInterrupt:
            logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        finally:
            # è°ƒç”¨è®­ç»ƒç»“æŸå›è°ƒ
            self._call_callbacks('on_train_end')
            logger.info("è®­ç»ƒå®Œæˆ")
    
    def _train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(self.train_dataloader, desc=f"Epoch {self.current_epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            # è°ƒç”¨batchå¼€å§‹å›è°ƒ
            self._call_callbacks('on_batch_begin', batch=batch_idx)
            
            try:
                # å‡†å¤‡è¾“å…¥ï¼ˆåŒ…å«éªŒè¯å’Œä¿®å¤ï¼‰
                # æ³¨æ„ï¼šåœ¨CPUä¸Šå®Œæˆæ‰€æœ‰éªŒè¯å’Œä¿®å¤ï¼Œç„¶åå†ç§»åŠ¨åˆ°GPU
                batch = self._prepare_batch(batch)
                
                # åœ¨ç§»åŠ¨åˆ°GPUåï¼Œå†æ¬¡éªŒè¯ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                self._validate_batch_on_device(batch, batch_idx)
                
                # å‰å‘ä¼ æ’­
                loss = self._train_step(batch)
            except RuntimeError as e:
                error_str = str(e)
                if "CUDA" in error_str or "device-side assert" in error_str or "index" in error_str.lower():
                    debug_logger = get_debug_logger()
                    debug_logger.error("=" * 80)
                    debug_logger.error(f"CUDAé”™è¯¯åœ¨batch {batch_idx}: {e}")
                    debug_logger.error("=" * 80)
                    debug_logger.error("å°è¯•åœ¨CPUä¸Šæ£€æŸ¥batchï¼ˆå¦‚æœå¯èƒ½ï¼‰...")
                    
                    # å°è¯•è·å–åŸå§‹batchï¼ˆåœ¨ç§»åŠ¨åˆ°GPUä¹‹å‰ï¼‰
                    # å¦‚æœbatchå·²ç»åœ¨GPUä¸Šä¸”CUDAé”™è¯¯ï¼Œå¯èƒ½æ— æ³•ç§»åŠ¨å›CPU
                    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨_prepare_batchä¹‹å‰ä¿å­˜ä¸€ä»½CPUå‰¯æœ¬
                    try:
                        # å°è¯•å°†tensorç§»å›CPUæ£€æŸ¥
                        batch_cpu = {}
                        for key, value in batch.items():
                            if isinstance(value, torch.Tensor):
                                try:
                                    # ä½¿ç”¨detach()å’Œcpu()ï¼Œé¿å…æ¢¯åº¦é—®é¢˜
                                    value_cpu = value.detach().cpu()
                                    batch_cpu[key] = value_cpu
                                    debug_logger.error(f"  {key}:")
                                    debug_logger.error(f"    shape: {value.shape}")
                                    debug_logger.error(f"    dtype: {value.dtype}")
                                    debug_logger.error(f"    device: {value.device}")
                                    if 'ids' in key.lower() or 'mask' in key.lower():
                                        debug_logger.error(f"    min: {value_cpu.min().item()}")
                                        debug_logger.error(f"    max: {value_cpu.max().item()}")
                                        if value.numel() < 100:
                                            debug_logger.error(f"    values: {value_cpu.tolist()}")
                                except Exception as inner_e:
                                    debug_logger.error(f"  {key}: æ— æ³•ç§»åŠ¨åˆ°CPUæ£€æŸ¥ - {inner_e}")
                            else:
                                debug_logger.error(f"  {key}: {type(value)} = {value}")
                    except Exception as check_e:
                        debug_logger.error(f"æ— æ³•æ£€æŸ¥batchè¯¦æƒ…: {check_e}")
                    
                    debug_logger.error("=" * 80)
                    debug_logger.error("å»ºè®®ï¼šæ£€æŸ¥æ•°æ®åŠ è½½å’Œ_prepare_batchä¸­çš„éªŒè¯é€»è¾‘")
                    debug_logger.error("=" * 80)
                    
                    # æ§åˆ¶å°åªè¾“å‡ºç®€è¦ä¿¡æ¯
                    logger.error(f"âŒ CUDAé”™è¯¯åœ¨batch {batch_idx}ï¼ˆè¯¦ç»†ä¿¡æ¯å·²å†™å…¥è°ƒè¯•æ—¥å¿—æ–‡ä»¶ï¼‰")
                    raise
                else:
                    raise
            
            # ç´¯ç§¯æŸå¤±
            total_loss += loss
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': f'{loss:.4f}'})
            
            # è°ƒç”¨batchç»“æŸå›è°ƒ
            batch_logs = {'loss': loss}
            self._call_callbacks('on_batch_end', batch=batch_idx, logs=batch_logs)
            
            self.global_step += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        
        return {'train_loss': avg_loss}
    
    def _train_step(self, batch: Dict[str, Any]) -> float:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # æ¸…é›¶æ¢¯åº¦
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        if self.use_amp:
            with torch.cuda.amp.autocast():
                outputs = self.model(**batch)
                loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
                loss = loss / self.gradient_accumulation_steps
        else:
            outputs = self.model(**batch)
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
            loss = loss / self.gradient_accumulation_steps
        
        # åå‘ä¼ æ’­
        if self.use_amp:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (self.global_step + 1) % self.gradient_accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm is not None:
                if self.use_amp:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    self.optimizer.step()
            else:
                if self.use_amp:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
        
        return loss.item() * self.gradient_accumulation_steps
    
    def _validate(self) -> Dict[str, float]:
        """éªŒè¯"""
        logger.info("å¼€å§‹éªŒè¯...")
        
        # è°ƒç”¨éªŒè¯å¼€å§‹å›è°ƒ
        self._call_callbacks('on_validation_begin')
        
        # è¯„ä¼°
        val_logs = self.evaluator.evaluate(self.val_dataloader)
        
        # æ·»åŠ val_å‰ç¼€
        val_logs = {f'val_{k}': v for k, v in val_logs.items()}
        
        # è°ƒç”¨éªŒè¯ç»“æŸå›è°ƒ
        self._call_callbacks('on_validation_end', logs=val_logs)
        
        return val_logs
    
    # def _prepare_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
    #     """
    #     å‡†å¤‡batchï¼Œç§»åŠ¨åˆ°è®¾å¤‡å¹¶éªŒè¯tensor shapeså’Œå€¼
        
    #     æ³¨æ„ï¼šBLIPç­‰æ¨¡å‹å¯¹attention_maskçš„å½¢çŠ¶å’Œå€¼æœ‰ä¸¥æ ¼è¦æ±‚
    #     """
    #     prepared_batch = {}
        
    #     # é¦–å…ˆéªŒè¯å…³é”®å­—æ®µ
    #     if 'input_ids' in batch:
    #         input_ids = batch['input_ids']
    #         if not isinstance(input_ids, torch.Tensor):
    #             raise TypeError(f"input_idsåº”è¯¥æ˜¯torch.Tensorï¼Œå¾—åˆ°{type(input_ids)}")
            
    #         # éªŒè¯input_ids shape
    #         if input_ids.dim() != 2:
    #             raise ValueError(f"input_idsåº”è¯¥æ˜¯2D tensor [batch_size, seq_len]ï¼Œå¾—åˆ°shape {input_ids.shape}")
            
    #         batch_size, seq_len = input_ids.shape
            
    #         # éªŒè¯attention_maskï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    #         if 'attention_mask' in batch:
    #             attention_mask = batch['attention_mask']
    #             if not isinstance(attention_mask, torch.Tensor):
    #                 raise TypeError(f"attention_maskåº”è¯¥æ˜¯torch.Tensorï¼Œå¾—åˆ°{type(attention_mask)}")
                
    #             # éªŒè¯attention_mask shape
    #             if attention_mask.shape != input_ids.shape:
    #                 logger.warning(
    #                     f"attention_mask shape {attention_mask.shape} ä¸ input_ids shape {input_ids.shape} ä¸åŒ¹é…ï¼Œ"
    #                     f"å°è¯•ä¿®å¤..."
    #                 )
    #                 # å°è¯•ä¿®å¤ï¼šå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•reshapeæˆ–é‡æ–°åˆ›å»º
    #                 if attention_mask.dim() == 1 and len(attention_mask) == seq_len:
    #                     # å¦‚æœæ˜¯1Dä¸”é•¿åº¦åŒ¹é…ï¼Œæ‰©å±•åˆ°batchç»´åº¦
    #                     attention_mask = attention_mask.unsqueeze(0).expand(batch_size, -1)
    #                 elif attention_mask.dim() == 2 and attention_mask.size(0) == batch_size:
    #                     # å¦‚æœbatchç»´åº¦åŒ¹é…ä½†seq_lenä¸åŒ¹é…ï¼Œé‡æ–°åˆ›å»º
    #                     if attention_mask.size(1) != seq_len:
    #                         # é‡æ–°åˆ›å»ºattention_maskï¼šépaddingä½ç½®ä¸º1
    #                         pad_id = getattr(self.model.config, 'pad_token_id', None) if hasattr(self.model, 'config') else None
    #                         if pad_id is None:
    #                             # å¦‚æœæ²¡æœ‰pad_token_idï¼Œå‡è®¾æ‰€æœ‰é0ä½ç½®éƒ½æ˜¯æœ‰æ•ˆtoken
    #                             attention_mask = (input_ids != 0).long()
    #                         else:
    #                             attention_mask = (input_ids != pad_id).long()
    #                 else:
    #                     # å®Œå…¨é‡æ–°åˆ›å»º
    #                     pad_id = getattr(self.model.config, 'pad_token_id', None) if hasattr(self.model, 'config') else None
    #                     if pad_id is None:
    #                         attention_mask = (input_ids != 0).long()
    #                     else:
    #                         attention_mask = (input_ids != pad_id).long()
                    
    #                 logger.info(f"ä¿®å¤åçš„attention_mask shape: {attention_mask.shape}")
                
    #             # éªŒè¯attention_maskå€¼ï¼ˆåº”è¯¥æ˜¯0æˆ–1ï¼‰
    #             unique_values = torch.unique(attention_mask)
    #             invalid_values = unique_values[(unique_values != 0) & (unique_values != 1)]
    #             if len(invalid_values) > 0:
    #                 logger.warning(
    #                     f"attention_maskåŒ…å«éæ³•å€¼: {invalid_values.tolist()}ï¼Œ"
    #                     f"å°†clampåˆ°[0, 1]èŒƒå›´"
    #                 )
    #                 attention_mask = torch.clamp(attention_mask, 0, 1).long()
                
    #             prepared_batch['attention_mask'] = attention_mask.to(self.device)
            
    #         # éªŒè¯labelsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    #         if 'labels' in batch:
    #             labels = batch['labels']
    #             if isinstance(labels, torch.Tensor):
    #                 if labels.shape != input_ids.shape:
    #                     logger.warning(
    #                         f"labels shape {labels.shape} ä¸ input_ids shape {input_ids.shape} ä¸åŒ¹é…"
    #                     )
    #                     # å°è¯•ä¿®å¤ï¼šå¦‚æœç»´åº¦ä¸åŒ¹é…
    #                     if labels.dim() == 1 and len(labels) == seq_len:
    #                         labels = labels.unsqueeze(0).expand(batch_size, -1)
    #                     elif labels.dim() == 2 and labels.size(0) == batch_size and labels.size(1) != seq_len:
    #                         # å¦‚æœseq_lenä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦paddingæˆ–truncation
    #                         logger.error(f"æ— æ³•ä¿®å¤labels shapeä¸åŒ¹é…: {labels.shape} vs {input_ids.shape}")
    #                         raise ValueError(f"labels shapeä¸åŒ¹é…: {labels.shape} vs {input_ids.shape}")
    #                 prepared_batch['labels'] = labels.to(self.device)
            
    #         prepared_batch['input_ids'] = input_ids.to(self.device)
        
    #     # å¤„ç†å…¶ä»–å­—æ®µ
    #     for key, value in batch.items():
    #         if key not in prepared_batch:  # é¿å…é‡å¤å¤„ç†
    #             if isinstance(value, torch.Tensor):
    #                 prepared_batch[key] = value.to(self.device)
    #             elif isinstance(value, (list, tuple)) and len(value) > 0 and isinstance(value[0], torch.Tensor):
    #                 # å¤„ç†tensoråˆ—è¡¨ï¼ˆå¦‚pixel_valuesçš„batchï¼‰
    #                 prepared_batch[key] = [v.to(self.device) for v in value]
    #             else:
    #                 prepared_batch[key] = value
        
    #     # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰tensoréƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
    #     for key, value in prepared_batch.items():
    #         if isinstance(value, torch.Tensor) and value.device != self.device:
    #             logger.warning(f"{key}ä¸åœ¨æ­£ç¡®è®¾å¤‡ä¸Š: {value.device} vs {self.device}ï¼Œç§»åŠ¨åˆ°{self.device}")
    #             prepared_batch[key] = value.to(self.device)
        
    #     return prepared_batch

    def _prepare_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‡†å¤‡batchï¼Œç§»åŠ¨åˆ°è®¾å¤‡å¹¶éªŒè¯tensor shapeså’Œå€¼
        
        ç‰¹åˆ«æ³¨æ„BLIPæ¨¡å‹çš„ç‰¹æ®Šè¦æ±‚
        """
        prepared_batch = {}
        
        # è·å–æ¨¡å‹è¯è¡¨å¤§å°
        vocab_size = None
        text_vocab_size = None
        if hasattr(self.model, 'config'):
            vocab_size = getattr(self.model.config, 'vocab_size', None)
            # BLIPæœ‰å•ç‹¬çš„text_config
            if hasattr(self.model.config, 'text_config'):
                text_vocab_size = getattr(self.model.config.text_config, 'vocab_size', None)
        
        # ä½¿ç”¨text_vocab_sizeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        effective_vocab_size = text_vocab_size or vocab_size
        
        if 'input_ids' in batch:
            input_ids = batch['input_ids']
            if not isinstance(input_ids, torch.Tensor):
                raise TypeError(f"input_idsåº”è¯¥æ˜¯torch.Tensorï¼Œå¾—åˆ°{type(input_ids)}")
            
            if input_ids.dim() != 2:
                raise ValueError(f"input_idsåº”è¯¥æ˜¯2D tensor [batch_size, seq_len]ï¼Œå¾—åˆ°shape {input_ids.shape}")
            
            batch_size, seq_len = input_ids.shape
            
            # åœ¨CPUä¸ŠéªŒè¯ï¼ˆç¡®ä¿åœ¨ç§»åŠ¨åˆ°GPUå‰å®Œæˆæ‰€æœ‰ä¿®å¤ï¼‰
            # å¦‚æœinput_idså·²ç»åœ¨GPUä¸Šï¼Œå…ˆç§»å›CPU
            if input_ids.is_cuda:
                input_ids_cpu = input_ids.cpu()
            else:
                input_ids_cpu = input_ids.clone()
            
            max_id = input_ids_cpu.max().item()
            min_id = input_ids_cpu.min().item()
            
            debug_logger = get_debug_logger()
            debug_logger.info(f"ğŸ“Š input_idsç»Ÿè®¡: min={min_id}, max={max_id}, vocab_size={effective_vocab_size}")
            
            # æ£€æŸ¥å¹¶ä¿®å¤
            if effective_vocab_size is not None:
                if max_id >= effective_vocab_size or min_id < 0:
                    debug_logger.error(f"âŒ input_idsè¶…å‡ºèŒƒå›´: [{min_id}, {max_id}] vs [0, {effective_vocab_size-1}]")
                    
                    # ä¿®å¤ç­–ç•¥
                    pad_id = getattr(self.model.config, 'pad_token_id', 0)
                    unk_id = getattr(self.model.config, 'unk_token_id', pad_id)
                    
                    debug_logger.warning(f"   ğŸ”§ Clampingåˆ°æœ‰æ•ˆèŒƒå›´...")
                    input_ids_cpu = torch.clamp(input_ids_cpu, 0, effective_vocab_size - 1)
                    input_ids = input_ids_cpu
                    
                    debug_logger.info(f"   âœ… ä¿®å¤å: min={input_ids.min().item()}, max={input_ids.max().item()}")
            
            # ç¡®ä¿input_idsåœ¨CPUä¸Šï¼Œç„¶åå†ç§»åŠ¨åˆ°GPU
            if input_ids.is_cuda:
                input_ids = input_ids.cpu()
            prepared_batch['input_ids'] = input_ids.to(self.device)
            
            # ===== å…³é”®ï¼šå¤„ç† decoder_input_ids (BLIPç‰¹æœ‰) =====
            if 'decoder_input_ids' in batch:
                decoder_input_ids = batch['decoder_input_ids']
                if isinstance(decoder_input_ids, torch.Tensor):
                    # ç¡®ä¿åœ¨CPUä¸Šå¤„ç†
                    if decoder_input_ids.is_cuda:
                        decoder_input_ids_cpu = decoder_input_ids.cpu()
                    else:
                        decoder_input_ids_cpu = decoder_input_ids.clone()
                    
                    max_dec_id = decoder_input_ids_cpu.max().item()
                    min_dec_id = decoder_input_ids_cpu.min().item()
                    
                    debug_logger = get_debug_logger()
                    debug_logger.info(f"ğŸ“Š decoder_input_idsç»Ÿè®¡: min={min_dec_id}, max={max_dec_id}")
                    
                    if effective_vocab_size is not None:
                        if max_dec_id >= effective_vocab_size or min_dec_id < 0:
                            debug_logger.error(f"âŒ decoder_input_idsè¶…å‡ºèŒƒå›´!")
                            decoder_input_ids_cpu = torch.clamp(decoder_input_ids_cpu, 0, effective_vocab_size - 1)
                            decoder_input_ids = decoder_input_ids_cpu
                            debug_logger.info(f"   âœ… decoderä¿®å¤å: min={decoder_input_ids.min().item()}, max={decoder_input_ids.max().item()}")
                    
                    # ç¡®ä¿åœ¨CPUä¸Šï¼Œç„¶åå†ç§»åŠ¨åˆ°GPU
                    if decoder_input_ids.is_cuda:
                        decoder_input_ids = decoder_input_ids.cpu()
                    prepared_batch['decoder_input_ids'] = decoder_input_ids.to(self.device)
            
            # å¤„ç† attention_mask
            if 'attention_mask' in batch:
                attention_mask = batch['attention_mask']
                if not isinstance(attention_mask, torch.Tensor):
                    raise TypeError(f"attention_maskåº”è¯¥æ˜¯torch.Tensorï¼Œå¾—åˆ°{type(attention_mask)}")
                
                debug_logger = get_debug_logger()
                if attention_mask.shape != input_ids.shape:
                    debug_logger.warning(f"attention_mask shapeä¸åŒ¹é…ï¼Œé‡æ–°åˆ›å»º...")
                    pad_id = getattr(self.model.config, 'pad_token_id', 0)
                    # ä½¿ç”¨CPUä¸Šçš„input_idsåˆ›å»ºattention_mask
                    input_ids_for_mask = prepared_batch.get('input_ids', input_ids)
                    if input_ids_for_mask.is_cuda:
                        input_ids_for_mask = input_ids_for_mask.cpu()
                    attention_mask = (input_ids_for_mask != pad_id).long()
                
                # éªŒè¯å€¼ï¼ˆåœ¨CPUä¸Šï¼‰
                if attention_mask.is_cuda:
                    attention_mask_cpu = attention_mask.cpu()
                else:
                    attention_mask_cpu = attention_mask.clone()
                
                unique_values = torch.unique(attention_mask_cpu)
                if not all(v in [0, 1] for v in unique_values.tolist()):
                    debug_logger.warning(f"attention_maskåŒ…å«éæ³•å€¼ï¼Œä¿®å¤ä¸­...")
                    attention_mask = torch.clamp(attention_mask_cpu, 0, 1).long()
                
                # ç¡®ä¿åœ¨CPUä¸Šï¼Œç„¶åå†ç§»åŠ¨åˆ°GPU
                if attention_mask.is_cuda:
                    attention_mask = attention_mask.cpu()
                prepared_batch['attention_mask'] = attention_mask.to(self.device)
            
            # ===== å…³é”®ï¼šå¤„ç† decoder_attention_mask =====
            if 'decoder_attention_mask' in batch:
                decoder_attention_mask = batch['decoder_attention_mask']
                if isinstance(decoder_attention_mask, torch.Tensor):
                    decoder_attention_mask_cpu = decoder_attention_mask.cpu()
                    unique_values = torch.unique(decoder_attention_mask_cpu)
                    debug_logger = get_debug_logger()
                    if not all(v in [0, 1] for v in unique_values.tolist()):
                        debug_logger.warning(f"decoder_attention_maskåŒ…å«éæ³•å€¼ï¼Œä¿®å¤ä¸­...")
                        decoder_attention_mask = torch.clamp(decoder_attention_mask_cpu, 0, 1).long()
                    prepared_batch['decoder_attention_mask'] = decoder_attention_mask.to(self.device)
            
            # å¤„ç† labels
            if 'labels' in batch:
                labels = batch['labels']
                if isinstance(labels, torch.Tensor):
                    debug_logger = get_debug_logger()
                    if labels.shape != input_ids.shape:
                        debug_logger.warning(f"labels shape {labels.shape} ä¸ input_ids shape {input_ids.shape} ä¸åŒ¹é…")
                        if labels.dim() == 1 and len(labels) == seq_len:
                            labels = labels.unsqueeze(0).expand(batch_size, -1)
                        elif labels.dim() == 2 and labels.size(0) == batch_size and labels.size(1) != seq_len:
                            # å¯¹äºBLIPï¼Œlabelså¯èƒ½æ˜¯answerçš„token idsï¼Œé•¿åº¦å¯èƒ½ä¸åŒ
                            debug_logger.info(f"labelsé•¿åº¦ä¸input_idsä¸åŒï¼Œè¿™å¯¹BLIPæ˜¯æ­£å¸¸çš„")
                    
                    # éªŒè¯labelså€¼ï¼ˆåœ¨CPUä¸Šï¼Œç¡®ä¿åœ¨ç§»åŠ¨åˆ°GPUå‰å®Œæˆæ‰€æœ‰ä¿®å¤ï¼‰
                    # å¦‚æœlabelså·²ç»åœ¨GPUä¸Šï¼Œå…ˆç§»å›CPU
                    if labels.is_cuda:
                        labels_cpu = labels.cpu()
                    else:
                        labels_cpu = labels.clone()
                    
                    valid_labels = labels_cpu[labels_cpu != -100]
                    if len(valid_labels) > 0:
                        max_label = valid_labels.max().item()
                        min_label = valid_labels.min().item()
                        
                        debug_logger.info(f"ğŸ“Š labelsç»Ÿè®¡: min={min_label}, max={max_label} (å¿½ç•¥-100)")
                        
                        if effective_vocab_size is not None:
                            if max_label >= effective_vocab_size or min_label < 0:
                                debug_logger.error(f"âŒ labelsè¶…å‡ºèŒƒå›´: [{min_label}, {max_label}] vs [0, {effective_vocab_size-1}]")
                                debug_logger.warning(f"   ğŸ”§ å°†éæ³•labelsè®¾ç½®ä¸º-100...")
                                
                                # åˆ›å»ºmaskå¹¶æ›¿æ¢
                                mask = (labels_cpu != -100) & ((labels_cpu < 0) | (labels_cpu >= effective_vocab_size))
                                labels_cpu[mask] = -100
                                labels = labels_cpu
                                
                                debug_logger.info(f"   âœ… labelsä¿®å¤å®Œæˆ")
                    
                    # ç¡®ä¿labelsåœ¨CPUä¸Šï¼Œç„¶åå†ç§»åŠ¨åˆ°GPU
                    if labels.is_cuda:
                        labels = labels.cpu()
                    prepared_batch['labels'] = labels.to(self.device)
        
        # å¤„ç†å…¶ä»–å­—æ®µ
        for key, value in batch.items():
            if key not in prepared_batch:
                if isinstance(value, torch.Tensor):
                    prepared_batch[key] = value.to(self.device)
                elif isinstance(value, (list, tuple)) and len(value) > 0 and isinstance(value[0], torch.Tensor):
                    prepared_batch[key] = [v.to(self.device) for v in value]
                else:
                    prepared_batch[key] = value
        
        return prepared_batch
    
    def _validate_batch_on_device(self, batch: Dict[str, Any], batch_idx: int):
        """
        åœ¨GPUä¸ŠéªŒè¯batchï¼ˆå¦‚æœå¯èƒ½ï¼‰
        
        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•åœ¨batchå·²ç»ç§»åŠ¨åˆ°GPUåè°ƒç”¨ï¼Œä¸»è¦ç”¨äºæœ€åçš„æ£€æŸ¥
        """
        debug_logger = get_debug_logger()
        
        # è·å–vocab_size
        vocab_size = None
        text_vocab_size = None
        if hasattr(self.model, 'config'):
            vocab_size = getattr(self.model.config, 'vocab_size', None)
            if hasattr(self.model.config, 'text_config'):
                text_vocab_size = getattr(self.model.config.text_config, 'vocab_size', None)
        
        effective_vocab_size = text_vocab_size or vocab_size
        
        # åªæ£€æŸ¥å…³é”®å­—æ®µï¼Œé¿å…é¢‘ç¹çš„CPU-GPUä¼ è¾“
        if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ—¶è¯¦ç»†æ£€æŸ¥
            for key in ['input_ids', 'labels', 'attention_mask']:
                if key in batch and isinstance(batch[key], torch.Tensor):
                    tensor = batch[key]
                    try:
                        # å°è¯•åœ¨GPUä¸Šæ£€æŸ¥ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                        if tensor.is_cuda:
                            # å¯¹äºCUDA tensorï¼Œå…ˆå°è¯•åœ¨GPUä¸Šæ£€æŸ¥min/max
                            # å¦‚æœå¤±è´¥ï¼Œè¯´æ˜tensorå¯èƒ½å·²ç»æŸå
                            try:
                                max_val = tensor.max().item()
                                min_val = tensor.min().item()
                                
                                if key == 'labels':
                                    # labelså¯ä»¥æ˜¯-100
                                    valid_tensor = tensor[tensor != -100]
                                    if len(valid_tensor) > 0:
                                        max_valid = valid_tensor.max().item()
                                        min_valid = valid_tensor.min().item()
                                        
                                        if effective_vocab_size and (max_valid >= effective_vocab_size or min_valid < 0):
                                            debug_logger.error(
                                                f"âš ï¸  GPUä¸Šçš„{key}åŒ…å«éæ³•å€¼: [{min_valid}, {max_valid}] vs vocab_size={effective_vocab_size}"
                                            )
                                else:
                                    if effective_vocab_size and (max_val >= effective_vocab_size or min_val < 0):
                                        debug_logger.error(
                                            f"âš ï¸  GPUä¸Šçš„{key}åŒ…å«éæ³•å€¼: [{min_val}, {max_val}] vs vocab_size={effective_vocab_size}"
                                        )
                            except RuntimeError as e:
                                debug_logger.warning(f"æ— æ³•åœ¨GPUä¸Šæ£€æŸ¥{key}: {e}")
                    except Exception as e:
                        debug_logger.warning(f"æ£€æŸ¥{key}æ—¶å‡ºé”™: {e}")
    
    def validate_first_batch(self):
        """
        éªŒè¯ç¬¬ä¸€ä¸ªè®­ç»ƒbatchçš„æ•°æ®
        
        åœ¨è®­ç»ƒå¼€å§‹å‰æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®ï¼Œç‰¹åˆ«æ˜¯token IDsæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        """
        debug_logger = get_debug_logger()
        debug_logger.info("=" * 60)
        debug_logger.info("ğŸ” éªŒè¯ç¬¬ä¸€ä¸ªè®­ç»ƒbatch...")
        debug_logger.info("=" * 60)
        
        # åªåœ¨æ§åˆ¶å°è¾“å‡ºç®€è¦ä¿¡æ¯
        logger.info("ğŸ” éªŒè¯ç¬¬ä¸€ä¸ªè®­ç»ƒbatchï¼ˆè¯¦ç»†ä¿¡æ¯å†™å…¥è°ƒè¯•æ—¥å¿—æ–‡ä»¶ï¼‰...")
        
        try:
            # è·å–ç¬¬ä¸€ä¸ªbatch
            first_batch = next(iter(self.train_dataloader))
            
            # è·å–vocab_size
            vocab_size = None
            text_vocab_size = None
            
            if hasattr(self.model, 'config'):
                vocab_size = getattr(self.model.config, 'vocab_size', None)
                # BLIPæœ‰å•ç‹¬çš„text_config
                if hasattr(self.model.config, 'text_config'):
                    text_vocab_size = getattr(self.model.config.text_config, 'vocab_size', None)
            
            effective_vocab_size = text_vocab_size or vocab_size
            
            if effective_vocab_size:
                debug_logger.info(f"ğŸ“Š æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {effective_vocab_size}")
            else:
                debug_logger.warning("âš ï¸  æ— æ³•è·å–æ¨¡å‹è¯æ±‡è¡¨å¤§å°ï¼Œè·³è¿‡token IDèŒƒå›´éªŒè¯")
            
            # æ£€æŸ¥æ¯ä¸ªå…³é”®å­—æ®µ
            for key, value in first_batch.items():
                if isinstance(value, torch.Tensor):
                    # ç§»åŠ¨åˆ°CPUæ£€æŸ¥ï¼Œé¿å…CUDAé”™è¯¯
                    value_cpu = value.cpu()
                    
                    debug_logger.info(f"\n  {key}:")
                    debug_logger.info(f"    shape: {value.shape}")
                    debug_logger.info(f"    dtype: {value.dtype}")
                    debug_logger.info(f"    device: {value.device}")
                    debug_logger.info(f"    min: {value_cpu.min().item()}")
                    debug_logger.info(f"    max: {value_cpu.max().item()}")
                    
                    # æ£€æŸ¥token IDå­—æ®µ
                    if 'id' in key.lower() and effective_vocab_size:
                        max_val = value_cpu.max().item()
                        min_val = value_cpu.min().item()
                        
                        if key == 'labels':
                            # labelså¯ä»¥æ˜¯-100ï¼Œåªæ£€æŸ¥é-100çš„å€¼
                            valid_values = value_cpu[value_cpu != -100]
                            if len(valid_values) > 0:
                                max_valid = valid_values.max().item()
                                min_valid = valid_values.min().item()
                                
                                if max_valid >= effective_vocab_size or min_valid < 0:
                                    debug_logger.error(
                                        f"    âŒ é”™è¯¯ï¼š{key}åŒ…å«éæ³•token ID: "
                                        f"[{min_valid}, {max_valid}] vs vocab_size={effective_vocab_size}"
                                    )
                                    logger.error(f"âŒ {key}åŒ…å«éæ³•token IDï¼Œè¯¦æƒ…è§è°ƒè¯•æ—¥å¿—")
                                else:
                                    debug_logger.info(f"    âœ… {key} token IDèŒƒå›´æ­£å¸¸: [{min_valid}, {max_valid}]")
                        else:
                            # input_idså’Œdecoder_input_idså¿…é¡»åœ¨[0, vocab_size-1]èŒƒå›´å†…
                            if max_val >= effective_vocab_size or min_val < 0:
                                debug_logger.error(
                                    f"    âŒ é”™è¯¯ï¼š{key}åŒ…å«éæ³•token ID: "
                                    f"[{min_val}, {max_val}] vs vocab_size={effective_vocab_size}"
                                )
                                logger.error(f"âŒ {key}åŒ…å«éæ³•token IDï¼Œè¯¦æƒ…è§è°ƒè¯•æ—¥å¿—")
                            else:
                                debug_logger.info(f"    âœ… {key} token IDèŒƒå›´æ­£å¸¸: [{min_val}, {max_val}]")
                    
                    # æ£€æŸ¥attention_mask
                    if 'mask' in key.lower():
                        unique_values = torch.unique(value_cpu)
                        invalid_values = unique_values[(unique_values != 0) & (unique_values != 1)]
                        if len(invalid_values) > 0:
                            debug_logger.error(
                                f"    âŒ é”™è¯¯ï¼š{key}åŒ…å«éæ³•å€¼ï¼ˆä¸æ˜¯0æˆ–1ï¼‰: {invalid_values.tolist()}"
                            )
                            logger.error(f"âŒ {key}åŒ…å«éæ³•å€¼ï¼Œè¯¦æƒ…è§è°ƒè¯•æ—¥å¿—")
                        else:
                            debug_logger.info(f"    âœ… {key}å€¼æ­£å¸¸ï¼ˆä»…åŒ…å«0å’Œ1ï¼‰")
            
            debug_logger.info("=" * 60)
            debug_logger.info("âœ… ç¬¬ä¸€ä¸ªbatchéªŒè¯å®Œæˆ")
            debug_logger.info("=" * 60)
            
            logger.info("âœ… ç¬¬ä¸€ä¸ªbatchéªŒè¯å®Œæˆï¼ˆè¯¦ç»†ä¿¡æ¯å·²å†™å…¥è°ƒè¯•æ—¥å¿—ï¼‰")
            
        except Exception as e:
            debug_logger = get_debug_logger()
            debug_logger.error(f"âŒ éªŒè¯ç¬¬ä¸€ä¸ªbatchæ—¶å‡ºé”™: {e}")
            debug_logger.error("è¿™å¯èƒ½å¯¼è‡´è®­ç»ƒæ—¶å‡ºç°CUDAé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½ä»£ç ")
            import traceback
            debug_logger.error(traceback.format_exc())
            logger.error(f"âŒ éªŒè¯ç¬¬ä¸€ä¸ªbatchæ—¶å‡ºé”™: {e}ï¼ˆè¯¦ç»†ä¿¡æ¯è§è°ƒè¯•æ—¥å¿—ï¼‰")
    
    def _call_callbacks(self, method_name: str, **kwargs):
        """è°ƒç”¨å›è°ƒå‡½æ•°"""
        for callback in self.callbacks:
            if hasattr(callback, method_name):
                try:
                    getattr(callback, method_name)(self, **kwargs)
                except Exception as e:
                    logger.error(f"å›è°ƒå‡½æ•° {callback.__class__.__name__}.{method_name} æ‰§è¡Œå¤±è´¥: {e}")
    
    def save_checkpoint(self, filepath: str, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'history': self.history,
            **kwargs
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, filepath)
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint.get('epoch', 0)
        self.history = checkpoint.get('history', [])
        
        if self.scheduler and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")


def create_trainer_from_config(
    data_config_path: str,
    model_name: str,
    model_type: Optional[str] = None,
    task: str = 'vqa',
    **kwargs
) -> Trainer:
    """
    ä»é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒå™¨
    
    Args:
        data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        model_type: æ¨¡å‹ç±»å‹
        task: ä»»åŠ¡ç±»å‹
        **kwargs: å…¶ä»–è®­ç»ƒå‚æ•°
        
    Returns:
        Trainerå®ä¾‹
    """
    # 1. åŠ è½½æ•°æ®
    logger.info("åŠ è½½æ•°æ®...")
    pipeline = DataPipeline(data_config_path)
    pipeline.setup()
    train_loader = pipeline.get_train_dataloader()
    val_loader = pipeline.get_val_dataloader() if 'validation' in pipeline.datasets else None
    
    # 2. åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    model_result = load_model(
        model_name=model_name,
        model_type=model_type,
        task=task,
        device=kwargs.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'),
        load_processor=True
    )
    model = model_result['model']
    processor = model_result.get('processor')
    
    # 3. é…ç½®ä¼˜åŒ–å™¨
    optimizer_config = kwargs.get('optimizer', {})
    lr = optimizer_config.get('lr', 3e-5)
    weight_decay = optimizer_config.get('weight_decay', 0.01)
    optimizer_type = optimizer_config.get('type', 'adamw')
    
    if optimizer_type.lower() == 'adamw':
        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type.lower() == 'adam':
        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type.lower() == 'sgd':
        optimizer = SGD(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # 4. é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    scheduler_config = kwargs.get('scheduler', {})
    if scheduler_config:
        scheduler_type = scheduler_config.get('type', 'cosine')
        if scheduler_type == 'cosine':
            scheduler = CosineAnnealingLR(
                optimizer,
                T_max=kwargs.get('num_epochs', 3)
            )
        elif scheduler_type == 'step':
            scheduler = StepLR(
                optimizer,
                step_size=scheduler_config.get('step_size', 1),
                gamma=scheduler_config.get('gamma', 0.1)
            )
        elif scheduler_type == 'reduce_on_plateau':
            scheduler = ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=scheduler_config.get('factor', 0.5),
                patience=scheduler_config.get('patience', 2)
            )
    
    # 5. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = None
    if task == 'vqa' and processor:
        evaluator = VQAEvaluator(model, processor, device=kwargs.get('device'))
    else:
        evaluator = Evaluator(model, device=kwargs.get('device'))
    
    # 6. åˆ›å»ºå›è°ƒå‡½æ•°
    callbacks = []
    
    # è¿›åº¦æ¡
    callbacks.append(ProgressBarCallback(verbose=1))
    
    # æ—©åœ
    if kwargs.get('early_stopping', {}).get('enabled', False):
        callbacks.append(EarlyStoppingCallback(
            monitor=kwargs['early_stopping'].get('monitor', 'val_loss'),
            patience=kwargs['early_stopping'].get('patience', 5)
        ))
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    save_dir = kwargs.get('save_dir', 'checkpoints')
    callbacks.append(ModelCheckpointCallback(
        save_dir=save_dir,
        monitor=kwargs.get('checkpoint_monitor', 'val_loss'),
        save_best_only=kwargs.get('save_best_only', True)
    ))
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å›è°ƒ
    if scheduler:
        callbacks.append(LearningRateSchedulerCallback(scheduler))
    
    # TensorBoardï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if kwargs.get('use_tensorboard', False):
        callbacks.append(TensorBoardCallback(log_dir=f'{save_dir}/tensorboard'))
    
    # CSVæ—¥å¿—
    callbacks.append(CSVLoggerCallback(filename=f'{save_dir}/training_log.csv'))
    
    # 7. å†»ç»“å±‚ï¼ˆå¦‚æœé…ç½®ï¼‰
    freeze_config = kwargs.get('freeze', {})
    if freeze_config.get('enabled', False):
        freeze_layers = freeze_config.get('layers', [])
        freeze_model(model, freeze_layers=freeze_layers if freeze_layers else None)
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=kwargs.get('device'),
        callbacks=callbacks,
        evaluator=evaluator,
        num_epochs=kwargs.get('num_epochs', 3),
        gradient_accumulation_steps=kwargs.get('gradient_accumulation_steps', 1),
        max_grad_norm=kwargs.get('max_grad_norm'),
        fp16=kwargs.get('fp16', False),
        save_dir=save_dir
    )
    
    return trainer


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("Traineræ¨¡å—åŠ è½½å®Œæˆ - æä¾›å®Œæ•´çš„è®­ç»ƒåŠŸèƒ½")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("""
    from training.trainer import create_trainer_from_config
    
    trainer = create_trainer_from_config(
        data_config_path='config/vqa_config.yaml',
        model_name='Salesforce/blip-vqa-base',
        model_type='blip',
        task='vqa',
        num_epochs=3,
        fp16=True
    )
    
    trainer.train()
    """)

